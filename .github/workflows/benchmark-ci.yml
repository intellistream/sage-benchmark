name: Benchmark CI

# Runs Q1–Q8 workloads against the real sagellm CPU backend and uploads
# results to Hugging Face via the upload-to-hf workflow.
#
# Stack (single process, no sidecar – closes sage-benchmark#19):
#   sagellm serve --backend cpu --with-embedding
#     → Gateway + Control Plane + LLM engine (port 8888)
#     → CPUEmbeddingEngine auto-started and registered (port 8890)
#   python __main__.py --all --quick   →  Q1–Q8 reduced-scale
#   scripts/aggregate_for_hf.py        →  prepare hf_data/
#   git push hf_data/                  →  triggers upload-to-hf.yml

on:
  workflow_dispatch:
    inputs:
      quick:
        description: "Run in quick (reduced-scale) mode"
        required: false
        default: "true"
        type: choice
        options:
          - "true"
          - "false"
      experiments:
        description: "Experiments to run (comma-separated Q-IDs or 'all')"
        required: false
        default: "all"
  schedule:
    # Every day at 02:00 UTC
    - cron: "0 2 * * *"

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    env:
      HF_ENDPOINT: https://hf-mirror.com
      SAGELLM_PORT: 8888
      SAGELLM_EMBED_PORT: 8890

    steps:
      # ── Checkout ──────────────────────────────────────────────────────────
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      # ── Python environment ────────────────────────────────────────────────
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          # isagellm>=0.5.1.9: ships CPUEmbeddingEngine + sagellm serve --with-embedding
          python -m pip install "isagellm>=0.5.1.9"
          python -m pip install -e .

      # ── Start full stack (LLM + Embedding in one process) ─────────────────
      - name: Start sagellm full stack (CPU + embedding, port ${{ env.SAGELLM_PORT }})
        run: |
          sagellm serve \
            --backend cpu \
            --model sshleifer/tiny-gpt2 \
            --port "$SAGELLM_PORT" \
            --with-embedding \
            --embedding-model sentence-transformers/all-MiniLM-L6-v2 \
            --embedding-port "$SAGELLM_EMBED_PORT" \
            > /tmp/sagellm.log 2>&1 &
          echo "SAGELLM_PID=$!" >> "$GITHUB_ENV"
          echo "sagellm serve started (PID $!)"

      # ── Wait for LLM gateway ───────────────────────────────────────────────
      - name: Wait for gateway /health
        run: |
          echo "Polling http://localhost:${SAGELLM_PORT}/health ..."
          for i in $(seq 1 72); do
            if curl -sf "http://localhost:${SAGELLM_PORT}/health" > /dev/null 2>&1; then
              echo "Gateway is healthy after ~$((i * 5))s"
              break
            fi
            if [ "$i" -eq 72 ]; then
              echo "::error::Gateway did not start within 6 minutes"
              echo "=== sagellm log ===" && cat /tmp/sagellm.log || true
              exit 1
            fi
            sleep 5
          done

      # ── Wait for embedding engine to be registered ─────────────────────────
      # sagellm --with-embedding starts embedding in background and registers it;
      # poll /v1/models until the embedding model appears (max 3 min).
      - name: Wait for embedding engine registration
        run: |
          echo "Waiting for embedding engine to register with control plane..."
          for i in $(seq 1 36); do
            MODELS=$(curl -sf "http://localhost:${SAGELLM_PORT}/v1/models" 2>/dev/null || echo '{}')
            if echo "$MODELS" | python -c "
import sys, json
d = json.load(sys.stdin)
models = [m.get('id','') for m in d.get('data', [])]
has_embed = any('MiniLM' in m or 'embed' in m.lower() for m in models)
print('models:', models)
sys.exit(0 if has_embed else 1)
" 2>/dev/null; then
              echo "Embedding engine registered after ~$((i * 5))s"
              break
            fi
            if [ "$i" -eq 36 ]; then
              echo "::warning::Embedding engine not registered after 3 min; continuing anyway"
              break
            fi
            sleep 5
          done

      # ── Smoke-test both endpoints ──────────────────────────────────────────
      - name: Smoke-test LLM and embedding endpoints
        run: |
          echo "--- LLM smoke test ---"
          curl -sf -X POST "http://localhost:${SAGELLM_PORT}/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d '{"model":"sshleifer/tiny-gpt2","messages":[{"role":"user","content":"hi"}],"max_tokens":5}' \
            | python -c "import sys,json; d=json.load(sys.stdin); print('LLM ok, tokens:', d.get('usage',{}))"

          echo "--- Embedding smoke test ---"
          curl -sf -X POST "http://localhost:${SAGELLM_PORT}/v1/embeddings" \
            -H "Content-Type: application/json" \
            -d '{"model":"BAAI/bge-small-zh-v1.5","input":["hello world"]}' \
            | python -c "import sys,json; d=json.load(sys.stdin); vecs=d['data']; print(f'Embedding ok, dim={len(vecs[0][\"embedding\"])}')"

      # ── Run Q1–Q8 benchmarks ───────────────────────────────────────────────
      - name: Run Q1–Q8 benchmarks
        run: |
          QUICK_FLAG=""
          if [ "${{ github.event.inputs.quick || 'true' }}" = "true" ]; then
            QUICK_FLAG="--quick"
          fi

          EXPERIMENTS="${{ github.event.inputs.experiments || 'all' }}"
          if [ "$EXPERIMENTS" = "all" ]; then
            python __main__.py --all $QUICK_FLAG --output-dir results/ci_run
          else
            IFS=',' read -ra EXP_LIST <<< "$EXPERIMENTS"
            for EXP in "${EXP_LIST[@]}"; do
              python __main__.py --experiment "$(echo "$EXP" | tr -d ' ')" $QUICK_FLAG --output-dir results/ci_run
            done
          fi

      # ── Aggregate results for HF upload ───────────────────────────────────
      - name: Aggregate results
        run: python scripts/aggregate_for_hf.py

      # ── Commit hf_data/ to trigger upload-to-hf.yml ───────────────────────
      - name: Commit and push hf_data (triggers upload-to-hf workflow)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add hf_data/
          if git diff --cached --quiet; then
            echo "No new benchmark results to push."
          else
            git commit -m "ci: benchmark results $(date -u +%Y-%m-%dT%H:%M:%SZ) [run ${{ github.run_id }}]"
            git push
            echo "Pushed hf_data/ — upload-to-hf workflow will now run."
          fi

      # ── Always upload artifacts ────────────────────────────────────────────
      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: results/ci_run
          if-no-files-found: warn

      - name: Upload sagellm logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: sagellm-logs-${{ github.run_id }}
          path: /tmp/sagellm.log
