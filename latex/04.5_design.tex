\section{Design Details}
\label{sec:design_details}

\paragraph{Design goals.}
SAGE targets hybrid RAG/agent workloads in which each query triggers a multi-stage pipeline---retrieval over vector/text indices, memory read/write, optional refinement (e.g., compression or reranking), and final generation/tool use---and the same workload must scale from local runs to heterogeneous clusters.
First, we prioritize \emph{composability} by elevating the pipeline---a dataflow DAG of operators---to the primary abstraction, enabling retrieval, memory, refinement, and generation to be expressed as composable, interchangeable stages rather than entangled control flow.
Second, we emphasize \emph{resource-aware execution} by supporting explicit placement and scheduling over CPU-only and GPU nodes, enabling mixed deployments where embedding/retrieval-heavy stages run on CPUs while generation-heavy stages are assigned to GPUs.
Third, we enforce \emph{isolation and stability} through streaming-style flow control (bounded queues and backpressure) and policy-driven routing decisions, so slow or bursty stages do not destabilize the rest of the pipeline.
This architecture also supports rigorous, repeatable system evaluation by representing each workflow as a declarative DAG with interchangeable operators, which enables controlled ablations and fair comparisons; we describe the evaluation methodology in \S\ref{sec:experiments}.

\subsection{Architecture Overview and Layering}
\label{subsec:dd_arch_overview}

SAGE is organized as a layered modular monolith with \textbf{one-way dependencies} to keep the system extensible while avoiding architectural erosion. Concretely, code in higher layers may depend only on lower layers (L5$\rightarrow$L4$\rightarrow$L3$\rightarrow$L2$\rightarrow$L1), and \emph{reverse} (``upward'') dependencies are disallowed. This separation makes it possible to evolve execution and runtime mechanisms independently from domain operators, and to enforce clean boundaries between reusable primitives (types, configurations, algorithms) and runtime-bound components (operators, backends, C++ extensions). When cross-layer instantiation is required (e.g., a lower layer needs to trigger creation of a higher-layer implementation), SAGE uses explicit registration and factory hooks rather than direct imports, preserving the one-way dependency invariant.

\paragraph{Core packages by layer.}
\begin{itemize}
  \item \textbf{L1 (Foundation): \texttt{sage-common}.} Shared types, configuration, and utilities (e.g., standardized ports and environment keys), intended to be dependency-minimal and reusable across the stack.
  \item \textbf{L2 (Platform services): \texttt{sage-platform}.} Abstract services such as queue, storage, and service lifecycle management that underpin distributed execution without tying the design to a single backend.
  \item \textbf{L3 (Core): \texttt{sage-kernel}, \texttt{sage-libs}.} The dataflow runtime and execution engine live in \texttt{sage-kernel}, while \texttt{sage-libs} provides algorithmic building blocks (e.g., RAG utilities and agentic logic) that remain runnable without heavyweight external services.
  \item \textbf{L4 (Domain/middleware): \texttt{sage-middleware}.} Domain operators and runtime-bound components for end-to-end pipelines (retrieval, memory, refinement, and generation connectors), including performance-critical C++ extensions (e.g., streaming, vector, and memory backends) exposed via Python bindings.
  \item \textbf{L5 (Interfaces and tooling): \texttt{sage-cli}, \texttt{sage-tools}.} User-facing command-line entry points and developer tooling (testing, quality checks, maintenance workflows).
\end{itemize}
In addition, SAGE ships a lightweight meta-package \texttt{sage} (PyPI: \texttt{isage}) that aggregates the core packages for installation convenience, without changing the layer boundaries above.

\paragraph{Independent repositories (outside the core).}
Several ecosystem components are intentionally maintained outside the SAGE core repository and are treated as external dependencies: \textbf{\texttt{isagellm}} (LLM control plane, gateway, and unified inference client), \textbf{\texttt{isage-benchmark}} (evaluation framework), \textbf{\texttt{sage-studio}} (visual workflow builder), and \textbf{\texttt{sage-examples}} (tutorials and applications). This separation keeps the core layering stable and lightweight while allowing fast iteration on product-facing services and benchmarking infrastructure.

\subsection{Compilation and Execution Graph}
\label{subsec:dd_compilation}
SAGE compiles a user-authored \emph{logical} pipeline DAG (a sequence of transformations registered in the environment) into a \emph{physical} execution graph that is explicit about parallelism, communication, and routing. Concretely, the \texttt{ExecutionGraph} component in \texttt{sage-kernel} lowers each logical operator (a \texttt{BaseTransformation}) into a set of \emph{task replicas} according to its declared parallelism; each replica is represented as a \texttt{TaskNode} and is associated with a \texttt{TaskFactory} (which in turn uses an \texttt{OperatorFactory}) to instantiate the runnable task. For every logical edge between operators, the compiler materializes a complete bipartite set of physical edges between upstream and downstream replicas (i.e., an $m{\times}n$ expansion when the upstream and downstream parallelisms are $m$ and $n$), producing an execution graph in which the degree of fan-out and fan-in is no longer implicit.

Edges are \emph{materialized as bounded channels} by allocating queue descriptors for communication at the node level. In particular, each task replica owns a bounded input channel (implemented as a platform-specific queue descriptor, e.g., \texttt{PythonQueueDescriptor} locally or \texttt{RayQueueDescriptor} remotely), and all upstream replicas targeting that replica write into this queue; this implements backpressure because the router performs blocking writes when the channel is full. Multi-input operators are supported by tagging each physical edge with an explicit input index, so that downstream tasks can demultiplex incoming packets by logical input port even though the physical channel is shared.

Fan-out and fan-in become first-class objects in the execution graph and routing layer. Fan-out is represented by grouping the outgoing physical edges of a replica into explicit output groups (one group per downstream logical connection), which are compiled into downstream connection metadata inside \texttt{TaskContext}. At runtime, the \texttt{BaseRouter} uses these groups to implement policy-driven delivery, including round-robin routing across downstream replicas, broadcast, and partitioned (hash-based) routing based on packet metadata. Symmetrically, fan-in is realized by multiple upstream replicas writing into the same bounded input queue of each downstream replica, making contention and flow control explicit and observable in the physical plan.

\subsection{Scheduling, Placement, and Backpressure}
\label{subsec:dd_scheduling}

At runtime, each task replica executes a lightweight \emph{worker loop} in \texttt{sage-kernel}: (i) dequeue one item from its input channel, (ii) execute the operator logic on that item, and (iii) route any emitted outputs to downstream replicas using partition hints carried in the data packet. Concretely, non-source tasks block on a queue descriptor, then invoke the operator's packet-processing method; downstream delivery is handled by the router (\texttt{BaseRouter}), which inspects \texttt{Packet} metadata (e.g., partition key and partition strategy) to implement round-robin, broadcast, or hash-partitioned routing across downstream replicas.

SAGE enforces stability via \emph{bounded queues} and \emph{backpressure propagation}. Each replica is provisioned with a bounded input channel (queue descriptors defined in \texttt{sage-platform}), and all upstream replicas write into these bounded channels. The router performs blocking writes into downstream queues; when a downstream stage is saturated (e.g., a GPU-heavy generation operator under bursty arrivals), queue occupancy naturally throttles upstream producers, preventing unbounded buffering and limiting interference across pipeline stages. This mechanism is particularly important for mixed workloads that combine retrieval and memory stages (often CPU-bound and parallel) with generation or refinement stages (often GPU-bound and latency-sensitive), since it provides isolation without requiring global synchronization.

Placement and scheduling are coordinated by the \texttt{Dispatcher} in \texttt{sage-kernel}, which separates \emph{decision} from \emph{execution}. A scheduler (\texttt{BaseScheduler} and its implementations) produces a \texttt{PlacementDecision} that encodes (a) resource requirements (CPU, GPU, memory, and custom resources) and (b) target-node affinity in distributed settings. The \texttt{PlacementExecutor} then translates this decision into physical placement---e.g., mapping resource requests to Ray actor options and optionally enforcing node affinity---so that GPU-heavy stages can be steered toward GPU nodes while retrieval or preprocessing stages can be placed on CPU-only nodes. Together, the scheduler/placement split and queue-based backpressure provide a resource-aware execution model that scales across heterogeneous CPU/GPU clusters while maintaining predictable behavior under load.

\subsection{State Management and Fault Tolerance}
\label{subsec:dd_ft}

SAGE distinguishes between \emph{transient} execution state (in-flight packets buffered in bounded queues) and \emph{durable} operator/task state (e.g., counters, caches, aggregation tables) that must be preserved to avoid recomputation. To make state explicit and composable, SAGE exposes a uniform state interface at the runtime boundary: each task can materialize a self-contained snapshot via \texttt{get\_state()} and rehydrate from a snapshot via \texttt{restore\_state(state)}. Concretely, the task-level snapshot aggregates (i) task metadata and progress counters and (ii) operator-provided state when available; operator state can in turn include user-function state when the operator wraps a stateful function. This layered snapshot structure lets SAGE checkpoint at a stable runtime boundary without requiring application code to understand queue internals or replica placement decisions.

Fault tolerance is configured declaratively through the environment configuration and instantiated by the kernel at submission time. SAGE currently provides two built-in recovery policies:

\textbf{(1) Checkpoint-based recovery} periodically persists task snapshots at a user-configurable interval and retries recovery up to a maximum number of attempts. In the steady state, the worker loop opportunistically triggers checkpoint writes during processing (and also forces a checkpoint upon exceptions to maximize recovery fidelity). Checkpoints are managed by a checkpoint manager that versions snapshots by task identifier and timestamp and stores them under a configurable directory. Upon failure, the recovery handler loads the most recent checkpoint and requests a task restart with state restoration through the dispatcher. The dispatcher stops and cleans up the failed task, re-creates a fresh replica using the normal scheduling and placement path, restores state, and resumes execution.

\textbf{(2) Restart-based recovery} targets stateless (or externally stateful) stages, where correctness does not require restoring in-memory state and the simplest recovery is to restart computation. SAGE parameterizes restart policies via explicit restart strategies (e.g., fixed delay, exponential backoff, and failure-rate-based control), allowing users to trade off recovery aggressiveness versus stability under repeated failures. In the current implementation, checkpoint-based recovery provides end-to-end task restart \emph{with} state restoration via the dispatcher, while restart-based recovery provides the restart policy surface and is being integrated with dispatcher-driven task restarts for fully stateless pipelines.

For distributed (remote) execution, SAGE complements exception-driven recovery with active liveness monitoring. A heartbeat monitor periodically polls each remote task for heartbeat statistics and treats repeated timeouts or stale heartbeats as failures; such failures are forwarded into the same fault-handler interface, ensuring a single recovery entry point regardless of whether failures manifest as explicit exceptions or as silent task death. Resource cleanup is centralized via a lifecycle manager that performs best-effort task and actor cleanup with bounded waiting, preventing leaked resources from compounding failures during recovery.

Finally, durable state requires a storage boundary that is stable across process restarts and placement changes. SAGE factors storage access into an L2 abstraction layer: \texttt{sage-platform} defines backend-agnostic key-value storage interfaces (\texttt{BaseKVBackend}) and provides lightweight implementations (e.g., in-memory dictionary with explicit load/store) with optional filesystem support (e.g., HDFS integration). While the current checkpoint manager persists snapshots to the local filesystem by default, this layering makes it straightforward to route checkpoints or auxiliary operator state to alternative backends by implementing the same storage interface, without entangling fault-tolerance logic with any particular storage system.

\subsection{Middleware Operators for RAG Pipelines}
\label{subsec:dd_middleware}

A practical RAG system is not a single retrieval invocation followed by a single generation invocation: it is a multi-stage subsystem that must (i) access external indices and memory, (ii) transform and filter retrieved evidence, (iii) construct prompts with provenance, and (iv) invoke LLM inference and downstream tools. In SAGE, we therefore promote RAG subsystems to \emph{first-class operators} in the dataflow graph, rather than hiding them behind black-box library calls. This design gives the runtime visibility into stage boundaries and costs---enabling pipeline-wide scheduling, parallelism scaling, backpressure propagation, and monitoring at the same granularity as other streaming transformations---while also making cross-stage optimizations (e.g., batching, caching, and placement decisions for retrieval-heavy vs.\ LLM-heavy stages) explicit and composable. Concretely, these domain operators live in the L4 middleware layer (not L3 libraries) because they depend on runtime-bound capabilities such as vector stores, memory backends, and refinement services.

SAGE's middleware layer provides three reusable operator namespaces for RAG and agentic applications:

\textbf{(i) RAG operators} encapsulate canonical RAG stages---retrieval, reranking, refinement, prompting, generation, evaluation, and lightweight orchestration. For example, \texttt{RAGPipeline} composes a retriever/reranker/refiner/generator sequence; retrievers such as \texttt{ChromaRetriever} and \texttt{MilvusRetriever} expose retrieval as map-style operators; prompt construction is implemented via prompt operators (e.g., \texttt{QAPromptor}); and generation is implemented via OpenAI-compatible endpoints or via the internal \texttt{SageLLMGenerator}. Importantly, context compression is also modeled as an operator: \texttt{RefinerOperator} wraps external refinement compressors (LongRefiner, REFORM, Provence) from the independent \texttt{isage-refiner} package, so compression becomes a schedulable, monitorable stage rather than a hidden post-processing step.

\textbf{(ii) LLM operators} provide LLM inference as an operator with a backend-decoupled engine factory interface. The primary entry point is \texttt{SageLLMGenerator}, which normalizes inputs (context, prompt, and options), instantiates an engine via a factory (no hard-coded backend classes), and exposes synchronous or streaming generation. This supports treating LLM inference uniformly as a dataflow transformation---subject to the same runtime controls (parallelism, placement, backpressure) as non-LLM stages.

\textbf{(iii) Tool operators} expose tool invocation as operators for agentic and RAG workflows (e.g., web search, paper search, URL text extraction, and optional heavy tools such as image captioning). This makes tool calls explicit nodes in the graph, enabling consistent logging, throttling, and policy routing across heterogeneous external services.

Internally, these operators are backed by core middleware components that provide the runtime-facing systems a RAG pipeline needs:
\begin{itemize}
  \item \textbf{\texttt{sage\_db}}: integrates the independent C++-backed SageVDB package (\texttt{isage-vdb}) as the vector database backend.
  \item \textbf{\texttt{sage\_mem}}: exposes memory backends as a namespace package, with NeuroMem-backed collections and managers provided by the independent \texttt{isage-neuromem} package.
  \item \textbf{\texttt{sage\_refiner}}: integrates the independent \texttt{isage-refiner} package at the operator level, keeping compression and refinement as explicit pipeline stages while allowing underlying algorithms to evolve independently.
  \item \textbf{\texttt{sage\_flow}}: provides vector-native streaming building blocks via the independent \texttt{isage-flow} package and its C++ extension for high-throughput streaming vector processing.
  \item \textbf{\texttt{sage\_tsdb}}: provides time-series storage and streaming analytics hooks via the independent C++-backed \texttt{isage-tsdb} package, plus SAGE-specific algorithms for windowing and out-of-order stream joins.
\end{itemize}

Placing these subsystems in L4 is also a performance choice. Vector search, vector-native streaming operators, and time-series window/join primitives benefit from native implementations (C++ cores with Python bindings) to reduce per-record overhead and enable batched execution paths. By integrating these high-performance backends as middleware components and exposing their use through explicit operators, SAGE combines domain-level performance (native backends) with system-level optimization (graph-visible stages under the unified scheduler and backpressure runtime).

\subsection{Algorithmic Libraries and Agentic Tooling}
\label{subsec:dd_libs}

SAGE separates \emph{algorithmic building blocks} from \emph{runtime-bound execution}, enabling rapid iteration without entangling methods with placement, backends, or orchestration details. The L3 package \texttt{sage-libs} is explicitly designed as an interface and registry layer: it defines abstract base classes, typed data structures, and factory/registration APIs for core domains such as agentic reasoning, RAG, fine-tuning, evaluation, privacy, safety, and algorithmic primitives (e.g., ANN and AMM). In this design, \texttt{sage-libs} modules remain independent of runtime placement and external services: a planner, reranker, or safety policy can be swapped without changing how the pipeline is scheduled, replicated, or backpressured.

Concretely, \texttt{sage-libs} provides:
\begin{itemize}
  \item \textbf{Agentic interfaces} that standardize agents, planners, tool selectors, and orchestration components, with concrete implementations intended to live in external packages (e.g., \texttt{isage-agentic}).
  \item \textbf{RAG utilities and interfaces}, including typed RAG artifacts for pipeline interoperability and lightweight built-ins such as document loaders and chunkers, while retrievers, rerankers, and pipelines are instantiated via registries.
  \item \textbf{Integrations} that are local and algorithmic (e.g., a HuggingFace client wrapper for local inference).
  \item \textbf{Built-in utilities} for data operations and basic processing that support experimental pipelines with minimal external dependencies.
\end{itemize}
In addition, \texttt{sage-libs} provides interface layers for evaluation, fine-tuning, privacy/unlearning, and safety guardrails, again emphasizing pluggability through registries and externalized heavyweight implementations.

The boundary with L4 middleware is deliberate: \textbf{libs provide reusable algorithms; middleware wraps them as operators}. Middleware (\texttt{sage-middleware}) is allowed to depend on runtime-bound backends and services (vector stores, memory systems, refinement engines, external APIs) and exposes these capabilities as schedulable dataflow operators (e.g., RAG retrievers, promptors, generators, tool operators). By keeping \texttt{sage-libs} focused on interfaces, policies, and lightweight utilities, SAGE makes it easy to compose and ablate algorithmic choices---e.g., swapping a retriever, reranker, refiner policy, planner strategy, or safety filter---without rewriting pipeline orchestration. This separation supports systematic evaluation: experiments can vary a single algorithmic component while holding the execution graph and runtime controls fixed, yielding cleaner ablations and more repeatable comparisons across agentic and RAG workloads.

\subsection{Inference Engine Integration}
\label{subsec:dd_inference}
SAGE treats LLM and embedding inference as an \emph{external service interface} rather than a library-internal dependency, decoupling the dataflow runtime from model serving. Concretely, SAGE integrates with the independent \texttt{isagellm} Gateway and Control Plane, which exposes OpenAI-compatible endpoints (e.g., \texttt{/v1/chat/completions} and \texttt{/v1/embeddings}) and manages underlying inference engines (e.g., vLLM-based backends) as separately scalable services. This separation allows SAGE pipelines to remain portable across deployment environments: the same compiled execution graph can target different model servers by changing only endpoint configuration (e.g., base URL, model identifier), without modifying operator logic or the kernel scheduler.

At the operator level, generation and embedding are implemented as request/response calls to OpenAI-compatible APIs. For example, RAG generation operators instantiate a standard OpenAI client with a configurable base URL and issue chat-completion requests with structured message payloads, while embedding components similarly invoke the embeddings API and return dense vectors for downstream retrieval and indexing. Because these operators depend only on a stable HTTP contract, SAGE can route requests through the Gateway/Control Plane (for unified engine management and admission control) or directly to a compatible backend, keeping the pipeline runtime agnostic to the concrete inference implementation.

This service-oriented interface also enables \emph{independent scaling} of inference backends versus pipeline stages. In practice, preprocessing, retrieval, and postprocessing stages can be replicated and scheduled on CPU-rich nodes, while LLM and embedding throughput is scaled by adding or resizing dedicated inference engines behind the Gateway/Control Plane. The two dimensions of parallelism---(i) dataflow operator replication in the SAGE execution graph and (ii) inference-engine provisioning in the serving layer---can therefore be tuned separately to meet latency and throughput targets and to reduce resource contention between GPU-bound model execution and CPU-bound orchestration.

Finally, the same pattern generalizes beyond LLMs: any request/response service (LLM, embedding, reranking, moderation, or domain tools) can be wrapped as an operator with explicit resource annotations (e.g., CPU/GPU requirements, concurrency limits, and batching policies). These annotations allow SAGE's runtime to incorporate service calls into its placement and backpressure mechanisms, preserving end-to-end stability while supporting heterogeneous backends and evolving model-serving stacks.
