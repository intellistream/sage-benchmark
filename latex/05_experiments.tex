% ===========================================================================
% SAGE ICML 2026 - Experiments Section (V2)
% Based on EXPERIMENT_WRITING_PROMPT_V2.md with corrected data
% ===========================================================================
\section{Evaluation}
\label{sec:experiments}

We evaluate SAGE through comprehensive experiments designed to characterize the system's capabilities and design trade-offs. As a novel declarative dataflow framework for LLM inference orchestration, our evaluation follows the methodology of foundational systems papers~\citep{DeanGhemawat2004MapReduce,ZahariaEtAl2010Spark}: we demonstrate \textit{what the system can do} rather than comparing against prior systems serving different goals.

% ---------------------------------------------------------------------------
\subsection{Experimental Setup}
\label{subsec:setup}
% ---------------------------------------------------------------------------

\paragraph{Hardware.}
We deploy SAGE on a 16-node CPU cluster connected via Gigabit Ethernet. Each node (\texttt{sage-node-1} to \texttt{sage-node-16}) has 8 CPU cores and 32GB RAM. LLM inference is offloaded to a dedicated NVIDIA A100 (80GB) GPU server running vLLM~\citep{kwon2023pagedattention}. This configuration reflects practical deployments where CPU nodes handle pipeline orchestration while GPU resources are centralized for model serving.

\paragraph{Software.}
SAGE is implemented in Python 3.11 with Ray 2.9.0~\citep{moritz2018ray} as the distributed runtime. We use Qwen2.5-3B-Instruct for LLM inference and BAAI/bge-large-en-v1.5 for embedding operations.

\paragraph{Workloads.}
We evaluate three representative pipeline types designed to stress different aspects of the system:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Compute}: CPU-intensive data preprocessing (tokenization,           feature extraction)---tests scheduling overhead with fast tasks.
    \item \textbf{RAG}: Retrieval-Augmented Generation with 4 stages           (query$\to$retrieve$\to$rerank$\to$generate)---tests LLM integration
          with I/O-bound workloads.
    \item \textbf{Mixed}: Heterogeneous pipeline combining compute-intensive           preprocessing with LLM stages---tests adaptive scheduling under           workload heterogeneity.
\end{itemize}
Unless otherwise noted, experiments use 5000 tasks to ensure statistical significance.

\paragraph{Metrics.}
We measure throughput (tasks/sec), end-to-end latency (average and P99), and load balance score---defined as $1 - \sigma/\mu$ where $\sigma$ and $\mu$ are the standard deviation and mean of per-node task counts, respectively.

% ---------------------------------------------------------------------------
\subsection{Node Scalability}
\label{subsec:eval_scalability}
% ---------------------------------------------------------------------------

We first investigate how SAGE scales with cluster size. Figure~\ref{fig:node_scalability} shows throughput as we increase nodes from 1 to 16 across all three pipeline types.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Figures/Experiment/node_scalability.pdf}
\caption{Node scalability across pipeline types. RAG achieves 8.6$\times$ speedup at
16 nodes, approaching ideal linear scaling. Compute shows modest scaling (3.8$\times$)
due to higher scheduling overhead relative to task duration.}
\label{fig:node_scalability}
\end{figure}

% [Table commented out - Figure already presents the same data visually]
% \begin{table}[t]
% \centering
% \fontsize{7.5}{9}\selectfont
% \caption{Throughput (tasks/sec) across node counts and pipeline types. Speedup is
% relative to single-node baseline.}
% \label{tab:scalability}
% \begin{tabular}{lccccc|c}
% \toprule
% \textbf{Pipeline} & \textbf{1 Node} & \textbf{2 Nodes} & \textbf{4 Nodes} & \textbf{8 Nodes} & \textbf{16 Nodes} & \textbf{Speedup} \\
% \midrule
% Compute & 38.14 & 55.03 & 57.16 & 38.90 & 19.39 & 0.5$\times$ \\
% RAG     & 1.55  & 3.03  & 5.84  & 11.41 & 17.60 & 11.4$\times$ \\
% Mixed   & 1.64  & 3.16  & 4.71  & 4.51  & 10.04 & 6.1$\times$ \\
% \bottomrule
% \end{tabular}
% \end{table}

The results reveal distinct scaling behaviors across pipeline types:

\paragraph{RAG achieves near-linear scaling.}
The RAG pipeline achieves \textbf{11.4$\times$ speedup} at 16 nodes (from 1.55 to 17.60 tasks/sec), demonstrating 71\% parallel efficiency. This near-linear scaling occurs because per-task latency (dominated by LLM inference at$\sim$500ms) greatly exceeds scheduling overhead ($<$5ms), making distributed coordination costs negligible.

\paragraph{Compute exhibits scalability inversion.}
The Compute pipeline exhibits an unexpected phenomenon: throughput peaks at 4 nodes (57.16 tasks/sec) then \textit{declines}, yielding only \textbf{0.5$\times$} at 16 nodes (19.39 vs 38.14 tasks/sec baseline). This \textit{scalability inversion} occurs because task duration ($\sim$25ms) is shorter than the distributed coordination overhead. By Amdahl's Law \cite{amdahl1967validity}, when the serial fraction $s > 0.5$, adding nodes degrades rather than improves performance. This result underscores that fine-grained tasks are fundamentally unsuitable for distributed execution.

\paragraph{Mixed workloads show moderate scaling with anomaly.}
The Mixed pipeline achieves \textbf{6.1$\times$ speedup} at 16 nodes (38\% efficiency), but exhibits non-monotonic behavior: throughput at 8 nodes (4.51 tasks/sec) is lower than at 4 nodes (4.71 tasks/sec). This anomaly suggests that heterogeneous workloads can trigger suboptimal scheduling decisions when load characteristics shift between scale points, motivating future work on adaptive scheduling policies.

\textbf{Insight:} The divergent scaling behaviors across pipelines highlight an
important design consideration: \textit{task granularity should exceed distributed coordination overhead}. We recommend task durations $>$100ms for efficient scaling. Developers working with fine-grained compute tasks should consider batching or coarsening before distributing across nodes.

% ---------------------------------------------------------------------------
\subsection{Scheduling Strategy Analysis}
\label{subsec:eval_scheduling}
% ---------------------------------------------------------------------------

SAGE's pluggable scheduler interface enables operators to select strategies based
on workload characteristics. We evaluate five scheduling policies across multiple
dimensions. Figure~\ref{fig:scheduler_radar} visualizes the trade-offs and
Table~\ref{tab:scheduler} presents the raw metrics.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{Figures/Experiment/scheduler_radar.pdf}
\caption{Radar chart comparing five scheduling strategies across throughput, latency
(inverted), load balance, and overhead dimensions. No single strategy dominates
all metrics.}
\label{fig:scheduler_radar}
\end{figure}

\begin{table}[t]
\centering
\fontsize{7.5}{9}\selectfont
\caption{Comparison of scheduling strategies (16 nodes, 5000 tasks).}
\label{tab:scheduler}
\begin{tabular}{lcccc}
\toprule
\textbf{Scheduler} & \textbf{Throughput} & \textbf{Avg Lat.} & \textbf{P99 Lat.} & \textbf{Balance} \\
\midrule
FIFO            & 18.5/s & 2.5s  & 7.0s  & 52\% \\
RoundRobin      & 17.8/s & 2.6s  & 7.2s  & 85\% \\
LoadAware-Spread & 16.4/s & 2.5s  & 6.5s  & 98\% \\
LoadAware-Pack  & 15.9/s & 2.7s  & 6.8s  & 75\% \\
Priority        & 19.2/s & 2.8s  & 12.0s & 90\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{FIFO: Maximum throughput, poor balance.}
FIFO achieves highest raw throughput (18.5/s) with minimal scheduling overhead, but suffers from severe load imbalance (52\% balance score). Fast nodes starve while slow nodes accumulate queued tasks.

\paragraph{LoadAware-Spread: Best tail latency.}
LoadAware-Spread trades 11\% throughput for near-perfect balance (98\%) by distributing tasks based on real-time queue depths. This significantly reduces P99 latency (6.5s vs 7.0s for FIFO), making it suitable for SLO-sensitive deployments.

\paragraph{Priority: Throughput at the cost of tail latency.}
Priority scheduling maximizes throughput for high-priority tasks (19.2/s) but exhibits \textit{priority inversion} under contention, causing 2$\times$ higher P99 latency (12.0s). This is a well-known challenge in priority scheduling for distributed systems~\citep{sha1990priority}.

\textbf{Insight:} No single scheduler dominates across all metrics. SAGE's declarative model decouples scheduling policy from pipeline definition, enabling runtime policy selection based on operational requirements. For latency-sensitive applications, we recommend LoadAware-Spread; for batch processing where throughput matters, FIFO or Priority may be preferable.

% ---------------------------------------------------------------------------
\subsection{Concurrency Scaling}
\label{subsec:eval_concurrency}
% ---------------------------------------------------------------------------

We investigate the relationship between pipeline concurrency and system performance to identify optimal operating points. Figure~\ref{fig:concurrency} presents throughput and P99 latency as concurrency varies from 1 to 32 on a single node.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Figures/Experiment/concurrency_scaling.pdf}
\caption{Throughput-latency trade-off across concurrency levels for Compute
and RAG pipelines. Both exhibit an optimal region at concurrency 4--8 (shaded).
Beyond this, resource contention causes throughput collapse and latency explosion.}
\label{fig:concurrency}
\end{figure}

% \begin{table}[t]
% \centering
% \fontsize{7.5}{9}\selectfont
% \caption{Concurrency scaling for Compute and RAG pipelines (single node, FIFO scheduler).}
% \label{tab:concurrency}
% \begin{tabular}{lcc|cc}
% \toprule
% & \multicolumn{2}{c}{\textbf{Throughput (tasks/s)}} & \multicolumn{2}{c}{\textbf{P99 Latency (ms)}} \\
% \textbf{Concurrency} & \textbf{Compute} & \textbf{RAG} & \textbf{Compute} & \textbf{RAG} \\
% \midrule
% 1  & 12.1 & 2.0  & 93    & 2397 \\
% 2  & 22.0 & 3.9  & 105   & 1077 \\
% 4  & 38.0 & 7.3  & 180   & 458 \\
% 8  & 40.3 & 11.5 & 415   & 650 \\
% 16 & 19.2 & 14.2 & 1245  & 1800 \\
% 32 & 8.9  & 7.8  & 7276  & 8500 \\
% \bottomrule
% \end{tabular}
% \end{table}

Both pipelines exhibit a characteristic \textit{optimal operating region} at
concurrency 4--8:

\paragraph{Below the optimum.}
Throughput increases near-linearly as more tasks execute in parallel. Compute achieves 3.3$\times$ speedup from concurrency 1 to 8; RAG achieves 5.8$\times$.

\paragraph{Beyond the optimum.}
Resource contention causes throughput collapse. At concurrency 32, Compute throughput drops to 22\% of peak while P99 latency increases 17$\times$. The system enters a congestion regime where queueing delays dominate.

\paragraph{Workload-dependent behavior.}
The divergent behaviors reflect workload characteristics: short-lived Compute tasks saturate CPU earlier, while RAG's LLM-bound nature allows higher concurrency before the GPU becomes the bottleneck.

\textbf{Insight:} Concurrency tuning is critical for optimal performance. SAGE's runtime should implement \textit{adaptive concurrency control} that dynamically adjusts parallelism based on observed queue depths and latency signals---a promising direction for future work.

% ---------------------------------------------------------------------------
\subsection{Multi-Pipeline Isolation}
\label{subsec:eval_isolation}
% ---------------------------------------------------------------------------

We evaluate SAGE's behavior when multiple pipelines execute concurrently, simulating multi-tenant deployments. Figure~\ref{fig:isolation} presents results for job scaling and admission control experiments.

% \begin{figure}[t]
% \centering
% \begin{minipage}[t]{0.48\linewidth}
%   \centering
%   \includegraphics[width=\linewidth]{Figures/Experiment/job_scaling.pdf}\\[-0.3em]
%   \small (a)
%   \label{fig:isolation-a}
% \end{minipage}\hfill
% \begin{minipage}[t]{0.48\linewidth}
%   \centering
%   \includegraphics[width=\linewidth]{Figures/Experiment/admission_control.pdf}\\[-0.3em]
%   \small (b)
%   \label{fig:isolation-b}
% \end{minipage}

% \caption{Multi-pipeline isolation. (a) Per-job throughput and efficiency
% degrade gracefully as concurrent jobs increase. (b) Staggered admission
% reduces P99 latency by 57\% at the cost of 30\% lower aggregate throughput.}
% \label{fig:isolation}
% \end{figure}
\begin{figure}[t]
\centering

\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/Experiment/job_scaling.pdf}\\[-0.3em]
  \small\textbf{(a)} Per-job throughput and efficiency degrade gracefully as concurrent jobs increase.
  \label{fig:isolation_a}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/Experiment/admission_control.pdf}\\[-0.3em]
  \small\textbf{(b)} Staggered admission reduces P99 latency by 57\% at the cost of 30\% lower aggregate throughput.
  \label{fig:isolation_b}
\end{minipage}

\caption{Multi-pipeline isolation.}
\label{fig:isolation}
\end{figure}




\paragraph{Job Scaling.}
Figure~\ref{fig:isolation_a}shows per-job throughput as we increase concurrent pipelines from 1 to 8. Efficiency (per-job throughput relative to single-job baseline) degrades gracefully from 100\% to 71\%, indicating effective resource isolation. The 29\% overhead at 8 jobs is primarily attributed to GPU memory bandwidth saturation and increased scheduling contention at the shared LLM endpoint.

% \begin{table}[t]
% \centering
% \fontsize{7.5}{9}\selectfont
% \caption{Multi-pipeline scaling and efficiency (5000 tasks per job).}
% \label{tab:job_scaling}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Num Jobs} & \textbf{Total Throughput} & \textbf{Per-Job} & \textbf{Efficiency} \\
% \midrule
% 1 & 12.7/s & 12.7/s & 100\% \\
% 2 & 24.0/s & 12.0/s & 94\% \\
% 4 & 44.0/s & 11.0/s & 87\% \\
% 8 & 72.0/s & 9.0/s  & 71\% \\
% \bottomrule
% \end{tabular}
% \end{table}

\paragraph{Admission Control.}
Figure~\ref{fig:isolation_b} demonstrates the impact of staggered job admission. Introducing a 5-second delay between job starts reduces P99 latency by \textbf{57\%} (77s $\to$ 33s) at the cost of 30\% lower aggregate throughput (43.6 $\to$ 30.7 tasks/s). This simple mechanism enables operators to trade throughput for predictable tail latencies based on SLO requirements.

% \begin{table}[t]
% \centering
% \fontsize{7.5}{9}\selectfont
% \caption{Impact of admission control on latency (4 concurrent pipelines).}
% \label{tab:admission}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Start Delay} & \textbf{Throughput} & \textbf{P99 Latency} \\
% \midrule
% 0s (simultaneous) & 43.6/s & 77s \\
% 1s & 44.3/s & 73s \\
% 2s & 39.2/s & 60s \\
% 5s (staggered) & 30.7/s & 33s \\
% \bottomrule
% \end{tabular}
% \end{table}

\textbf{Insight:} Simple admission control policies can significantly improve tail latency in multi-tenant LLM inference systems. SAGE's declarative model naturally supports such policies without modifying pipeline definitions---the scheduler handles admission independently of application logic.

% ---------------------------------------------------------------------------
\subsection{Limitations}
\label{subsec:limitations}
% ---------------------------------------------------------------------------

We discuss several limitations observed during evaluation:

\paragraph{Scheduling overhead at high parallelism.}
Certain scheduling policies (LoadAware, Priority) encounter performance degradation at very high parallelism levels ($>$64 concurrent tasks). This is attributed to the underlying Ray runtime's actor scheduling overhead, which grows superlinearly with task count. Future work could explore native scheduling implementations that bypass Ray's actor model for latency-critical paths.

\paragraph{Distributed coordination cost.}
For fine-grained compute tasks ($<$10ms), the overhead of distributed coordination can exceed task execution time. This is a fundamental tension in distributed systems~\cite{amdahl1967validity}, not specific to SAGE. Practitioners should ensure task granularity is sufficiently coarse (we recommend $>$100ms) to amortize coordination costs.

\paragraph{Centralized service endpoint bottleneck.}
While SAGE's service-oriented design avoids shared-data coordination challenges inherent in distributed systems, routing all requests through centralized service endpoints (e.g., LLM inference server, vector database for retrieval) creates a bottleneck under high task volumes. As concurrent pipelines increase, request queuing and resource contention at these shared services become the dominant performance limiters. Future work could explore request-level load
balancing across replicated service instances.

% ---------------------------------------------------------------------------
\subsection{Discussion}
\label{subsec:discussion}
% ---------------------------------------------------------------------------

Our experiments reveal several key insights for LLM inference orchestration:

\begin{enumerate}[noitemsep]
    \item \textbf{Declarative abstraction works.} SAGE's pipeline DSL
    successfully abstracts away distribution, scheduling, and fault
    tolerance, enabling 8.6$\times$ scaling with no code changes.

    \item \textbf{Scheduling is workload-dependent.} No single scheduler
    dominates; LoadAware suits SLO-sensitive workloads, while FIFO
    maximizes throughput for batch processing.

    \item \textbf{Admission control matters.} Simple staggered admission
    reduces tail latency by 57\%, a low-hanging fruit for production
    deployments that requires no complex fair-share scheduling.

    \item \textbf{Task granularity is critical.} Fine-grained tasks
    ($<$10ms) should be batched to amortize distributed overhead; we
    recommend $>$100ms per task for efficient scaling.
\end{enumerate}

These findings provide guidance for both SAGE users and future system
designers in the emerging LLM orchestration space. The interplay between
scheduling policy, concurrency level, and workload characteristics creates
a rich design space that merits further exploration.
