# Configuration for Experiment 5.3: End-to-End System Evaluation
# Tests complete SAGE system including kernel scheduling, middleware, dataflow

experiment:
  name: "exp_5_3_e2e"
  description: "End-to-End System Evaluation - Complete SAGE Pipeline"
  section: "5.3"

hardware:
  gpus: 2
  gpu_type: "A100"
  cpu_cores: 64
  memory_gb: 256

models:
  llm:
    name: "Qwen/Qwen2.5-7B-Instruct"
    instances: 2
    device: "cuda"
    tensor_parallel: 1
  embedding:
    name: "BAAI/bge-m3"
    instances: 1
    device: "cpu"

# Components to test
components:
- name: "job_manager"
  module: "sage.kernel.runtime.job_manager"
  enabled: true
- name: "node_selector"
  module: "sage.kernel.scheduler.node_selector"
  enabled: true
- name: "control_plane"
  module: "sage.common.components.sage_llm.sageLLM.control_plane"
  enabled: true
- name: "gateway"
  module: "sage.gateway"
  enabled: true

# E2E test scenarios
scenarios:
- name: "simple_pipeline"
  description: "Single LLM call"
  steps:
  - type: "llm_chat"
    input: "What is machine learning?"
  repeat: 100

- name: "rag_pipeline"
  description: "Embedding + Retrieval + LLM"
  steps:
  - type: "embedding"
    input: "query text"
  - type: "retrieval"
    top_k: 5
  - type: "llm_chat"
    input: "Answer based on context: {context}"
  repeat: 50

- name: "multi_step_agent"
  description: "Multi-step agent with tool calls"
  steps:
  - type: "llm_chat"
    input: "Plan task: {task}"
  - type: "tool_selection"
    candidates: 10
  - type: "llm_chat"
    input: "Execute with tool: {tool}"
  repeat: 30

- name: "batch_embedding"
  description: "Batch embedding processing"
  steps:
  - type: "embedding_batch"
    batch_size: 32
  repeat: 20

# Heterogeneous hardware test
heterogeneous:
  enabled: true
  configurations:
  - name: "gpu_only"
    llm_device: "cuda"
    embedding_device: "cuda"
  - name: "cpu_embedding"
    llm_device: "cuda"
    embedding_device: "cpu"
  - name: "mixed"
    llm_device: "cuda"
    embedding_device: "cpu"
    cpu_workers: 4

# Failure recovery test
failure_recovery:
  enabled: true
  scenarios:
  - name: "backend_restart"
    description: "Simulate vLLM backend restart"
    inject_at_request: 500
    recovery_timeout_s: 30
  - name: "timeout_handling"
    description: "Test request timeout handling"
    artificial_delay_ms: 10000
    timeout_ms: 5000

workload:
  total_requests: 500
  warmup_requests: 50
  llm_ratio: 0.6
  request_rate: 30.0
  input_tokens:
    min: 256
    max: 512
  output_tokens:
    min: 64
    max: 256
  seed: 42

metrics:
  latency_percentiles: [50, 95, 99]
  slo_targets:
    pipeline_p99_ms: 2000
    chat_p99_ms: 500
    embedding_p99_ms: 100
  report_interval_s: 10
  track_component_breakdown: true
  track_resource_utilization: true

output:
  results_dir: "results/exp_5_3"
  save_raw_data: true
  generate_plots: true
  export_latex: true
  component_breakdown: true
