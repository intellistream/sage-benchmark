experiment:
  name: "q4_scalefrontier"
  description: "Q4 ScaleFrontier (scalability): Scale-out throughput/latency workload family"
  section: "Q4"

hardware:
  gpus: 2
  gpu_type: "A100"
  cpu_cores: 64
  memory_gb: 256

models:
  llm:
    name: "Qwen/Qwen2.5-7B-Instruct"
    instances: 2
    device: "cuda"
    tensor_parallel: 1
  embedding:
    name: "BAAI/bge-m3"
    instances: 1
    device: "cpu"

workload:
  total_requests: 1500
  warmup_requests: 100
  llm_ratio: 0.7
  request_rate: 70.0
  input_tokens:
    min: 256
    max: 512
  output_tokens:
    min: 64
    max: 256
  seed: 42

policies:
  - fifo
  - priority
  - slo_aware
  - hybrid

metrics:
  latency_percentiles: [50, 95, 99]
  slo_targets:
    chat_p99_ms: 600
    embedding_p99_ms: 120
  report_interval_s: 10

output:
  results_dir: "results/q4"
  save_raw_data: true
  generate_plots: true
  export_latex: true
