# Default configuration for ICML benchmark experiments
# This file provides sensible defaults for a 2x A100 setup

# =============================================================================
# Experiment 5.1: Control Plane Unified Scheduling
# =============================================================================
# Demonstrates unified LLM+embedding scheduling improves performance
# Baselines: SAGE unified vs vLLM-only vs separated services

experiment:
  name: "exp_5_1_control_plane"
  description: "Control Plane Unified Scheduling - Mixed LLM+Embedding Workload"
  section: "5.1"

hardware:
  gpus: 2
  gpu_type: "A100"
  cpu_cores: 64
  memory_gb: 256

models:
  llm:
    name: "Qwen/Qwen2.5-7B-Instruct"
    instances: 2  # One per GPU, or 2 for tensor parallel
    device: "cuda"
    tensor_parallel: 1  # Set to 2 if using both GPUs for one model
  embedding:
    name: "BAAI/bge-m3"
    instances: 1
    device: "cpu"  # CPU to save GPU memory for LLM

workload:
  total_requests: 1000
  warmup_requests: 100
  llm_ratio: 0.7  # 70% LLM, 30% embedding
  request_rate: 50.0  # req/s
  input_tokens:
    min: 256
    max: 512
  output_tokens:
    min: 64
    max: 256
  seed: 42

baselines:
- name: "sage_unified"
  enabled: true
  description: "SAGE with unified Control Plane"
- name: "vllm_only"
  enabled: true
  description: "vLLM server with client-side embedding"
- name: "separated"
  enabled: true
  description: "Separate vLLM and embedding services"

metrics:
  latency_percentiles: [50, 95, 99]
  slo_targets:
    chat_p99_ms: 500
    embedding_p99_ms: 100
  report_interval_s: 10

output:
  results_dir: "results/exp_5_1"
  save_raw_data: true
  generate_plots: true
  export_latex: true
