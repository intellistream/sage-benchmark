"""
Workload 4 - Task 5: å›¾éå†ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å›¾å†…å­˜æœåŠ¡å’Œæ£€ç´¢ç®—å­ã€‚
"""

import numpy as np

try:
    from graph_memory import (
        GraphMemoryRetriever,
        GraphMemoryService,
        build_knowledge_graph,
    )
    from models import DocumentEvent, JoinedEvent, QueryEvent
except ImportError:
    from workload4.graph_memory import (
        GraphMemoryRetriever,
        GraphMemoryService,
        build_knowledge_graph,
    )
    from workload4.models import DocumentEvent, JoinedEvent, QueryEvent


def example_1_graph_service_basic():
    """ç¤ºä¾‹ 1: åŸºç¡€å›¾æœåŠ¡ä½¿ç”¨"""
    print("=" * 80)
    print("ç¤ºä¾‹ 1: åŸºç¡€å›¾æœåŠ¡ä½¿ç”¨")
    print("=" * 80)

    # 1. å‡†å¤‡çŸ¥è¯†åº“
    np.random.seed(42)
    knowledge_base = []

    topics = ["AI", "ML", "DL", "NLP", "CV", "RL", "KG", "IR", "DB", "HPC"]

    for i, topic in enumerate(topics):
        # ä¸ºæ¯ä¸ªä¸»é¢˜ç”Ÿæˆ embedding
        embedding = np.random.randn(128).astype(np.float32)
        embedding = embedding / np.linalg.norm(embedding)

        knowledge_base.append(
            {
                "node_id": f"topic_{topic.lower()}",
                "content": f"Knowledge about {topic} ({topic} related content)",
                "embedding": embedding.tolist(),
                "node_type": "concept",
            }
        )

    # 2. åˆ›å»ºå¹¶æ„å»ºå›¾æœåŠ¡
    service = GraphMemoryService(
        config={},
        embedding_dim=128,
        similarity_threshold=0.5,  # è¾ƒä½é˜ˆå€¼ï¼Œæ„å»ºæ›´å¤šè¾¹
    )

    service.build_graph(knowledge_base)

    print("âœ“ å›¾æ„å»ºå®Œæˆ:")
    print(f"  - èŠ‚ç‚¹æ•°: {service.graph.number_of_nodes()}")
    print(f"  - è¾¹æ•°: {service.graph.number_of_edges()}")
    print()

    # 3. æ‰§è¡Œæœç´¢
    query_emb = knowledge_base[0]["embedding"]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»é¢˜çš„ embedding

    results = service.search(
        query_embedding=query_emb,
        max_depth=2,
        max_nodes=5,
        beam_width=3,
    )

    print(f"âœ“ æœç´¢ç»“æœ (è¿”å› {len(results)} ä¸ªèŠ‚ç‚¹):")
    for i, result in enumerate(results, 1):
        path_str = " -> ".join(result["path"])
        print(
            f"  {i}. {result['node_id']} (æ·±åº¦={result['depth']}, "
            f"ç›¸å…³åº¦={result['relevance_score']:.3f})"
        )
        print(f"     è·¯å¾„: {path_str}")
        print(f"     å†…å®¹: {result['content'][:50]}...")
        print()


def example_2_graph_with_documents():
    """ç¤ºä¾‹ 2: ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾"""
    print("=" * 80)
    print("ç¤ºä¾‹ 2: ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾")
    print("=" * 80)

    # æ¨¡æ‹Ÿä¸€äº›æŠ€æœ¯æ–‡æ¡£
    documents = [
        {
            "node_id": "doc_transformer",
            "content": "Transformer architecture uses self-attention mechanism",
            "embedding": np.random.randn(64).tolist(),
        },
        {
            "node_id": "doc_bert",
            "content": "BERT is based on Transformer encoder",
            "embedding": np.random.randn(64).tolist(),
        },
        {
            "node_id": "doc_gpt",
            "content": "GPT uses Transformer decoder for generation",
            "embedding": np.random.randn(64).tolist(),
        },
        {
            "node_id": "doc_attention",
            "content": "Attention mechanism computes weighted sum of values",
            "embedding": np.random.randn(64).tolist(),
        },
        {
            "node_id": "doc_llm",
            "content": "Large Language Models are trained on massive corpora",
            "embedding": np.random.randn(64).tolist(),
        },
    ]

    # å½’ä¸€åŒ– embeddings
    for doc in documents:
        emb = np.array(doc["embedding"])
        doc["embedding"] = (emb / np.linalg.norm(emb)).tolist()

    # æ„å»ºå›¾
    graph = build_knowledge_graph(
        documents,
        embedding_dim=64,
        similarity_threshold=0.3,
    )

    print("âœ“ çŸ¥è¯†å›¾ç»Ÿè®¡:")
    print(f"  - èŠ‚ç‚¹æ•°: {graph.number_of_nodes()}")
    print(f"  - è¾¹æ•°: {graph.number_of_edges()}")
    print()

    # æ‰“å°è¾¹çš„æƒé‡
    print("âœ“ å›¾çš„è¾¹:")
    for source, target, data in graph.edges(data=True):
        print(f"  {source} -> {target} (æƒé‡={data['weight']:.3f})")
    print()


def example_3_graph_retriever_operator():
    """ç¤ºä¾‹ 3: å›¾éå†ç®—å­ä½¿ç”¨"""
    print("=" * 80)
    print("ç¤ºä¾‹ 3: å›¾éå†ç®—å­ä½¿ç”¨")
    print("=" * 80)

    # åˆ›å»ºç®—å­
    retriever = GraphMemoryRetriever(
        max_depth=3,
        max_nodes=10,
        beam_width=5,
    )

    print("âœ“ ç®—å­é…ç½®:")
    print(f"  - æœ€å¤§æ·±åº¦: {retriever.max_depth}")
    print(f"  - æœ€å¤§èŠ‚ç‚¹æ•°: {retriever.max_nodes}")
    print(f"  - Beam å®½åº¦: {retriever.beam_width}")
    print()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    query = QueryEvent(
        query_id="query_1",
        query_text="What is machine learning?",
        query_type="factual",
        category="technology",
        timestamp=1000.0,
        embedding=np.random.randn(128).tolist(),
    )

    doc = DocumentEvent(
        doc_id="doc_1",
        doc_text="Machine learning is a subset of AI",
        doc_category="technology",
        timestamp=1001.0,
        embedding=np.random.randn(128).tolist(),
    )

    joined = JoinedEvent(
        joined_id="query_1_1002.0",
        query=query,
        matched_docs=[doc],
        join_timestamp=1002.0,
        semantic_score=0.85,
    )

    print("âœ“ è¾“å…¥æ•°æ®:")
    print(f"  - Query ID: {joined.query.query_id}")
    print(f"  - Query Text: {joined.query.query_text}")
    print(f"  - Embedding ç»´åº¦: {len(joined.query.embedding)}")
    print()

    # æ³¨æ„: execute() éœ€è¦åœ¨å®é™… SAGE ç¯å¢ƒä¸­è°ƒç”¨æœåŠ¡
    print("âœ“ ç®—å­å¯ä»¥åœ¨ SAGE Pipeline ä¸­ä½¿ç”¨:")
    print("  graph_results = joined.map(GraphMemoryRetriever(...))")
    print()


def example_4_advanced_bfs_traversal():
    """ç¤ºä¾‹ 4: é«˜çº§ BFS éå†"""
    print("=" * 80)
    print("ç¤ºä¾‹ 4: é«˜çº§ BFS éå†ï¼ˆå±•ç¤ºè·¯å¾„ï¼‰")
    print("=" * 80)

    # åˆ›å»ºä¸€ä¸ªå°å‹çŸ¥è¯†å›¾
    np.random.seed(123)
    knowledge_base = []

    # åˆ›å»ºåˆ†å±‚ç»“æ„: æ ¹èŠ‚ç‚¹ -> å­èŠ‚ç‚¹ -> å¶å­èŠ‚ç‚¹
    levels = [
        ["root"],
        ["child_1", "child_2", "child_3"],
        ["leaf_1", "leaf_2", "leaf_3", "leaf_4"],
    ]

    idx = 0
    for level_idx, level in enumerate(levels):
        for node_name in level:
            embedding = np.random.randn(32).astype(np.float32)
            # åŒå±‚èŠ‚ç‚¹ç›¸ä¼¼åº¦é«˜ä¸€äº›
            if level_idx > 0:
                embedding += 0.3 * np.random.randn(32)
            embedding = embedding / np.linalg.norm(embedding)

            knowledge_base.append(
                {
                    "node_id": node_name,
                    "content": f"Content of {node_name}",
                    "embedding": embedding.tolist(),
                    "node_type": "level_" + str(level_idx),
                }
            )
            idx += 1

    # æ„å»ºæœåŠ¡
    service = GraphMemoryService(
        config={},
        embedding_dim=32,
        similarity_threshold=0.4,
    )
    service.build_graph(knowledge_base)

    print("âœ“ åˆ†å±‚çŸ¥è¯†å›¾:")
    print("  - ç¬¬ 0 å±‚ (root): 1 ä¸ªèŠ‚ç‚¹")
    print("  - ç¬¬ 1 å±‚ (child): 3 ä¸ªèŠ‚ç‚¹")
    print("  - ç¬¬ 2 å±‚ (leaf): 4 ä¸ªèŠ‚ç‚¹")
    print(f"  - æ€»è¾¹æ•°: {service.graph.number_of_edges()}")
    print()

    # ä»æ ¹èŠ‚ç‚¹å¼€å§‹éå†
    root_embedding = knowledge_base[0]["embedding"]

    results = service.search(
        query_embedding=root_embedding,
        max_depth=2,
        max_nodes=8,
        beam_width=3,
    )

    print("âœ“ BFS éå†ç»“æœ (ä» root å¼€å§‹):")
    for i, result in enumerate(results, 1):
        indent = "  " * result["depth"]
        path_str = " -> ".join(result["path"])
        print(
            f"{i}. {indent}{result['node_id']} "
            f"(æ·±åº¦={result['depth']}, åˆ†æ•°={result['relevance_score']:.3f})"
        )
        print(f"   {indent}è·¯å¾„: {path_str}")
    print()


def example_5_service_registration():
    """ç¤ºä¾‹ 5: æœåŠ¡æ³¨å†Œï¼ˆä¼ªä»£ç ï¼‰"""
    print("=" * 80)
    print("ç¤ºä¾‹ 5: åœ¨ SAGE ç¯å¢ƒä¸­æ³¨å†Œå›¾å†…å­˜æœåŠ¡")
    print("=" * 80)

    print("""
åœ¨å®é™… SAGE Pipeline ä¸­æ³¨å†ŒæœåŠ¡çš„æ­¥éª¤:

1. å‡†å¤‡çŸ¥è¯†åº“æ•°æ®:
   knowledge_base = [
       {"node_id": "...", "content": "...", "embedding": [...], ...},
       ...
   ]

2. åœ¨ RemoteEnvironment ä¸­æ³¨å†ŒæœåŠ¡:
   from workload4.graph_memory import register_graph_memory_service

   success = register_graph_memory_service(
       env=remote_env,
       knowledge_base=knowledge_base,
       embedding_dim=1024,
       similarity_threshold=0.7,
       service_name="graph_memory",
   )

3. åœ¨ Pipeline ä¸­ä½¿ç”¨ç®—å­:
   graph_results = joined_stream.map(
       GraphMemoryRetriever(
           max_depth=3,
           max_nodes=200,
           beam_width=10,
           service_name="graph_memory",
       )
   )

4. ç®—å­ä¼šè‡ªåŠ¨è°ƒç”¨æœåŠ¡:
   - call_service("graph_memory", "search", ...)
   - è¿”å› list[GraphMemoryResult]
    """)
    print()


if __name__ == "__main__":
    print("\n" + "ğŸ” Workload 4 å›¾éå†ç¤ºä¾‹" + "\n")

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    example_1_graph_service_basic()
    example_2_graph_with_documents()
    example_3_graph_retriever_operator()
    example_4_advanced_bfs_traversal()
    example_5_service_registration()

    print("=" * 80)
    print("âœ“ æ‰€æœ‰ç¤ºä¾‹æ‰§è¡Œå®Œæˆ")
    print("=" * 80)
