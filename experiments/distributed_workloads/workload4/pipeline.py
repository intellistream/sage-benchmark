"""
Workload 4 Pipeline Factory
============================

æ•´åˆæ‰€æœ‰ç®—å­ï¼Œæž„å»ºå®Œæ•´çš„ Workload 4 åˆ†å¸ƒå¼æ•°æ®æµã€‚

Pipeline ç»“æž„:
1. åŒæµæº(Query + Document)
2. Embedding é¢„è®¡ç®—
3. Semantic Join (60s å¤§çª—å£)
4. åŒè·¯ VDB æ£€ç´¢(4-stage each)
5. å›¾éåŽ†å†…å­˜æ£€ç´¢
6. ç»“æžœæ±‡èš
7. DBSCAN èšç±»åŽ»é‡
8. 5ç»´è¯„åˆ†é‡æŽ’åº
9. MMR å¤šæ ·æ€§è¿‡æ»¤
10. åŒå±‚ Batch èšåˆ
11. æ‰¹é‡ LLM ç”Ÿæˆ
12. Metrics Sink
"""

from __future__ import annotations

import time
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from sage.kernel.api.local_environment import LocalEnvironment
    from sage.kernel.api.remote_environment import RemoteEnvironment

try:
    # æµæ±‡èšå’Œåˆ†æµå·¥å…·
    from .aggregation import MergeVDBResultsJoin
    from .batching import CategoryBatchAggregator, GlobalBatchAggregator
    from .clustering import DBSCANClusteringOperator
    from .config import Workload4Config
    from .generation import BatchLLMGenerator, Workload4MetricsSink
    from .graph_memory import GraphMemoryRetriever
    from .models import (
        Workload4Metrics,
    )
    from .reranking import MMRDiversityFilter, MultiDimensionalReranker
    from .semantic_join import SemanticJoinOperator
    from .sources import (
        EmbeddingPrecompute,
        Workload4DocumentSource,
        Workload4QuerySource,
    )
    from .tag_utils import TagFilter, TagMapper
    from .vdb_retrieval import VDBRetriever
except ImportError:
    # æµæ±‡èšå’Œåˆ†æµå·¥å…·
    from aggregation import MergeVDBResultsJoin
    from batching import CategoryBatchAggregator, GlobalBatchAggregator
    from clustering import DBSCANClusteringOperator
    from generation import BatchLLMGenerator, Workload4MetricsSink
    from graph_memory import GraphMemoryRetriever

    # ðŸ”§ ä¸´æ—¶æ·»åŠ ï¼šå•æºæµ‹è¯•ç”¨çš„è½¬æ¢å™¨
    from models import (
        Workload4Metrics,
    )
    from reranking import MMRDiversityFilter, MultiDimensionalReranker
    from semantic_join import SemanticJoinOperator
    from sources import (
        EmbeddingPrecompute,
        Workload4DocumentSource,
        Workload4QuerySource,
    )
    from tag_utils import TagFilter, TagMapper
    from vdb_retrieval import (
        VDBRetriever,
    )

    from config import Workload4Config


# =============================================================================
# Service Registration
# =============================================================================


def register_embedding_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œ Embedding æœåŠ¡ã€‚

    ä½¿ç”¨è¿œç«¯ Embedding API(OpenAI å…¼å®¹)ã€‚
    """
    try:
        from .services import EmbeddingService

        env.register_service(
            "embedding",
            EmbeddingService,
            base_url=config.embedding_base_url,
            model=config.embedding_model,
        )

        print(f"âœ“ Registered embedding_service: {config.embedding_base_url}")
        return True

    except Exception as e:
        print(f"âœ— Failed to register embedding_service: {e}")
        return False


def register_vdb_services(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> dict[str, bool]:
    """
    æ³¨å†ŒåŒè·¯ VDB æœåŠ¡(vdb1 å’Œ vdb2)ã€‚

    ä½¿ç”¨çœŸå®žçš„ FiQA æ•°æ®é›†(57,638 æ–‡æ¡£ï¼Œ1024 ç»´)ã€‚
    vdb1 å’Œ vdb2 å…±äº«ç›¸åŒçš„ FAISS ç´¢å¼•(fiqa_faiss.index)ã€‚
    ç´¢å¼•å’Œæ–‡æ¡£å­˜å‚¨åœ¨ config.vdb_index_dirã€‚

    **æ•°æ®æº**ï¼š/home/sage/data/fiqa_faiss.index + fiqa_documents.jsonl
    """
    results = {}

    # Import Service class from module
    from .services import FAISSVDBService

    for vdb_name in ["vdb1", "vdb2"]:
        try:
            env.register_service(
                vdb_name,
                FAISSVDBService,
                vdb_name=vdb_name,
                dimension=config.embedding_dimension,
                index_dir=config.vdb_index_dir,
                dataset_name=config.vdb_dataset_name,
            )

            results[vdb_name] = True
            print(f"âœ“ Registered {vdb_name} (FiQA dataset, shared index)")

        except Exception as e:
            results[vdb_name] = False
            print(f"âœ— Failed to register {vdb_name}: {e}")
            import traceback

            traceback.print_exc()

    return results


def register_graph_memory_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œå›¾å†…å­˜æœåŠ¡ã€‚

    ä½¿ç”¨ Mock å›¾ç»“æž„æˆ– NeuroMem Graph backendã€‚
    """
    # Import Service class from module
    from .services import GraphMemoryService

    try:
        env.register_service(
            "graph_memory",
            GraphMemoryService,
            max_depth=config.graph_max_depth,
            max_nodes=config.graph_max_nodes,
        )

        print("âœ“ Registered graph_memory_service")
        return True

    except Exception as e:
        print(f"âœ— Failed to register graph_memory_service: {e}")
        return False


def register_llm_service(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> bool:
    """
    æ³¨å†Œ LLM æœåŠ¡ã€‚

    ä½¿ç”¨è¿œç«¯ LLM API(OpenAI å…¼å®¹)ã€‚
    """
    # Import Service class from module
    from .services import LLMService

    try:
        env.register_service(
            "llm",
            LLMService,
            base_url=config.llm_base_url,
            model=config.llm_model,
            max_tokens=config.llm_max_tokens,
        )

        print(f"âœ“ Registered llm_service: {config.llm_base_url}")
        return True

    except Exception as e:
        print(f"âœ— Failed to register llm_service: {e}")
        return False


def register_all_services(
    env: LocalEnvironment | RemoteEnvironment,
    config: Workload4Config,
) -> dict[str, bool]:
    """
    æ³¨å†Œæ‰€æœ‰å¿…è¦çš„ servicesã€‚

    Returns:
        æœåŠ¡æ³¨å†Œç»“æžœå­—å…¸
    """
    results = {}

    print("\n" + "=" * 80)
    print("Registering Workload 4 Services")
    print("=" * 80)

    # 1. Embedding Service
    results["embedding"] = register_embedding_service(env, config)

    # 2. VDB Services (vdb1, vdb2)
    vdb_results = register_vdb_services(env, config)
    results.update(vdb_results)

    # 3. Graph Memory Service
    results["graph_memory"] = register_graph_memory_service(env, config)

    # 4. LLM Service
    results["llm"] = register_llm_service(env, config)

    print("=" * 80)
    print(f"Service Registration Summary: {sum(results.values())}/{len(results)} successful")
    print("=" * 80)

    return results


# =============================================================================
# Pipeline Factory
# =============================================================================


class Workload4Pipeline:
    """
    Workload 4 Pipeline å·¥åŽ‚ã€‚

    æ•´åˆæ‰€æœ‰ç®—å­ï¼Œæž„å»ºå®Œæ•´çš„åˆ†å¸ƒå¼æ•°æ®æµã€‚
    """

    def __init__(self, config: Workload4Config):
        """
        åˆå§‹åŒ– Pipelineã€‚

        Args:
            config: Workload 4 é…ç½®
        """
        self.config = config
        self.env = None
        self.metrics = None

    def _create_environment(self, name: str):
        """åˆ›å»ºæ‰§è¡ŒçŽ¯å¢ƒ(æœ¬åœ°æˆ–è¿œç¨‹)"""
        if self.config.use_remote:
            from pathlib import Path

            from sage.kernel.api.remote_environment import RemoteEnvironment

            # workload4 æ‰€åœ¨ç›®å½•(å½“å‰æ–‡ä»¶çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•)
            workload_dir = str(Path(__file__).parent.parent)

            # RemoteEnvironment å‚æ•°ï¼šname, config, host, port, scheduler, extra_python_paths
            env = RemoteEnvironment(
                name=name,
                scheduler=self.config.scheduler_type,  # "fifo" æˆ– "load_aware"
                extra_python_paths=[workload_dir],  # è®©è¿œç¨‹èŠ‚ç‚¹èƒ½æ‰¾åˆ° workload4 æ¨¡å—
            )
            return env
        else:
            from sage.kernel.api.local_environment import LocalEnvironment

            return LocalEnvironment(name=name)

    def build(self, name: str = "workload4_benchmark") -> Workload4Pipeline:
        """
        æž„å»ºå®Œæ•´ pipelineã€‚

        Pipeline ç»“æž„:
        1. åŒæµæº(Query + Document)
        2. Embedding é¢„è®¡ç®—
        3. Semantic Join (60s å¤§çª—å£, parallelism=16)
        4. å›¾éåŽ†å†…å­˜æ£€ç´¢
        5. åŒè·¯ VDB æ£€ç´¢(4-stage each)
        6. æ±‡èšæ‰€æœ‰æ£€ç´¢ç»“æžœ
        7. DBSCAN èšç±»åŽ»é‡
        8. 5ç»´è¯„åˆ†é‡æŽ’åº
        9. MMR å¤šæ ·æ€§è¿‡æ»¤
        10. åŒå±‚ Batch èšåˆ
        11. æ‰¹é‡ LLM ç”Ÿæˆ
        12. Metrics Sink

        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        print("\n" + "=" * 80)
        print("Building Workload 4 Pipeline")
        print("=" * 80)
        print(f"Pipeline Name: {name}")
        print(f"Use Remote: {self.config.use_remote}")
        print(f"Num Nodes: {self.config.num_nodes}")
        print(f"Num Tasks: {self.config.num_tasks}")
        print(f"Duration: {self.config.duration}s")
        print("=" * 80)

        # === 1. åˆ›å»ºçŽ¯å¢ƒ ===
        # CRITICAL: Create environment in a local scope and build pipeline immediately
        # This avoids storing RemoteEnvironment in self, which causes serialization issues
        env = self._create_environment(name)
        print(f"âœ“ Created {'Remote' if self.config.use_remote else 'Local'}Environment")
        if self.config.use_remote:
            print(f"  Scheduler: {self.config.scheduler_type}")

        # === 2. æ³¨å†Œæ‰€æœ‰ services ===
        service_results = register_all_services(env, self.config)

        if not all(service_results.values()):
            failed = [k for k, v in service_results.items() if not v]
            print(f"âš ï¸  Some services failed to register: {failed}")
            print("   Pipeline may not work correctly.")

        # === 3. æž„å»ºåŒæµæº ===
        print("\n" + "=" * 80)
        print("Building Data Streams")
        print("=" * 80)

        # Query æµ(ä¼ å…¥ç±»è€Œéžå®žä¾‹)
        query_stream = env.from_source(
            Workload4QuerySource,
            num_tasks=self.config.num_tasks,
            qps=self.config.query_qps,
            query_types=list(self.config.query_type_distribution.keys()),
            categories=list(self.config.category_distribution.keys()),
            use_fiqa=False,  # å¯é…ç½®
        )
        print(f"âœ“ Created Query Stream (QPS={self.config.query_qps})")

        # Document æµ(ä¼ å…¥ç±»è€Œéžå®žä¾‹)
        doc_stream = env.from_source(
            Workload4DocumentSource,
            num_docs=self.config.num_tasks * 20,  # æ¯ä¸ªqueryå¯¹åº”20ä¸ªdoc
            qps=self.config.doc_qps,
            categories=list(self.config.category_distribution.keys()),
        )
        print(f"âœ“ Created Document Stream (QPS={self.config.doc_qps})")

        # === 4. Embedding é¢„è®¡ç®— ===
        query_stream = query_stream.map(
            EmbeddingPrecompute,
            embedding_base_url=self.config.embedding_base_url,
            embedding_model=self.config.embedding_model,
            batch_size=32,
            field_name="query_text",
        )
        print("âœ“ Added EmbeddingPrecompute for Query Stream")

        doc_stream = doc_stream.map(
            EmbeddingPrecompute,
            embedding_base_url=self.config.embedding_base_url,
            embedding_model=self.config.embedding_model,
            batch_size=32,
            field_name="doc_text",
        )
        print("âœ“ Added EmbeddingPrecompute for Document Stream")

        # === 5. Semantic Join ===
        print("\n" + "=" * 80)
        print("Building Semantic Join")
        print("=" * 80)

        # SemanticJoinOperator æ˜¯ BaseCoMapFunctionï¼Œä½¿ç”¨ comap è€Œä¸æ˜¯ join
        # å®šä¹‰å…¼å®¹ StopSignal çš„ key selector
        def joined_key_selector(x):
            """ä»Ž JoinedEvent æå– joined_idï¼Œå…¼å®¹ StopSignal"""
            from sage.kernel.runtime.communication.packet import StopSignal

            if isinstance(x, StopSignal):
                return x  # StopSignal ç›´æŽ¥è¿”å›žè‡ªèº«ï¼Œè®©å®ƒç»§ç»­ä¼ é€’
            return hash(x.joined_id) % self.config.join_parallelism

        joined_stream = query_stream.connect(doc_stream).comap(
            SemanticJoinOperator,
            window_seconds=self.config.join_window_seconds,
            threshold=self.config.join_threshold,
            max_matches=self.config.join_max_matches,
            batch_compute=True,
        )
        # .keyby(joined_key_selector)
        print(
            f"âœ“ Added Semantic Join (window={self.config.join_window_seconds}s, "
            f"threshold={self.config.join_threshold}, "
            f"parallelism={self.config.join_parallelism})"
        )

        # === 6. å›¾éåŽ†å†…å­˜æ£€ç´¢(ä¸²è¡Œç¬¬ä¸€æ­¥)===
        print("\n" + "=" * 80)
        print("Building Graph Memory Retrieval (ä¸²è¡Œç¬¬ä¸€æ­¥)")
        print("=" * 80)

        graph_stream = joined_stream.map(
            GraphMemoryRetriever,
            max_depth=self.config.graph_max_depth,
            max_nodes=self.config.graph_max_nodes,
            beam_width=self.config.graph_bfs_beam_width,
        )
        print(
            f"âœ“ Added Graph Memory Retrieval (max_depth={self.config.graph_max_depth}, "
            f"max_nodes={self.config.graph_max_nodes})"
        )

        # === 7. åŒè·¯ VDB æ£€ç´¢(ä½¿ç”¨ Tag+Filter æ¨¡å¼)===
        # Tag+Filter æ¨¡å¼ï¼šæ¯æ¡æ•°æ®éƒ½å¤åˆ¶åˆ°ä¸¤ä¸ªåˆ†æ”¯
        # graph_stream â†’ tag("vdb1"|"vdb2") â†’ filter(vdb1) â†’ VDBRetriever(vdb1)
        #                                    â†’ filter(vdb2) â†’ VDBRetriever(vdb2)
        print("\n" + "=" * 80)
        print("Building VDB Retrieval Branches (Tag+Filter æ¨¡å¼)")
        print("=" * 80)

        # ç»™æ¯æ¡æ•°æ®æ‰“ä¸Š vdb1 å’Œ vdb2 æ ‡ç­¾(flatmap ä¼šå¤åˆ¶ä¸¤ä»½)
        tagged_stream = graph_stream.flatmap(
            TagMapper,
            tags=["vdb1", "vdb2"],
        )
        print("âœ“ Added TagMapper (tags=['vdb1', 'vdb2'])")

        # VDB1 åˆ†æ”¯ï¼šè¿‡æ»¤å‡º tag=vdb1 çš„æ•°æ®
        vdb1_filtered = tagged_stream.filter(
            TagFilter,
            target_tag="vdb1",
        ).sink(
            Workload4MetricsSink,
            metrics_output_dir=self.config.metrics_output_dir,
            verbose=True,
        )
        self.env = env
        return self
        vdb1_stream = vdb1_filtered.map(
            VDBRetriever,
            vdb_name="vdb1",
            top_k=self.config.vdb1_top_k,
            stage=1,
        )
        print(f"âœ“ Added VDB1 Branch (tag=vdb1, top_k={self.config.vdb1_top_k})")

        # VDB2 åˆ†æ”¯ï¼šè¿‡æ»¤å‡º tag=vdb2 çš„æ•°æ®
        vdb2_filtered = tagged_stream.filter(
            TagFilter,
            target_tag="vdb2",
        )
        vdb2_stream = vdb2_filtered.map(
            VDBRetriever,
            vdb_name="vdb2",
            top_k=self.config.vdb2_top_k,
            stage=1,
        )
        print(f"âœ“ Added VDB2 Branch (tag=vdb2, top_k={self.config.vdb2_top_k})")

        # === 8. æ±‡èšæ‰€æœ‰æ£€ç´¢ç»“æžœ(ä½¿ç”¨ keyby + join)===
        # çŽ°åœ¨åªéœ€è¦åˆå¹¶ VDB1 + VDB2(graph å·²ç»åœ¨ä¸Šæ¸¸)
        # æµç¨‹ï¼šVDB1 + VDB2 â†’ Join â†’ åˆå¹¶ç»“æžœ
        print("\n" + "=" * 80)
        print("Building Result Aggregation (keyby + join, VDB1 + VDB2)")
        print("=" * 80)

        # å®šä¹‰å…¼å®¹ StopSignal çš„ key selector
        def vdb_key_selector(x):
            """ä»Ž VDBResultsWrapper æå– query_idï¼Œå…¼å®¹ StopSignal"""
            from sage.kernel.runtime.communication.packet import StopSignal

            if isinstance(x, StopSignal):
                return x  # StopSignal ç›´æŽ¥è¿”å›žè‡ªèº«ï¼Œè®©å®ƒç»§ç»­ä¼ é€’
            return x.query_id  # VDBResultsWrapper æœ‰ query_id å­—æ®µ

        # VDB1 å’Œ VDB2 ç»“æžœåˆå¹¶(æŒ‰ query_id)
        # æ³¨æ„ï¼šå› ä¸º graph åœ¨ä¸Šæ¸¸ä¸²è¡Œï¼ŒVDB ç»“æžœå·²ç»åŒ…å« graph ä¿¡æ¯
        vdb1_keyed = vdb1_stream.keyby(vdb_key_selector)
        vdb2_keyed = vdb2_stream.keyby(vdb_key_selector)

        all_results = vdb1_keyed.connect(vdb2_keyed).join(
            MergeVDBResultsJoin,
            parallelism=self.config.join_parallelism,
        )
        print("âœ“ Added VDB1+VDB2 Join (graph å·²åœ¨ä¸Šæ¸¸ä¸²è¡Œæ‰§è¡Œ)")

        # === 9. DBSCAN èšç±»åŽ»é‡ ===
        print("\n" + "=" * 80)
        print("Building Clustering & Deduplication")
        print("=" * 80)

        deduplicated_stream = all_results.map(
            DBSCANClusteringOperator,
            eps=self.config.dbscan_eps,
            min_samples=self.config.dbscan_min_samples,
            metric="cosine",
        )
        print(
            f"âœ“ Added DBSCAN Clustering (eps={self.config.dbscan_eps}, "
            f"min_samples={self.config.dbscan_min_samples})"
        )

        # === 10. 5ç»´è¯„åˆ†é‡æŽ’åº ===
        print("\n" + "=" * 80)
        print("Building Reranking")
        print("=" * 80)

        reranked_stream = deduplicated_stream.map(
            MultiDimensionalReranker,
            score_weights=self.config.rerank_score_weights,
            top_k=self.config.rerank_top_k,
        )
        print(
            f"âœ“ Added MultiDimensional Reranking (5 dimensions, top_k={self.config.rerank_top_k})"
        )

        # === 11. MMR å¤šæ ·æ€§è¿‡æ»¤ ===
        reranked_stream = reranked_stream.map(
            MMRDiversityFilter,
            lambda_param=self.config.mmr_lambda,
            top_k=self.config.rerank_top_k,
        )
        print(f"âœ“ Added MMR Diversity Filter (lambda={self.config.mmr_lambda})")

        # === 12. åŒå±‚ Batch èšåˆ ===
        print("\n" + "=" * 80)
        print("Building Batch Aggregation")
        print("=" * 80)

        # ç¬¬ä¸€å±‚: Category Batch
        category_batched = reranked_stream.keyby(
            lambda x: x.query.category  # æŒ‰ category åˆ†ç»„
        ).map(
            CategoryBatchAggregator,
            batch_size=self.config.category_batch_size,
            timeout_ms=self.config.category_batch_timeout_ms,
        )
        print(
            f"âœ“ Added Category Batch (size={self.config.category_batch_size}, "
            f"timeout={self.config.category_batch_timeout_ms}ms)"
        )

        # ç¬¬äºŒå±‚: Global Batch
        global_batched = category_batched.map(
            GlobalBatchAggregator,
            batch_size=self.config.global_batch_size,
            timeout_ms=self.config.global_batch_timeout_ms,
        )
        print(
            f"âœ“ Added Global Batch (size={self.config.global_batch_size}, "
            f"timeout={self.config.global_batch_timeout_ms}ms)"
        )

        # === 13. æ‰¹é‡ LLM ç”Ÿæˆ ===
        print("\n" + "=" * 80)
        print("Building LLM Generation")
        print("=" * 80)

        generated_stream = global_batched.map(
            BatchLLMGenerator,
            llm_base_url=self.config.llm_base_url,
            llm_model=self.config.llm_model,
            max_tokens=self.config.llm_max_tokens,
        )
        print(f"âœ“ Added Batch LLM Generator (model={self.config.llm_model})")

        # === 14. Metrics Sink ===
        print("\n" + "=" * 80)
        print("Building Metrics Sink")
        print("=" * 80)

        generated_stream.sink(
            Workload4MetricsSink,
            metrics_output_dir=self.config.metrics_output_dir,
            verbose=True,
        )
        print(f"âœ“ Added Metrics Sink (output_dir={self.config.metrics_output_dir})")

        print("\n" + "=" * 80)
        print("Pipeline Build Complete")
        print("=" * 80)

        # Store environment for run()
        self.env = env

        return self

    def run(self) -> Workload4Metrics:
        """
        æ‰§è¡Œ pipelineã€‚

        Returns:
            Workload4Metrics: æ±‡æ€»æŒ‡æ ‡
        """
        if self.env is None:
            raise RuntimeError("Pipeline not built. Call build() first.")

        print("\n" + "=" * 80)
        print("Starting Workload 4 Execution")
        print("=" * 80)
        print(f"Duration: {self.config.duration}s")
        print(f"Expected Tasks: {self.config.num_tasks}")
        print("=" * 80)

        start_time = time.time()

        try:
            # æ‰§è¡Œ pipeline
            self.env.submit(autostop=True)

            end_time = time.time()
            elapsed = end_time - start_time

            print("\n" + "=" * 80)
            print("Workload 4 Execution Complete")
            print("=" * 80)
            print(f"Elapsed Time: {elapsed:.2f}s")
            print("=" * 80)

            # æ”¶é›†æ±‡æ€»æŒ‡æ ‡
            # TODO: ä»Ž Sink æ”¶é›†è¯¦ç»†æŒ‡æ ‡
            # Issue URL: https://github.com/intellistream/SAGE/issues/1424
            self.metrics = Workload4Metrics(
                task_id="summary",
                query_id="summary",
                query_arrival_time=start_time,
                doc_arrival_time=start_time,
                join_time=0.0,
                vdb1_start_time=0.0,
                vdb1_end_time=0.0,
                vdb2_start_time=0.0,
                vdb2_end_time=0.0,
                graph_start_time=0.0,
                graph_end_time=0.0,
                clustering_time=0.0,
                reranking_time=0.0,
                batch_time=0.0,
                generation_time=0.0,
                end_to_end_time=elapsed,
                join_matched_docs=0,
                vdb1_results=0,
                vdb2_results=0,
                graph_nodes_visited=0,
                clusters_found=0,
                duplicates_removed=0,
                final_top_k=0,
                cpu_time=0.0,
                memory_peak_mb=0.0,
            )

            return self.metrics

        except Exception as e:
            print(f"\nâœ— Pipeline execution failed: {e}")
            import traceback

            traceback.print_exc()
            raise


# =============================================================================
# Convenience Functions
# =============================================================================


def create_workload4_pipeline(
    config: Workload4Config | None = None, **config_overrides
) -> Workload4Pipeline:
    """
    åˆ›å»º Workload 4 Pipeline(ä¾¿æ·å‡½æ•°)ã€‚

    Args:
        config: Workload4Config å®žä¾‹(å¯é€‰)
        **config_overrides: è¦†ç›–é…ç½®é¡¹

    Returns:
        Workload4Pipeline å®žä¾‹

    Example:
        >>> pipeline = create_workload4_pipeline(num_tasks=50, duration=600)
        >>> pipeline.build().run()
    """
    if config is None:
        config = Workload4Config()

    # åº”ç”¨è¦†ç›–
    for key, value in config_overrides.items():
        if hasattr(config, key):
            setattr(config, key, value)
        else:
            print(f"âš ï¸  Unknown config key: {key}")

    return Workload4Pipeline(config)


def run_workload4(config: Workload4Config | None = None, **config_overrides) -> Workload4Metrics:
    """
    ä¸€é”®è¿è¡Œ Workload 4(ä¾¿æ·å‡½æ•°)ã€‚

    Args:
        config: Workload4Config å®žä¾‹(å¯é€‰)
        **config_overrides: è¦†ç›–é…ç½®é¡¹

    Returns:
        Workload4Metrics: æ±‡æ€»æŒ‡æ ‡

    Example:
        >>> metrics = run_workload4(num_tasks=100, use_remote=True)
    """
    pipeline = create_workload4_pipeline(config, **config_overrides)
    pipeline.build()
    return pipeline.run()
