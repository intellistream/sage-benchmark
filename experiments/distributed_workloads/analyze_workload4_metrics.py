#!/usr/bin/env python3
"""
Workload 4 Metrics Analysis Script

ç”¨äºåˆ†æ Workload 4 åŸºå‡†æµ‹è¯•çš„æ€§èƒ½æŒ‡æ ‡ã€‚

ç”¨æ³•:
    python analyze_workload4_metrics.py /tmp/sage_metrics_workload4/metrics.csv
    python analyze_workload4_metrics.py /tmp/sage_metrics_workload4/metrics.csv --output report.html
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any

import pandas as pd


def load_metrics(metrics_path: Path) -> pd.DataFrame:
    """åŠ è½½æŒ‡æ ‡ CSV æ–‡ä»¶"""
    if not metrics_path.exists():
        print(f"âŒ æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {metrics_path}")
        sys.exit(1)

    df = pd.read_csv(metrics_path)
    print(f"âœ… å·²åŠ è½½ {len(df)} æ¡è®°å½•")
    return df


def compute_latency_stats(df: pd.DataFrame) -> dict[str, Any]:
    """è®¡ç®—å»¶è¿Ÿç»Ÿè®¡"""
    latencies = df["end_to_end_time"] * 1000  # è½¬æ¢ä¸º ms

    stats = {
        "count": len(latencies),
        "mean": latencies.mean(),
        "std": latencies.std(),
        "min": latencies.min(),
        "p50": latencies.quantile(0.50),
        "p90": latencies.quantile(0.90),
        "p95": latencies.quantile(0.95),
        "p99": latencies.quantile(0.99),
        "max": latencies.max(),
    }

    return stats


def compute_stage_stats(df: pd.DataFrame) -> dict[str, dict[str, float]]:
    """è®¡ç®—å„ stage å»¶è¿Ÿç»Ÿè®¡"""
    stages = {
        "query_embedding": df["query_embedding_time"],
        "doc_embedding": df["doc_embedding_time"],
        "semantic_join": df["join_time"] - df["query_arrival_time"],
        "vdb1_retrieval": df["vdb1_end_time"] - df["vdb1_start_time"],
        "vdb2_retrieval": df["vdb2_end_time"] - df["vdb2_start_time"],
        "graph_memory": df["graph_end_time"] - df["graph_start_time"],
        "clustering": df["clustering_time"],
        "reranking": df["reranking_time"],
        "batch_wait": df["batch_time"],
        "generation": df["generation_time"],
    }

    stats = {}
    for stage_name, stage_times in stages.items():
        stage_times_ms = stage_times * 1000
        stats[stage_name] = {
            "p50": stage_times_ms.quantile(0.50),
            "p95": stage_times_ms.quantile(0.95),
            "p99": stage_times_ms.quantile(0.99),
            "mean": stage_times_ms.mean(),
        }

    return stats


def compute_resource_stats(df: pd.DataFrame) -> dict[str, Any]:
    """è®¡ç®—èµ„æºä½¿ç”¨ç»Ÿè®¡"""
    stats = {
        "cpu_time": {
            "mean": df["cpu_time"].mean(),
            "max": df["cpu_time"].max(),
        },
        "memory_peak_mb": {
            "mean": df["memory_peak_mb"].mean(),
            "max": df["memory_peak_mb"].max(),
        },
    }

    return stats


def compute_quality_stats(df: pd.DataFrame) -> dict[str, Any]:
    """è®¡ç®—è´¨é‡æŒ‡æ ‡ç»Ÿè®¡"""
    stats = {
        "join_success_rate": (df["join_matched_docs"] > 0).mean() * 100,
        "avg_matched_docs": df["join_matched_docs"].mean(),
        "avg_vdb1_results": df["vdb1_results"].mean(),
        "avg_vdb2_results": df["vdb2_results"].mean(),
        "avg_graph_nodes": df["graph_nodes_visited"].mean(),
        "avg_clusters": df["clusters_found"].mean(),
        "dedup_rate": (df["duplicates_removed"] / (df["vdb1_results"] + df["vdb2_results"])).mean()
        * 100,
        "avg_final_topk": df["final_top_k"].mean(),
    }

    return stats


def print_summary(stats: dict[str, Any]) -> None:
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("Workload 4 Performance Summary")
    print("=" * 80)

    # å»¶è¿Ÿç»Ÿè®¡
    latency = stats["latency"]
    print("\nğŸ“Š å»¶è¿Ÿç»Ÿè®¡ (ms):")
    print(f"  æ€»ä»»åŠ¡æ•°: {latency['count']}")
    print(f"  å¹³å‡å»¶è¿Ÿ: {latency['mean']:.1f} ms")
    print(f"  æ ‡å‡†å·®:   {latency['std']:.1f} ms")
    print(f"  P50:      {latency['p50']:.1f} ms")
    print(f"  P90:      {latency['p90']:.1f} ms")
    print(f"  P95:      {latency['p95']:.1f} ms")
    print(f"  P99:      {latency['p99']:.1f} ms")
    print(f"  æœ€å¤§:     {latency['max']:.1f} ms")

    # Stage ç»Ÿè®¡
    print("\nâ±ï¸  å„ Stage å»¶è¿Ÿ (P50/P95/P99 ms):")
    for stage_name, stage_stats in stats["stages"].items():
        print(
            f"  {stage_name:20s}: "
            f"{stage_stats['p50']:6.1f} / {stage_stats['p95']:6.1f} / {stage_stats['p99']:6.1f}"
        )

    # èµ„æºç»Ÿè®¡
    resource = stats["resource"]
    print("\nğŸ’» èµ„æºä½¿ç”¨:")
    print(f"  å¹³å‡ CPU æ—¶é—´: {resource['cpu_time']['mean']:.2f} s")
    print(f"  å³°å€¼ CPU æ—¶é—´: {resource['cpu_time']['max']:.2f} s")
    print(f"  å¹³å‡å†…å­˜å³°å€¼: {resource['memory_peak_mb']['mean']:.1f} MB")
    print(f"  æœ€å¤§å†…å­˜å³°å€¼: {resource['memory_peak_mb']['max']:.1f} MB")

    # è´¨é‡ç»Ÿè®¡
    quality = stats["quality"]
    print("\nâœ… è´¨é‡æŒ‡æ ‡:")
    print(f"  Join æˆåŠŸç‡:    {quality['join_success_rate']:.1f}%")
    print(f"  å¹³å‡åŒ¹é…æ–‡æ¡£:   {quality['avg_matched_docs']:.1f}")
    print(f"  å¹³å‡ VDB1 ç»“æœ: {quality['avg_vdb1_results']:.1f}")
    print(f"  å¹³å‡ VDB2 ç»“æœ: {quality['avg_vdb2_results']:.1f}")
    print(f"  å¹³å‡å›¾èŠ‚ç‚¹:     {quality['avg_graph_nodes']:.1f}")
    print(f"  å¹³å‡èšç±»æ•°:     {quality['avg_clusters']:.1f}")
    print(f"  å»é‡ç‡:         {quality['dedup_rate']:.1f}%")
    print(f"  æœ€ç»ˆ Top-K:     {quality['avg_final_topk']:.1f}")

    print("\n" + "=" * 80)


def generate_html_report(stats: dict[str, Any], output_path: Path) -> None:
    """ç”Ÿæˆ HTML æŠ¥å‘Š"""
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Workload 4 Performance Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        h1 {{ color: #333; }}
        h2 {{ color: #555; margin-top: 30px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #4CAF50; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .metric {{ font-weight: bold; }}
    </style>
</head>
<body>
    <h1>Workload 4 Performance Report</h1>

    <h2>å»¶è¿Ÿç»Ÿè®¡</h2>
    <table>
        <tr><th>æŒ‡æ ‡</th><th>å€¼ (ms)</th></tr>
        <tr><td>æ€»ä»»åŠ¡æ•°</td><td>{stats["latency"]["count"]}</td></tr>
        <tr><td>å¹³å‡å»¶è¿Ÿ</td><td>{stats["latency"]["mean"]:.1f}</td></tr>
        <tr><td>P50</td><td>{stats["latency"]["p50"]:.1f}</td></tr>
        <tr><td>P95</td><td>{stats["latency"]["p95"]:.1f}</td></tr>
        <tr><td>P99</td><td>{stats["latency"]["p99"]:.1f}</td></tr>
    </table>

    <h2>å„ Stage å»¶è¿Ÿ</h2>
    <table>
        <tr><th>Stage</th><th>P50 (ms)</th><th>P95 (ms)</th><th>P99 (ms)</th></tr>
"""

    for stage_name, stage_stats in stats["stages"].items():
        html += f"""        <tr>
            <td>{stage_name}</td>
            <td>{stage_stats["p50"]:.1f}</td>
            <td>{stage_stats["p95"]:.1f}</td>
            <td>{stage_stats["p99"]:.1f}</td>
        </tr>
"""

    html += """    </table>

    <h2>èµ„æºä½¿ç”¨</h2>
    <table>
        <tr><th>æŒ‡æ ‡</th><th>å¹³å‡å€¼</th><th>æœ€å¤§å€¼</th></tr>
        <tr>
            <td>CPU æ—¶é—´ (s)</td>
            <td>{:.2f}</td>
            <td>{:.2f}</td>
        </tr>
        <tr>
            <td>å†…å­˜å³°å€¼ (MB)</td>
            <td>{:.1f}</td>
            <td>{:.1f}</td>
        </tr>
    </table>

    <h2>è´¨é‡æŒ‡æ ‡</h2>
    <table>
        <tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>
        <tr><td>Join æˆåŠŸç‡</td><td>{:.1f}%</td></tr>
        <tr><td>å¹³å‡åŒ¹é…æ–‡æ¡£</td><td>{:.1f}</td></tr>
        <tr><td>å»é‡ç‡</td><td>{:.1f}%</td></tr>
    </table>
</body>
</html>
""".format(
        stats["resource"]["cpu_time"]["mean"],
        stats["resource"]["cpu_time"]["max"],
        stats["resource"]["memory_peak_mb"]["mean"],
        stats["resource"]["memory_peak_mb"]["max"],
        stats["quality"]["join_success_rate"],
        stats["quality"]["avg_matched_docs"],
        stats["quality"]["dedup_rate"],
    )

    output_path.write_text(html)
    print(f"âœ… HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Analyze Workload 4 metrics")
    parser.add_argument("metrics_file", type=Path, help="Path to metrics CSV file")
    parser.add_argument("--output", type=Path, help="Output HTML report path")
    parser.add_argument("--json", action="store_true", help="Output JSON format")

    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    df = load_metrics(args.metrics_file)

    # è®¡ç®—ç»Ÿè®¡
    stats = {
        "latency": compute_latency_stats(df),
        "stages": compute_stage_stats(df),
        "resource": compute_resource_stats(df),
        "quality": compute_quality_stats(df),
    }

    # æ‰“å°æ‘˜è¦
    print_summary(stats)

    # JSON è¾“å‡º
    if args.json:
        json_output = args.metrics_file.parent / "analysis.json"
        json_output.write_text(json.dumps(stats, indent=2))
        print(f"âœ… JSON æŠ¥å‘Šå·²ç”Ÿæˆ: {json_output}")

    # HTML è¾“å‡º
    if args.output:
        generate_html_report(stats, args.output)


if __name__ == "__main__":
    main()
