"""
Pipeline B: Long Context Refiner (é•¿æ–‡æœ¬ç²¾ç‚¼)
============================================

ä½¿ç”¨ LLMLingua-2 å‹ç¼©ç®—æ³•çš„ RAG Pipelineã€‚
LLMLingua-2 åŸºäº BERT token åˆ†ç±»ï¼Œæ¯” LLM-based æ–¹æ³•å¿«å¾—å¤šã€‚

æ‹“æ‰‘: Source â†’ Retriever â†’ LLMLingua2 â†’ Promptor â†’ Generator â†’ Evaluators â†’ Sink

ç‰¹ç‚¹:
    - å¿«é€Ÿå‹ç¼©ï¼šä½¿ç”¨ BERT æ¨¡å‹è¿›è¡Œ token åˆ†ç±»ï¼Œæ— éœ€ LLM æ¨ç†
    - å¤šè¯­è¨€æ”¯æŒï¼šä½¿ç”¨ mBERT æˆ– XLM-RoBERTa æ¨¡å‹
    - Token çº§ç²¾ç¡®å‹ç¼©ï¼šæ¯ä¸ª token ç‹¬ç«‹åˆ†ç±»
    - å¯é€‰çš„ä¸Šä¸‹æ–‡çº§è¿‡æ»¤ï¼šç²—åˆ°ç»†çš„å‹ç¼©ç­–ç•¥
    - ä½¿ç”¨ register_service é¿å…åˆ†å¸ƒå¼è®¿é—®é—®é¢˜

å‚è€ƒè®ºæ–‡: https://arxiv.org/abs/2403.12968
æ•°æ®é›†: HuggingFace FlashRAG Datasets (NQ)
"""

from __future__ import annotations

import os
import sys
import time
from dataclasses import dataclass
from typing import Any, Optional

import numpy as np

from sage.common.core.functions import MapFunction, SinkFunction, SourceFunction
from sage.common.utils.config.loader import load_config
from sage.common.utils.logging.custom_logger import CustomLogger
from sage.kernel.api.local_environment import LocalEnvironment


@dataclass
class RefinerConfig:
    """Refiner Pipeline é…ç½®"""

    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path: str = ""

    # è¿è¡Œæ—¶é…ç½®
    enable_profile: bool = True
    pipeline_timeout: float = 3600.0  # 1 hour for processing


# ============================================================================
# Service Registration Helpers
# ============================================================================


def register_embedding_service(
    env: LocalEnvironment,
    model_name: str = "BAAI/bge-large-en-v1.5",
    device: str = "cuda:0",
) -> bool:
    """
    æ³¨å†Œ Embedding æœåŠ¡ï¼Œç”¨äºæ£€ç´¢æ—¶çš„å‘é‡åŒ–ã€‚

    ä½¿ç”¨ UnifiedInferenceClient æˆ– EmbeddingFactoryã€‚
    """
    try:
        from sage.common.components.sage_embedding import EmbeddingClientAdapter, EmbeddingFactory

        # åˆ›å»º embedding æœåŠ¡åŒ…è£…ç±»
        class EmbeddingService:
            """Embedding æœåŠ¡å°è£…"""

            def __init__(self, model_name: str, device: str, **kwargs):
                self.model_name = model_name
                self.device = device
                self._embedder = None
                self._client = None

            def _ensure_initialized(self):
                if self._embedder is None:
                    raw_embedder = EmbeddingFactory.create(
                        "hf", model=self.model_name, device=self.device
                    )
                    self._client = EmbeddingClientAdapter(raw_embedder)
                    self._embedder = raw_embedder

            def embed(self, texts: list[str]) -> list[list[float]]:
                """æ‰¹é‡ embedding"""
                self._ensure_initialized()
                return self._client.embed(texts)

            def embed_single(self, text: str) -> list[float]:
                """å•ä¸ª embedding"""
                self._ensure_initialized()
                return self._embedder.embed(text)

        env.register_service(
            "embedding_service",
            EmbeddingService,
            model_name=model_name,
            device=device,
        )
        print(f"[Pipeline] Registered embedding_service (model={model_name})")
        return True
    except ImportError as e:
        print(f"[Pipeline] Embedding service not available: {e}")
        return False
    except Exception as e:
        print(f"[Pipeline] Failed to register embedding_service: {e}")
        return False


def register_vector_db_service(
    env: LocalEnvironment,
    index_path: str,
    documents_path: str,
    mapping_path: Optional[str] = None,
    dimension: int = 1024,
) -> bool:
    """
    æ³¨å†Œ Vector DB æœåŠ¡ï¼Œé¢„åŠ è½½ FAISS ç´¢å¼•ã€‚

    é¿å…åˆ†å¸ƒå¼è®¿é—®é—®é¢˜ï¼šåœ¨ Head èŠ‚ç‚¹åŠ è½½ä¸€æ¬¡ï¼Œé€šè¿‡ call_service è®¿é—®ã€‚
    """
    try:
        import json

        import faiss

        class FAISSVectorDBService:
            """FAISS Vector DB æœåŠ¡å°è£…"""

            def __init__(
                self,
                index_path: str,
                documents_path: str,
                mapping_path: Optional[str] = None,
                dimension: int = 1024,
                **kwargs,
            ):
                self.index_path = index_path
                self.documents_path = documents_path
                self.mapping_path = mapping_path
                self.dimension = dimension
                self._index = None
                self._documents = None
                self._mapping = None

            def _ensure_initialized(self):
                if self._index is not None:
                    return

                print(f"[VectorDB] Loading FAISS index from {self.index_path}")
                self._index = faiss.read_index(self.index_path)
                print(f"[VectorDB] Index loaded: {self._index.ntotal} vectors")

                print(f"[VectorDB] Loading documents from {self.documents_path}")
                self._documents = []
                with open(self.documents_path, encoding="utf-8") as f:
                    for line in f:
                        self._documents.append(json.loads(line.strip()))
                print(f"[VectorDB] Loaded {len(self._documents)} documents")

                if self.mapping_path and os.path.exists(self.mapping_path):
                    with open(self.mapping_path, encoding="utf-8") as f:
                        self._mapping = json.load(f)
                    print(f"[VectorDB] Loaded mapping with {len(self._mapping)} entries")

            def search(self, query_vector: np.ndarray, top_k: int = 100) -> list[dict[str, Any]]:
                """æœç´¢æœ€è¿‘é‚»"""
                self._ensure_initialized()

                if query_vector.ndim == 1:
                    query_vector = query_vector.reshape(1, -1)

                query_vector = query_vector.astype(np.float32)
                distances, indices = self._index.search(query_vector, top_k)

                results = []
                for i, idx in enumerate(indices[0]):
                    if idx < 0 or idx >= len(self._documents):
                        continue
                    doc = self._documents[idx].copy()
                    doc["score"] = float(distances[0][i])
                    doc["index"] = int(idx)
                    results.append(doc)

                return results

        env.register_service(
            "vector_db",
            FAISSVectorDBService,
            index_path=index_path,
            documents_path=documents_path,
            mapping_path=mapping_path,
            dimension=dimension,
        )
        print(f"[Pipeline] Registered vector_db (FAISS index={index_path})")
        return True
    except ImportError as e:
        print(f"[Pipeline] FAISS not available: {e}")
        return False
    except Exception as e:
        print(f"[Pipeline] Failed to register vector_db: {e}")
        return False


def register_refiner_service(
    env: LocalEnvironment,
    model_name: str = "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    device: str = "cuda:0",
    rate: float = 0.5,
) -> bool:
    """
    æ³¨å†Œ LLMLingua-2 Refiner æœåŠ¡ã€‚

    åœ¨ Head èŠ‚ç‚¹åŠ è½½æ¨¡å‹ï¼Œé¿å…åˆ†å¸ƒå¼åŠ è½½é—®é¢˜ã€‚
    """
    try:
        from sage.middleware.components.sage_refiner import LLMLingua2Compressor

        class RefinerService:
            """LLMLingua-2 Refiner æœåŠ¡å°è£…"""

            def __init__(
                self,
                model_name: str,
                device: str = "cuda:0",
                rate: float = 0.5,
                **kwargs,
            ):
                self.model_name = model_name
                self.device = device
                self.default_rate = rate
                self._compressor = None

            def _ensure_initialized(self):
                if self._compressor is not None:
                    return

                print(f"[Refiner] Loading LLMLingua-2 model: {self.model_name}")
                self._compressor = LLMLingua2Compressor(
                    model_name=self.model_name,
                    device=self.device,
                )
                print("[Refiner] Model loaded successfully")

            def compress(
                self,
                context: list[str] | str,
                rate: Optional[float] = None,
                **kwargs,
            ) -> dict[str, Any]:
                """å‹ç¼©ä¸Šä¸‹æ–‡"""
                self._ensure_initialized()
                return self._compressor.compress(
                    context=context,
                    rate=rate or self.default_rate,
                    **kwargs,
                )

        env.register_service(
            "refiner_service",
            RefinerService,
            model_name=model_name,
            device=device,
            rate=rate,
        )
        print(f"[Pipeline] Registered refiner_service (model={model_name})")
        return True
    except ImportError as e:
        print(f"[Pipeline] LLMLingua-2 not available: {e}")
        return False
    except Exception as e:
        print(f"[Pipeline] Failed to register refiner_service: {e}")
        return False


def register_llm_service(
    env: LocalEnvironment,
    base_url: str = "http://localhost:8001/v1",
    model_name: str = "Qwen/Qwen2.5-7B-Instruct",
    api_key: str = "",
) -> bool:
    """
    æ³¨å†Œ LLM æœåŠ¡ã€‚
    """
    try:
        import httpx

        class LLMService:
            """LLM æœåŠ¡å°è£…"""

            def __init__(
                self,
                base_url: str,
                model_name: str,
                api_key: str = "",
                **kwargs,
            ):
                self.base_url = base_url.rstrip("/")
                self.model_name = model_name
                self.api_key = api_key

            def chat(
                self,
                messages: list[dict[str, str]],
                max_tokens: int = 512,
                temperature: float = 0.7,
                **kwargs,
            ) -> str:
                """Chat completion"""
                headers = {"Content-Type": "application/json"}
                if self.api_key:
                    headers["Authorization"] = f"Bearer {self.api_key}"

                with httpx.Client(timeout=120.0) as client:
                    response = client.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json={
                            "model": self.model_name,
                            "messages": messages,
                            "max_tokens": max_tokens,
                            "temperature": temperature,
                            **kwargs,
                        },
                    )
                    response.raise_for_status()
                    result = response.json()

                return result["choices"][0]["message"]["content"]

        env.register_service(
            "llm_service",
            LLMService,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
        )
        print(f"[Pipeline] Registered llm_service (base_url={base_url})")
        return True
    except Exception as e:
        print(f"[Pipeline] Failed to register llm_service: {e}")
        return False


# ============================================================================
# Pipeline Operators (ä½¿ç”¨ call_service è®¿é—®æœåŠ¡)
# ============================================================================


class DataSourceOperator(SourceFunction):
    """æ•°æ®æº Operator: ä» HuggingFace åŠ è½½æ•°æ®é›†"""

    def __init__(
        self,
        dataset_name: str = "RUC-NLPIR/FlashRAG_datasets",
        dataset_config: str = "nq",
        split: str = "test",
        max_samples: int = 20,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.dataset_name = dataset_name
        self.dataset_config = dataset_config
        self.split = split
        self.max_samples = max_samples
        self._data = None
        self._index = 0

    def _load_data(self):
        if self._data is not None:
            return

        from datasets import load_dataset

        print(f"[Source] Loading dataset: {self.dataset_name}/{self.dataset_config}")
        dataset = load_dataset(
            self.dataset_name,
            self.dataset_config,
            split=self.split,
        )

        self._data = list(dataset.select(range(min(self.max_samples, len(dataset)))))
        print(f"[Source] Loaded {len(self._data)} samples")

    def execute(self, data: Any = None) -> Optional[dict]:
        self._load_data()

        if self._index >= len(self._data):
            return None

        sample = self._data[self._index]
        self._index += 1

        return {
            "id": sample.get("id", self._index),
            "query": sample.get("question", sample.get("query", "")),
            "ground_truth": sample.get("answer", sample.get("answers", [])),
        }


class RetrieverOperator(MapFunction):
    """æ£€ç´¢ Operator: ä½¿ç”¨æ³¨å†Œçš„æœåŠ¡è¿›è¡Œæ£€ç´¢"""

    def __init__(self, top_k: int = 100, **kwargs):
        super().__init__(**kwargs)
        self.top_k = top_k

    def execute(self, data: dict) -> dict:
        query = data.get("query", "")

        # è·å– embedding
        embedding_service = self.call_service("embedding_service")
        query_embedding = embedding_service.embed_single(query)
        query_vector = np.array(query_embedding, dtype=np.float32)

        # æ£€ç´¢
        vector_db = self.call_service("vector_db")
        results = vector_db.search(query_vector, top_k=self.top_k)

        data["retrieval_results"] = results
        data["query_embedding"] = query_embedding
        print(f"[Retriever] Query: {query[:50]}... -> {len(results)} docs")
        return data


class RefinerOperator(MapFunction):
    """å‹ç¼© Operator: ä½¿ç”¨æ³¨å†Œçš„ LLMLingua-2 æœåŠ¡"""

    def __init__(self, rate: float = 0.5, **kwargs):
        super().__init__(**kwargs)
        self.rate = rate

    def execute(self, data: dict) -> dict:
        retrieval_results = data.get("retrieval_results", [])

        if not retrieval_results:
            data["refining_results"] = []
            data["compressed_context"] = ""
            return data

        # æå–æ–‡æœ¬
        texts = [
            doc.get("text", doc.get("content", doc.get("passage", ""))) for doc in retrieval_results
        ]

        # å‹ç¼©
        refiner = self.call_service("refiner_service")
        result = refiner.compress(texts, rate=self.rate)

        data["refining_results"] = [result.get("compressed_prompt", "")]
        data["compressed_context"] = result.get("compressed_prompt", "")
        data["original_tokens"] = result.get("origin_tokens", 0)
        data["compressed_tokens"] = result.get("compressed_tokens", 0)
        data["compression_rate"] = result.get("rate", self.rate)

        print(
            f"[Refiner] Compressed {data['original_tokens']} -> {data['compressed_tokens']} tokens"
        )
        return data


class PromptorOperator(MapFunction):
    """Prompt æ„å»º Operator"""

    def __init__(self, template: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.template = template or (
            "Answer the question based on the given context.\n\n"
            "Context:\n{context}\n\n"
            "Question: {question}\n\n"
            "Answer:"
        )

    def execute(self, data: dict) -> dict:
        query = data.get("query", "")
        context = data.get("compressed_context", "")

        prompt = self.template.format(context=context, question=query)
        data["prompt"] = prompt
        return data


class GeneratorOperator(MapFunction):
    """ç”Ÿæˆ Operator: ä½¿ç”¨æ³¨å†Œçš„ LLM æœåŠ¡"""

    def __init__(self, max_tokens: int = 512, temperature: float = 0.7, **kwargs):
        super().__init__(**kwargs)
        self.max_tokens = max_tokens
        self.temperature = temperature

    def execute(self, data: dict) -> dict:
        prompt = data.get("prompt", "")

        llm = self.call_service("llm_service")
        response = llm.chat(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=self.max_tokens,
            temperature=self.temperature,
        )

        data["generated_answer"] = response
        print(f"[Generator] Generated: {response[:100]}...")
        return data


class EvaluatorOperator(MapFunction):
    """è¯„ä¼° Operator: è®¡ç®— F1 ç­‰æŒ‡æ ‡"""

    def execute(self, data: dict) -> dict:
        generated = data.get("generated_answer", "")
        ground_truth = data.get("ground_truth", "")

        # ç®€å•çš„ F1 è®¡ç®—
        if isinstance(ground_truth, list):
            ground_truth = " ".join(str(g) for g in ground_truth)

        gen_tokens = set(generated.lower().split())
        gt_tokens = set(ground_truth.lower().split())

        if not gen_tokens or not gt_tokens:
            data["f1_score"] = 0.0
        else:
            common = gen_tokens & gt_tokens
            precision = len(common) / len(gen_tokens) if gen_tokens else 0
            recall = len(common) / len(gt_tokens) if gt_tokens else 0
            data["f1_score"] = (
                2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            )

        print(f"[Evaluator] F1 Score: {data['f1_score']:.4f}")
        return data


class ResultSinkOperator(SinkFunction):
    """ç»“æœè¾“å‡º Operator"""

    def __init__(self, output_path: Optional[str] = None, verbose: bool = True, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.verbose = verbose
        self.results = []

    def execute(self, data: dict) -> None:
        result = {
            "id": data.get("id"),
            "query": data.get("query"),
            "generated_answer": data.get("generated_answer"),
            "f1_score": data.get("f1_score"),
            "compression_rate": data.get("compression_rate"),
            "original_tokens": data.get("original_tokens"),
            "compressed_tokens": data.get("compressed_tokens"),
        }
        self.results.append(result)

        if self.verbose:
            print(f"\n{'=' * 60}")
            print(f"Query: {result['query']}")
            answer = result.get("generated_answer", "")
            print(f"Answer: {answer[:200] if answer else 'N/A'}...")
            print(
                f"F1: {result['f1_score']:.4f}, Compression: {result.get('compression_rate', 0):.2%}"
            )
            print(f"{'=' * 60}\n")

        if self.output_path:
            import json

            with open(self.output_path, "a") as f:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")


# ============================================================================
# Refiner Pipeline è¿è¡Œå‡½æ•°
# ============================================================================


def pipeline_run(config: dict) -> None:
    """è¿è¡Œ LLMLingua-2 RAG Pipeline

    Args:
        config: ä» YAML é…ç½®æ–‡ä»¶åŠ è½½çš„é…ç½®å­—å…¸
    """
    print("""
========================================================================
              Pipeline B: Long Context Refiner (LLMLingua-2)
========================================================================
  Pipeline: Source -> Retriever -> Refiner -> Promptor -> Generator -> Evaluator

  Integrated Services (é€šè¿‡ register_service æ³¨å†Œï¼Œé¿å…åˆ†å¸ƒå¼è®¿é—®é—®é¢˜):
    - embedding_service: BGE Embedding æ¨¡å‹
    - vector_db: FAISS å‘é‡ç´¢å¼•
    - refiner_service: LLMLingua-2 å‹ç¼©æ¨¡å‹
    - llm_service: vLLM/OpenAI å…¼å®¹æœåŠ¡

  Features:
    - BERT-based token classification (å¿«é€Ÿï¼Œæ— éœ€ LLM æ¨ç†)
    - Multilingual support (mBERT/XLM-RoBERTa)
    - Token-level precise compression
========================================================================
    """)

    env = LocalEnvironment("refiner_pipeline")

    # æ³¨å†ŒæœåŠ¡
    print("\n[Pipeline] Registering services...")

    # Embedding æœåŠ¡
    embedding_cfg = config.get("retriever", {}).get("embedding", {})
    register_embedding_service(
        env,
        model_name=embedding_cfg.get("model", "BAAI/bge-large-en-v1.5"),
        device=f"cuda:{embedding_cfg.get('gpu_device', 0)}",
    )

    # Vector DB æœåŠ¡
    retriever_cfg = config.get("retriever", {})
    faiss_cfg = retriever_cfg.get("faiss", {})
    register_vector_db_service(
        env,
        index_path=os.path.expandvars(faiss_cfg.get("index_path", "")),
        documents_path=os.path.expandvars(faiss_cfg.get("documents_path", "")),
        mapping_path=os.path.expandvars(faiss_cfg.get("mapping_path", ""))
        if faiss_cfg.get("mapping_path")
        else None,
        dimension=retriever_cfg.get("dimension", 1024),
    )

    # Refiner æœåŠ¡
    refiner_cfg = config.get("llmlingua2", {})
    register_refiner_service(
        env,
        model_name=refiner_cfg.get(
            "model_name", "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank"
        ),
        device=refiner_cfg.get("device", "cuda:0"),
        rate=refiner_cfg.get("rate", 0.5),
    )

    # LLM æœåŠ¡
    generator_cfg = config.get("generator", {}).get("vllm", {})
    register_llm_service(
        env,
        base_url=generator_cfg.get("base_url", "http://localhost:8001/v1"),
        model_name=generator_cfg.get("model_name", "Qwen/Qwen2.5-7B-Instruct"),
        api_key=generator_cfg.get("api_key", ""),
    )

    print()

    # æ„å»º Pipeline
    source_cfg = config.get("source", {})
    (
        env.from_source(
            DataSourceOperator,
            dataset_name=source_cfg.get("hf_dataset_name", "RUC-NLPIR/FlashRAG_datasets"),
            dataset_config=source_cfg.get("hf_dataset_config", "nq"),
            split=source_cfg.get("hf_split", "test"),
            max_samples=source_cfg.get("max_samples", 20),
        )
        .map(RetrieverOperator, top_k=retriever_cfg.get("top_k", 100))
        .map(RefinerOperator, rate=refiner_cfg.get("rate", 0.5))
        .map(PromptorOperator)
        .map(GeneratorOperator)
        .map(EvaluatorOperator)
        .sink(ResultSinkOperator, verbose=True)
    )

    try:
        start_time = time.time()
        env.submit(autostop=True)
        duration = time.time() - start_time
        print(f"\n[Pipeline] Completed in {duration:.2f} seconds")
    except KeyboardInterrupt:
        print("\nâš ï¸  KeyboardInterrupt: ç”¨æˆ·æ‰‹åŠ¨åœæ­¢")
    except Exception as e:
        print(f"\nâŒ Pipelineå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\nğŸ”„ æ¸…ç†ç¯å¢ƒ...")
        env.close()
        print("âœ… ç¯å¢ƒå·²å…³é—­")


# ============================================================================
# Refiner Pipeline å°è£…ç±»
# ============================================================================


class RefinerPipeline:
    """Refiner Pipeline å°è£…ç±»

    ä½¿ç”¨ LLMLingua-2 BERT token åˆ†ç±»è¿›è¡Œå¿«é€Ÿä¸Šä¸‹æ–‡å‹ç¼©ã€‚
    é€šè¿‡ register_service æ³¨å†ŒæœåŠ¡ï¼Œé¿å…åˆ†å¸ƒå¼è®¿é—®é—®é¢˜ã€‚

    ç¤ºä¾‹:
        >>> config_path = "config/config_llmlingua2.yaml"
        >>> pipeline = RefinerPipeline.from_config(config_path)
        >>> result = pipeline.run()
    """

    def __init__(self, config: dict):
        """åˆå§‹åŒ– Pipeline

        Args:
            config: ä» YAML åŠ è½½çš„é…ç½®å­—å…¸
        """
        self.config = config
        self.env: Optional[LocalEnvironment] = None

    @classmethod
    def from_config(cls, config_path: str) -> RefinerPipeline:
        """ä»é…ç½®æ–‡ä»¶åˆ›å»º Pipeline

        Args:
            config_path: YAML é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            RefinerPipeline å®ä¾‹
        """
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Configuration file not found: {config_path}")

        config = load_config(config_path)
        return cls(config)

    def run(self) -> dict:
        """è¿è¡Œ Pipeline

        Returns:
            åŒ…å«è¿è¡Œç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        pipeline_run(self.config)
        duration = time.time() - start_time

        return {
            "pipeline": "B (LLMLingua-2 Refiner)",
            "duration_seconds": duration,
            "config": {
                "source": self.config.get("source", {}).get("hf_dataset_name", ""),
                "max_samples": self.config.get("source", {}).get("max_samples", 0),
                "model": self.config.get("llmlingua2", {}).get("model_name", ""),
                "rate": self.config.get("llmlingua2", {}).get("rate", 0.5),
            },
        }


# ============================================================================
# ç‹¬ç«‹è¿è¡Œå…¥å£
# ============================================================================


if __name__ == "__main__":
    CustomLogger.disable_global_console_debug()

    # æ£€æŸ¥æ˜¯å¦åœ¨æµ‹è¯•æ¨¡å¼ä¸‹è¿è¡Œ
    if os.getenv("SAGE_EXAMPLES_MODE") == "test" or os.getenv("SAGE_TEST_MODE") == "true":
        print("ğŸ§ª Test mode detected - LLMLingua-2 pipeline requires pre-built FAISS index")
        print("âœ… Test passed: Example structure validated")
        sys.exit(0)

    # é…ç½®æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨ benchmark_refiner çš„é…ç½®)
    config_path = os.path.join(
        os.path.dirname(__file__),
        "..",
        "..",
        "..",
        "..",
        "benchmark_refiner",
        "config",
        "config_llmlingua2.yaml",
    )

    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"âŒ Configuration file not found: {config_path}")
        print("Please ensure the config file exists before running this example.")
        sys.exit(1)

    config = load_config(config_path)

    # æ£€æŸ¥ LLMLingua-2 ç›¸å…³é…ç½®
    if config.get("llmlingua2", {}).get("enabled", True):
        print("ğŸš€ LLMLingua-2 compression enabled")
        print(f"   Model: {config['llmlingua2'].get('model_name', 'default')}")
        print(f"   Rate: {config['llmlingua2'].get('rate', 0.5)}")
    else:
        print("â„¹ï¸  LLMLingua-2 disabled - running in baseline mode")

    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if config.get("retriever", {}).get("type") == "wiki18_faiss":
        index_path = config["retriever"]["faiss"]["index_path"]
        # å±•å¼€ç¯å¢ƒå˜é‡
        index_path = os.path.expandvars(index_path)
        if not os.path.exists(index_path):
            print(f"âŒ FAISS index file not found: {index_path}")
            print(
                "Please build the FAISS index first using build_milvus_dense_index.py or similar."
            )
            print("Or modify the config to use a different retriever type.")
            sys.exit(1)

    pipeline_run(config)
