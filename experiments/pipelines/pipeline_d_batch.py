"""
Pipeline D: Batch Processing (ÊâπÂ§ÑÁêÜ)
=====================================

ÊãìÊâë: Source ‚Üí Batch ‚Üí Window ‚Üí Future(LLM) ‚Üí Aggregate ‚Üí Sink

ÁÆóÂ≠ê:
- Source: Âä†ËΩΩÊï∞ÊçÆÊµÅ
- Map (Batch): Â∞ÜÊï∞ÊçÆÂàÜÊâπ
- Map (Window): ÊªëÂä®/ÊªöÂä®Á™óÂè£ËÅöÂêà
- Map (LLM): ÊâπÈáè LLM Ë∞ÉÁî® (Future ËØ≠‰πâ)
- Sink (Aggregate): ËÅöÂêàÁªìÊûúÂπ∂ËæìÂá∫

Êï∞ÊçÆÈõÜ: BBH, GPQA
"""

from __future__ import annotations

import os
import time
from dataclasses import dataclass
from typing import Any, Optional

# Á¶ÅÁî®‰ª£ÁêÜÔºåÁ°Æ‰øùÂÜÖÁΩëÊúçÂä°ÂèØËÆøÈóÆ
os.environ.pop("http_proxy", None)
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("https_proxy", None)
os.environ.pop("HTTPS_PROXY", None)

import httpx
from sage.common.config.ports import SagePorts
from sage.common.core import (
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import FlownetEnvironment

_DEFAULT_LLM_URL = f"http://localhost:{SagePorts.LLM_DEFAULT}/v1"

from ..common.execution_guard import run_pipeline_bounded
from .scheduler import HeadNodeScheduler


@dataclass
class BatchConfig:
    """Batch Pipeline ÈÖçÁΩÆ"""

    # Êï∞ÊçÆÈõÜ
    dataset_name: str = "bbh"
    num_samples: int = 100

    # ÊâπÂ§ÑÁêÜ
    batch_size: int = 8
    window_size: int = 16

    # Ê®°Âûã
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # ÊúçÂä°Á´ØÁÇπ
    llm_base_url: str = _DEFAULT_LLM_URL

    # ËøêË°åÊó∂
    job_manager_host: str = "localhost"
    job_manager_port: int = 19001
    request_timeout: float = 120.0


@dataclass
class BatchItem:
    """ÊâπÂ§ÑÁêÜÊï∞ÊçÆÈ°π"""

    item_id: int
    query: str
    answer: str = ""
    batch_id: int = 0
    window_id: int = 0


# ============================================================================
# Source: Êï∞ÊçÆÂä†ËΩΩ
# ============================================================================


class BatchSourceFunction(SourceFunction):
    """Batch Source: Âä†ËΩΩÊï∞ÊçÆÈõÜ"""

    def __init__(
        self,
        dataset_name: str = "bbh",
        num_samples: int = 100,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.dataset_name = dataset_name
        self.num_samples = num_samples
        self._data: list[BatchItem] = []
        self._index = 0
        self._loaded = False

    def _load_data(self) -> None:
        """Âä†ËΩΩÊï∞ÊçÆÈõÜ"""
        if self._loaded:
            return

        if self.dataset_name == "bbh":
            from sage.data.sources.bbh.dataloader import BBHDataLoader

            loader = BBHDataLoader()
        else:
            from sage.data.sources.gpqa.dataloader import GPQADataLoader

            loader = GPQADataLoader()

        raw_data = loader.load()

        for i, sample in enumerate(raw_data[: self.num_samples]):
            self._data.append(
                BatchItem(
                    item_id=i,
                    query=sample.get("question", sample.get("query", "")),
                )
            )

        self._loaded = True
        print(f"üìÇ Loaded {len(self._data)} samples from {self.dataset_name}")

    def execute(self, data: Any = None) -> Optional[BatchItem]:
        """ËøîÂõû‰∏ã‰∏Ä‰∏™Êï∞ÊçÆÈ°π"""
        self._load_data()

        if self._index >= len(self._data):
            return None

        item = self._data[self._index]
        self._index += 1
        return item


# ============================================================================
# Map (Batch): ÂàÜÊâπÂ§ÑÁêÜ
# ============================================================================


class BatchingMapFunction(MapFunction):
    """Map (Batch): Â∞ÜÊï∞ÊçÆÂàÜÊâπ"""

    def __init__(self, batch_size: int = 8, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        self._buffer: list[BatchItem] = []
        self._batch_id = 0

    def execute(self, item: BatchItem) -> Optional[list[BatchItem]]:
        """ÊâßË°åÂàÜÊâπ"""
        item.batch_id = self._batch_id
        self._buffer.append(item)

        if len(self._buffer) >= self.batch_size:
            batch = self._buffer
            self._buffer = []
            self._batch_id += 1
            print(f"üì¶ Batch {item.batch_id}: {len(batch)} items")
            return batch

        return None


# ============================================================================
# Map (Window): Á™óÂè£ËÅöÂêà
# ============================================================================


class WindowMapFunction(MapFunction):
    """Map (Window): Á™óÂè£ËÅöÂêà"""

    def __init__(self, window_size: int = 16, **kwargs):
        super().__init__(**kwargs)
        self.window_size = window_size
        self._window_buffer: list[BatchItem] = []
        self._window_id = 0

    def execute(self, batch: Optional[list[BatchItem]]) -> Optional[list[BatchItem]]:
        """ÊâßË°åÁ™óÂè£ËÅöÂêà"""
        if batch is None:
            return None

        for item in batch:
            item.window_id = self._window_id
        self._window_buffer.extend(batch)

        if len(self._window_buffer) >= self.window_size:
            window = self._window_buffer
            self._window_buffer = []
            self._window_id += 1
            print(f"ü™ü Window {window[0].window_id}: {len(window)} items")
            return window

        return None


# ============================================================================
# Map (LLM): ÊâπÈáè LLM Ë∞ÉÁî®
# ============================================================================


class BatchLLMMapFunction(MapFunction):
    """Map (LLM): ÊâπÈáè LLM Ë∞ÉÁî®"""

    def __init__(
        self,
        llm_base_url: str = _DEFAULT_LLM_URL,
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: float = 120.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.timeout = timeout

    def execute(self, window: Optional[list[BatchItem]]) -> Optional[list[BatchItem]]:
        """ÊâßË°åÊâπÈáè LLM Ë∞ÉÁî®"""
        if window is None:
            return None

        with httpx.Client(timeout=self.timeout) as client:
            for item in window:
                response = client.post(
                    f"{self.llm_base_url}/chat/completions",
                    json={
                        "model": self.llm_model,
                        "messages": [{"role": "user", "content": item.query}],
                        "max_tokens": 128,
                        "temperature": 0.7,
                    },
                )
                response.raise_for_status()
                result = response.json()
                item.answer = result["choices"][0]["message"]["content"]

        print(f"ü§ñ LLM processed {len(window)} items")
        return window


# ============================================================================
# Sink (Aggregate): ËÅöÂêàÁªìÊûú
# ============================================================================


class BatchSinkFunction(SinkFunction):
    """Batch Sink: ËÅöÂêàÁªìÊûúÂπ∂ËæìÂá∫"""

    def __init__(self, output_path: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.results: list[dict] = []
        self.total_items = 0

    def execute(self, window: Optional[list[BatchItem]]) -> None:
        """ËæìÂá∫ÁªìÊûú"""
        if window is None:
            return

        for item in window:
            self.total_items += 1
            result = {
                "item_id": item.item_id,
                "query": item.query,
                "answer": item.answer,
                "batch_id": item.batch_id,
                "window_id": item.window_id,
            }
            self.results.append(result)
            print(f"‚úÖ [{item.item_id}] Q: {item.query[:30]}... ‚Üí A: {item.answer[:30]}...")

        if self.output_path:
            import json

            with open(self.output_path, "a") as f:
                for r in self.results[-len(window) :]:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")


# ============================================================================
# Batch Pipeline Â∞ÅË£Ö
# ============================================================================


class BatchPipeline:
    """Batch Pipeline Â∞ÅË£ÖÁ±ª"""

    def __init__(self, config: BatchConfig):
        self.config = config
        self.env: Optional[FlownetEnvironment] = None

    def build(self) -> FlownetEnvironment:
        """ÊûÑÂª∫ Batch Pipeline"""
        scheduler = HeadNodeScheduler()

        self.env = FlownetEnvironment(
            "batch_pipeline",
            config={
                "flownet": {
                    "job_manager_host": self.config.job_manager_host,
                    "job_manager_port": self.config.job_manager_port,
                }
            },
            scheduler=scheduler,
        )

        # ÊûÑÂª∫ Pipeline: Source ‚Üí Map(Batch) ‚Üí Map(Window) ‚Üí Map(LLM) ‚Üí Sink
        (
            self.env.from_source(
                BatchSourceFunction,
                dataset_name=self.config.dataset_name,
                num_samples=self.config.num_samples,
            )
            .map(BatchingMapFunction, batch_size=self.config.batch_size)
            .map(WindowMapFunction, window_size=self.config.window_size)
            .map(
                BatchLLMMapFunction,
                llm_base_url=self.config.llm_base_url,
                llm_model=self.config.llm_model,
                timeout=self.config.request_timeout,
            )
            .sink(BatchSinkFunction)
        )

        return self.env

    def run(self) -> dict:
        """ËøêË°å Pipeline"""
        if self.env is None:
            self.build()

        start_time = time.time()
        try:
            run_pipeline_bounded(self.env, timeout_seconds=90.0, poll_interval_seconds=0.2)
        finally:
            self.env.close()

        duration = time.time() - start_time
        return {
            "pipeline": "D (Batch)",
            "duration_seconds": duration,
            "config": {
                "dataset": self.config.dataset_name,
                "batch_size": self.config.batch_size,
                "window_size": self.config.window_size,
            },
        }
