"""
Pipeline D: Batch Processing (æ‰¹å¤„ç†)
=====================================

æ‹“æ‰‘: Source â†’ Batch â†’ Window â†’ Future(LLM) â†’ Aggregate â†’ Sink

ç®—å­:
- Source: åŠ è½½æ•°æ®æµ
- Map (Batch): å°†æ•°æ®åˆ†æ‰¹
- Map (Window): æ»‘åŠ¨/æ»šåŠ¨çª—å£èšåˆ
- Map (LLM): æ‰¹é‡ LLM è°ƒç”¨ (Future è¯­ä¹‰)
- Sink (Aggregate): èšåˆç»“æœå¹¶è¾“å‡º

æ•°æ®é›†: BBH, GPQA
"""

from __future__ import annotations

import os
import time
from dataclasses import dataclass
from typing import Any, Optional

# ç¦ç”¨ä»£ç†ï¼Œç¡®ä¿å†…ç½‘æœåŠ¡å¯è®¿é—®
os.environ.pop("http_proxy", None)
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("https_proxy", None)
os.environ.pop("HTTPS_PROXY", None)

import httpx

from sage.common.core import (
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import RemoteEnvironment

from .scheduler import HeadNodeScheduler


@dataclass
class BatchConfig:
    """Batch Pipeline é…ç½®"""

    # æ•°æ®é›†
    dataset_name: str = "bbh"
    num_samples: int = 100

    # æ‰¹å¤„ç†
    batch_size: int = 8
    window_size: int = 16

    # æ¨¡å‹
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # æœåŠ¡ç«¯ç‚¹
    llm_base_url: str = "http://localhost:8001/v1"

    # è¿è¡Œæ—¶
    job_manager_host: str = "localhost"
    job_manager_port: int = 19001
    request_timeout: float = 120.0


@dataclass
class BatchItem:
    """æ‰¹å¤„ç†æ•°æ®é¡¹"""

    item_id: int
    query: str
    answer: str = ""
    batch_id: int = 0
    window_id: int = 0


# ============================================================================
# Source: æ•°æ®åŠ è½½
# ============================================================================


class BatchSourceFunction(SourceFunction):
    """Batch Source: åŠ è½½æ•°æ®é›†"""

    def __init__(
        self,
        dataset_name: str = "bbh",
        num_samples: int = 100,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.dataset_name = dataset_name
        self.num_samples = num_samples
        self._data: list[BatchItem] = []
        self._index = 0
        self._loaded = False

    def _load_data(self) -> None:
        """åŠ è½½æ•°æ®é›†"""
        if self._loaded:
            return

        if self.dataset_name == "bbh":
            from sage.data.sources.bbh.dataloader import BBHDataLoader

            loader = BBHDataLoader()
        else:
            from sage.data.sources.gpqa.dataloader import GPQADataLoader

            loader = GPQADataLoader()

        raw_data = loader.load()

        for i, sample in enumerate(raw_data[: self.num_samples]):
            self._data.append(
                BatchItem(
                    item_id=i,
                    query=sample.get("question", sample.get("query", "")),
                )
            )

        self._loaded = True
        print(f"ğŸ“‚ Loaded {len(self._data)} samples from {self.dataset_name}")

    def execute(self, data: Any = None) -> Optional[BatchItem]:
        """è¿”å›ä¸‹ä¸€ä¸ªæ•°æ®é¡¹"""
        self._load_data()

        if self._index >= len(self._data):
            return None

        item = self._data[self._index]
        self._index += 1
        return item


# ============================================================================
# Map (Batch): åˆ†æ‰¹å¤„ç†
# ============================================================================


class BatchingMapFunction(MapFunction):
    """Map (Batch): å°†æ•°æ®åˆ†æ‰¹"""

    def __init__(self, batch_size: int = 8, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        self._buffer: list[BatchItem] = []
        self._batch_id = 0

    def execute(self, item: BatchItem) -> Optional[list[BatchItem]]:
        """æ‰§è¡Œåˆ†æ‰¹"""
        item.batch_id = self._batch_id
        self._buffer.append(item)

        if len(self._buffer) >= self.batch_size:
            batch = self._buffer
            self._buffer = []
            self._batch_id += 1
            print(f"ğŸ“¦ Batch {item.batch_id}: {len(batch)} items")
            return batch

        return None


# ============================================================================
# Map (Window): çª—å£èšåˆ
# ============================================================================


class WindowMapFunction(MapFunction):
    """Map (Window): çª—å£èšåˆ"""

    def __init__(self, window_size: int = 16, **kwargs):
        super().__init__(**kwargs)
        self.window_size = window_size
        self._window_buffer: list[BatchItem] = []
        self._window_id = 0

    def execute(self, batch: Optional[list[BatchItem]]) -> Optional[list[BatchItem]]:
        """æ‰§è¡Œçª—å£èšåˆ"""
        if batch is None:
            return None

        for item in batch:
            item.window_id = self._window_id
        self._window_buffer.extend(batch)

        if len(self._window_buffer) >= self.window_size:
            window = self._window_buffer
            self._window_buffer = []
            self._window_id += 1
            print(f"ğŸªŸ Window {window[0].window_id}: {len(window)} items")
            return window

        return None


# ============================================================================
# Map (LLM): æ‰¹é‡ LLM è°ƒç”¨
# ============================================================================


class BatchLLMMapFunction(MapFunction):
    """Map (LLM): æ‰¹é‡ LLM è°ƒç”¨"""

    def __init__(
        self,
        llm_base_url: str = "http://localhost:8001/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: float = 120.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.timeout = timeout

    def execute(self, window: Optional[list[BatchItem]]) -> Optional[list[BatchItem]]:
        """æ‰§è¡Œæ‰¹é‡ LLM è°ƒç”¨"""
        if window is None:
            return None

        with httpx.Client(timeout=self.timeout) as client:
            for item in window:
                response = client.post(
                    f"{self.llm_base_url}/chat/completions",
                    json={
                        "model": self.llm_model,
                        "messages": [{"role": "user", "content": item.query}],
                        "max_tokens": 128,
                        "temperature": 0.7,
                    },
                )
                response.raise_for_status()
                result = response.json()
                item.answer = result["choices"][0]["message"]["content"]

        print(f"ğŸ¤– LLM processed {len(window)} items")
        return window


# ============================================================================
# Sink (Aggregate): èšåˆç»“æœ
# ============================================================================


class BatchSinkFunction(SinkFunction):
    """Batch Sink: èšåˆç»“æœå¹¶è¾“å‡º"""

    def __init__(self, output_path: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.results: list[dict] = []
        self.total_items = 0

    def execute(self, window: Optional[list[BatchItem]]) -> None:
        """è¾“å‡ºç»“æœ"""
        if window is None:
            return

        for item in window:
            self.total_items += 1
            result = {
                "item_id": item.item_id,
                "query": item.query,
                "answer": item.answer,
                "batch_id": item.batch_id,
                "window_id": item.window_id,
            }
            self.results.append(result)
            print(f"âœ… [{item.item_id}] Q: {item.query[:30]}... â†’ A: {item.answer[:30]}...")

        if self.output_path:
            import json

            with open(self.output_path, "a") as f:
                for r in self.results[-len(window) :]:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")


# ============================================================================
# Batch Pipeline å°è£…
# ============================================================================


class BatchPipeline:
    """Batch Pipeline å°è£…ç±»"""

    def __init__(self, config: BatchConfig):
        self.config = config
        self.env: Optional[RemoteEnvironment] = None

    def build(self) -> RemoteEnvironment:
        """æ„å»º Batch Pipeline"""
        scheduler = HeadNodeScheduler()

        self.env = RemoteEnvironment(
            "batch_pipeline",
            host=self.config.job_manager_host,
            port=self.config.job_manager_port,
            scheduler=scheduler,
        )

        # æ„å»º Pipeline: Source â†’ Map(Batch) â†’ Map(Window) â†’ Map(LLM) â†’ Sink
        (
            self.env.from_source(
                BatchSourceFunction,
                dataset_name=self.config.dataset_name,
                num_samples=self.config.num_samples,
            )
            .map(BatchingMapFunction, batch_size=self.config.batch_size)
            .map(WindowMapFunction, window_size=self.config.window_size)
            .map(
                BatchLLMMapFunction,
                llm_base_url=self.config.llm_base_url,
                llm_model=self.config.llm_model,
                timeout=self.config.request_timeout,
            )
            .sink(BatchSinkFunction)
        )

        return self.env

    def run(self) -> dict:
        """è¿è¡Œ Pipeline"""
        if self.env is None:
            self.build()

        start_time = time.time()
        try:
            self.env.submit()
            time.sleep(15)  # æ‰¹å¤„ç†éœ€è¦æ›´å¤šæ—¶é—´
        finally:
            self.env.close()

        duration = time.time() - start_time
        return {
            "pipeline": "D (Batch)",
            "duration_seconds": duration,
            "config": {
                "dataset": self.config.dataset_name,
                "batch_size": self.config.batch_size,
                "window_size": self.config.window_size,
            },
        }
