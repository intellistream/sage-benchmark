#!/usr/bin/env python3
"""
æ¨¡å—åŒ– Adaptive-RAG Pipeline å®ç°

è®¾è®¡åŸåˆ™ï¼š
1. ç®—å­è§£è€¦: Retrieverã€Prompterã€Generator æ˜¯ç‹¬ç«‹çš„ MapFunction
2. è¿­ä»£æ”¯æŒ: é€šè¿‡ "å±•å¼€å¾ªç¯" æˆ– "çŠ¶æ€æœº" æ¨¡å¼å®ç°

æµå¤„ç†ä¸­çš„è¿­ä»£é—®é¢˜ï¼š
- ä¼ ç»Ÿ DAG æ˜¯æ— ç¯çš„ï¼Œä¸æ”¯æŒ feedback loop
- è§£å†³æ–¹æ¡ˆæœ‰ä¸‰ç§ï¼š
  A. å¾ªç¯å±•å¼€ (Unroll): é¢„è®¾ max_iterationsï¼Œåˆ›å»º N ä¸ªä¸²è”é˜¶æ®µ
  B. çŠ¶æ€æœº (StateMachine): å•ä¸ªç®—å­å†…éƒ¨ç»´æŠ¤çŠ¶æ€ï¼Œç”¨æ¡ä»¶åˆ¤æ–­è¿­ä»£
  C. é€’å½’æäº¤ (ReSubmit): Sink å°†éœ€è¦ç»§ç»­è¿­ä»£çš„æ•°æ®é‡æ–°æäº¤åˆ° Source

æœ¬æ–‡ä»¶å®ç°æ–¹æ¡ˆ A (å¾ªç¯å±•å¼€)ï¼Œé€‚åˆå·²çŸ¥æœ€å¤§è¿­ä»£æ¬¡æ•°çš„åœºæ™¯ã€‚

æ¶æ„å›¾:
```
=== ç®€å•æŸ¥è¯¢ (ZERO) ===
Source -> Classifier -> filter(ZERO) -> Generator -> Sink

=== å•æ¬¡æ£€ç´¢ (SINGLE) ===
Source -> Classifier -> filter(SINGLE) -> Retriever -> Prompter -> Generator -> Sink

=== è¿­ä»£æ£€ç´¢ (MULTI) - å¾ªç¯å±•å¼€æ¨¡å¼ ===
Source -> Classifier -> filter(MULTI) -> [Stage1] -> [Stage2] -> [Stage3] -> FinalGenerator -> Sink
                                           |           |           |
                                           v           v           v
                                        Retriever   Retriever   Retriever
                                           |           |           |
                                           v           v           v
                                        Reasoner   Reasoner   Reasoner
                                           |           |           |
                                           v           v           v
                                        SubQuery   SubQuery   (ç›´æ¥åˆæˆ)
```
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field

from sage.common.core import (
    FilterFunction,
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import FlownetEnvironment
from sage.kernel.api import LocalEnvironment

# æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—è¿è¡Œ
try:
    from .classifier import ClassificationResult, create_classifier
except ImportError:
    from classifier import ClassificationResult, create_classifier


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================


@dataclass
class QueryData:
    """æŸ¥è¯¢æ•°æ®"""

    query: str
    query_id: str = ""
    metadata: dict = field(default_factory=dict)


@dataclass
class ClassifiedQuery:
    """åˆ†ç±»åçš„æŸ¥è¯¢"""

    query: str
    query_id: str
    complexity: str  # ZERO, SINGLE, MULTI
    confidence: float


@dataclass
class RetrievalState:
    """æ£€ç´¢çŠ¶æ€ - ç”¨äºè¿­ä»£æ£€ç´¢"""

    query: str
    original_query: str
    query_id: str
    complexity: str
    iteration: int = 0
    max_iterations: int = 3
    retrieved_docs: list = field(default_factory=list)
    reasoning_chain: list = field(default_factory=list)
    context: str = ""
    should_continue: bool = True


@dataclass
class PromptData:
    """Prompt æ•°æ®"""

    query: str
    query_id: str
    context: str
    complexity: str
    prompt: str = ""


@dataclass
class ResultData:
    """ç»“æœæ•°æ®"""

    query: str
    answer: str
    strategy_used: str
    complexity: str
    retrieval_steps: int = 0
    processing_time_ms: float = 0.0


# ============================================================================
# ç‹¬ç«‹ç®—å­: Classifier
# ============================================================================


class QueryClassifier(MapFunction):
    """æŸ¥è¯¢åˆ†ç±»å™¨ - åˆ¤æ–­æŸ¥è¯¢å¤æ‚åº¦"""

    def __init__(self, classifier_type: str = "rule", **kwargs):
        super().__init__(**kwargs)
        self.classifier_type = classifier_type
        self._classifier = None

    def execute(self, data: QueryData) -> ClassifiedQuery:
        if self._classifier is None:
            self._classifier = create_classifier(self.classifier_type)

        result: ClassificationResult = self._classifier.classify(data.query)

        print(f"ğŸ“‹ Classified: {data.query} -> {result.level.name}")

        return ClassifiedQuery(
            query=data.query,
            query_id=data.query_id,
            complexity=result.level.name,
            confidence=result.confidence,
        )


# ============================================================================
# ç‹¬ç«‹ç®—å­: Retriever
# ============================================================================


class SimpleRetriever(MapFunction):
    """
    æ£€ç´¢å™¨ - ç‹¬ç«‹çš„æ£€ç´¢ç®—å­

    è¾“å…¥: RetrievalState (åŒ…å«å½“å‰æŸ¥è¯¢å’Œå·²æ£€ç´¢çš„æ–‡æ¡£)
    è¾“å‡º: RetrievalState (æ›´æ–° retrieved_docs)
    """

    # ç®€å•çŸ¥è¯†åº“
    KNOWLEDGE_BASE = [
        {
            "content": "Machine learning is a subset of artificial intelligence that learns from data.",
            "id": "1",
        },
        {"content": "Deep learning uses neural networks with multiple layers.", "id": "2"},
        {"content": "Python is a popular programming language for ML tasks.", "id": "3"},
        {"content": "BERT is a transformer-based model for NLP tasks.", "id": "4"},
        {"content": "RAG combines retrieval with generation for better answers.", "id": "5"},
        {"content": "Transformers use attention mechanisms for sequence modeling.", "id": "6"},
        {"content": "GPT models are autoregressive language models.", "id": "7"},
        {"content": "Vector databases store embeddings for similarity search.", "id": "8"},
    ]

    def __init__(self, top_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.top_k = top_k

    def execute(self, state: RetrievalState) -> RetrievalState:
        """æ‰§è¡Œæ£€ç´¢ï¼Œæ›´æ–°çŠ¶æ€"""
        query_words = set(state.query.lower().split())
        scored_docs = []

        for doc in self.KNOWLEDGE_BASE:
            # å»é‡ï¼šè·³è¿‡å·²æ£€ç´¢çš„æ–‡æ¡£
            if doc["id"] in [d.get("id") for d in state.retrieved_docs]:
                continue

            content_words = set(doc["content"].lower().split())
            overlap = len(query_words & content_words)
            if overlap > 0:
                scored_docs.append({**doc, "score": overlap / len(query_words)})

        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        new_docs = scored_docs[: self.top_k]

        # æ›´æ–°çŠ¶æ€
        state.retrieved_docs.extend(new_docs)
        state.context = "\n".join(
            [f"[Doc {i + 1}]: {d['content']}" for i, d in enumerate(state.retrieved_docs)]
        )

        print(f"  ğŸ” Retrieved {len(new_docs)} new docs (total: {len(state.retrieved_docs)})")

        return state


# ============================================================================
# ç‹¬ç«‹ç®—å­: Prompter
# ============================================================================


class SimplePrompter(MapFunction):
    """
    Prompt æ„å»ºå™¨ - å°† context + query ç»„åˆæˆ prompt

    è¾“å…¥: RetrievalState æˆ– ClassifiedQuery
    è¾“å‡º: PromptData
    """

    def __init__(self, template: str = "default", **kwargs):
        super().__init__(**kwargs)
        self.template = template

    def execute(self, data: RetrievalState | ClassifiedQuery) -> PromptData:
        if isinstance(data, RetrievalState):
            context = data.context or "No relevant documents found."
            query = data.original_query
            query_id = data.query_id
            complexity = data.complexity

            if self.template == "iterative":
                # è¿­ä»£æ£€ç´¢çš„æœ€ç»ˆåˆæˆ prompt
                chain_text = "\n".join(
                    [f"Step {i + 1}: {r}" for i, r in enumerate(data.reasoning_chain)]
                )
                prompt = f"""Based on the following reasoning steps and gathered context, provide a comprehensive answer.

Reasoning steps:
{chain_text}

Context:
{context}

Question: {query}

Answer:"""
            else:
                prompt = f"""Answer the question based on the provided context. If no relevant info, say so.

Context:
{context}

Question: {query}

Answer:"""
        else:
            # ClassifiedQuery - ç›´æ¥ç”Ÿæˆï¼ˆæ— æ£€ç´¢ï¼‰
            query = data.query
            query_id = data.query_id
            complexity = data.complexity
            context = ""
            prompt = f"Answer the following question directly: {query}"

        return PromptData(
            query=query,
            query_id=query_id,
            context=context,
            complexity=complexity,
            prompt=prompt,
        )


# ============================================================================
# ç‹¬ç«‹ç®—å­: Generator
# ============================================================================


class SimpleGenerator(MapFunction):
    """
    ç”Ÿæˆå™¨ - è°ƒç”¨ LLM ç”Ÿæˆå›å¤

    è¾“å…¥: PromptData
    è¾“å‡º: ResultData
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        max_tokens: int = 512,
        strategy_name: str = "unknown",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.strategy_name = strategy_name

    def execute(self, data: PromptData) -> ResultData:
        import requests

        start_time = time.time()

        messages = [{"role": "user", "content": data.prompt}]

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": self.max_tokens,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()
            answer = result["choices"][0]["message"]["content"]
        except Exception as e:
            answer = f"[Generation Error] {str(e)}"

        return ResultData(
            query=data.query,
            answer=answer,
            strategy_used=self.strategy_name,
            complexity=data.complexity,
            retrieval_steps=0,
            processing_time_ms=(time.time() - start_time) * 1000,
        )


# ============================================================================
# è¿­ä»£æ£€ç´¢ä¸“ç”¨ç®—å­: Reasoner (å­æŸ¥è¯¢ç”Ÿæˆ)
# ============================================================================


class IterativeReasoner(MapFunction):
    """
    è¿­ä»£æ¨ç†å™¨ - æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦ç»§ç»­æ£€ç´¢ï¼Œç”Ÿæˆå­æŸ¥è¯¢

    è¾“å…¥: RetrievalState
    è¾“å‡º: RetrievalState (æ›´æ–° query å’Œ reasoning_chain)
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model

    def _llm_call(self, messages: list[dict]) -> str:
        import requests

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": 100,
                    "temperature": 0.5,
                },
                timeout=30,
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Error] {str(e)}"

    def execute(self, state: RetrievalState) -> RetrievalState:
        state.iteration += 1

        # è®°å½•å½“å‰æ­¥éª¤
        state.reasoning_chain.append(
            f"Query: {state.query} -> Found {len(state.retrieved_docs)} docs"
        )

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æˆ–è¶³å¤Ÿçš„æ–‡æ¡£
        if state.iteration >= state.max_iterations or len(state.retrieved_docs) >= 6:
            state.should_continue = False
            print(f"  ğŸ§  Reasoner: Iteration {state.iteration} - STOP (enough info)")
            return state

        # ç”Ÿæˆä¸‹ä¸€æ­¥å­æŸ¥è¯¢
        messages = [
            {
                "role": "system",
                "content": "Generate a follow-up search query. Reply with ONLY the query.",
            },
            {
                "role": "user",
                "content": f"Original: {state.original_query}\nContext so far:\n{state.context}\n\nFollow-up query:",
            },
        ]

        new_query = self._llm_call(messages).strip()
        state.query = new_query

        print(f"  ğŸ§  Reasoner: Iteration {state.iteration} - New query: {new_query[:50]}")

        return state


# ============================================================================
# è¾…åŠ©ç®—å­: çŠ¶æ€è½¬æ¢
# ============================================================================


class ClassifiedToState(MapFunction):
    """å°† ClassifiedQuery è½¬æ¢ä¸º RetrievalState"""

    def __init__(self, max_iterations: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.max_iterations = max_iterations

    def execute(self, data: ClassifiedQuery) -> RetrievalState:
        return RetrievalState(
            query=data.query,
            original_query=data.query,
            query_id=data.query_id,
            complexity=data.complexity,
            max_iterations=self.max_iterations,
        )


class StateToPrompt(MapFunction):
    """å°† RetrievalState è½¬æ¢ä¸º PromptDataï¼ˆæœ€ç»ˆåˆæˆï¼‰"""

    def execute(self, state: RetrievalState) -> PromptData:
        chain_text = "\n".join([f"Step {i + 1}: {r}" for i, r in enumerate(state.reasoning_chain)])
        context = state.context or "No documents found."

        prompt = f"""Based on the reasoning steps and context, provide a comprehensive answer.

Reasoning:
{chain_text}

Context:
{context}

Question: {state.original_query}

Answer:"""

        return PromptData(
            query=state.original_query,
            query_id=state.query_id,
            context=context,
            complexity=state.complexity,
            prompt=prompt,
        )


# ============================================================================
# è¿­ä»£é˜¶æ®µå°è£… (å¾ªç¯å±•å¼€æ¨¡å¼)
# ============================================================================


class IterationStage(MapFunction):
    """
    è¿­ä»£é˜¶æ®µ - å°è£…ä¸€è½® Retrieve + Reason

    è¿™æ˜¯å¾ªç¯å±•å¼€æ¨¡å¼çš„æ ¸å¿ƒï¼šæ¯ä¸ª Stage æ˜¯ç‹¬ç«‹çš„ MapFunctionï¼Œ
    å¯ä»¥ä¸²è”å¤šä¸ª Stage å®ç° N è½®è¿­ä»£ã€‚
    """

    def __init__(
        self,
        stage_id: int,
        top_k: int = 2,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.stage_id = stage_id
        self.top_k = top_k
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self._retriever = None
        self._reasoner = None

    def _get_retriever(self):
        if self._retriever is None:
            self._retriever = SimpleRetriever(top_k=self.top_k)
        return self._retriever

    def _get_reasoner(self):
        if self._reasoner is None:
            self._reasoner = IterativeReasoner(
                llm_base_url=self.llm_base_url,
                llm_model=self.llm_model,
            )
        return self._reasoner

    def execute(self, state: RetrievalState) -> RetrievalState:
        # å¦‚æœå‰ä¸€é˜¶æ®µå†³å®šåœæ­¢ï¼Œç›´æ¥é€ä¼ 
        if not state.should_continue:
            return state

        print(f"  ğŸ“ Stage {self.stage_id}: Processing...")

        # 1. æ£€ç´¢
        state = self._get_retriever().execute(state)

        # 2. æ¨ç†ï¼ˆå†³å®šæ˜¯å¦ç»§ç»­ï¼Œç”Ÿæˆå­æŸ¥è¯¢ï¼‰
        state = self._get_reasoner().execute(state)

        return state


# ============================================================================
# Filter ç®—å­
# ============================================================================


class ComplexityFilter(FilterFunction):
    """æŒ‰å¤æ‚åº¦è¿‡æ»¤"""

    def __init__(self, target_complexity: str, **kwargs):
        super().__init__(**kwargs)
        self.target_complexity = target_complexity

    def execute(self, data: ClassifiedQuery) -> bool:
        return data.complexity == self.target_complexity


# ============================================================================
# Sink ç®—å­
# ============================================================================


class ResultSink(SinkFunction):
    """ç»“æœæ”¶é›†å™¨"""

    _all_results: list[ResultData] = []

    def __init__(self, branch_name: str = "", **kwargs):
        super().__init__(**kwargs)
        self.branch_name = branch_name
        self.count = 0

    def execute(self, data: ResultData):
        self.count += 1
        ResultSink._all_results.append(data)

        answer_display = data.answer[:200] + "..." if len(data.answer) > 200 else data.answer
        print(
            f"\nğŸ¯ [{self.branch_name}] Result #{self.count}:\n"
            f"   Query: {data.query}\n"
            f"   Strategy: {data.strategy_used}\n"
            f"   Answer: {answer_display}"
        )

        return data

    @classmethod
    def get_all_results(cls) -> list[ResultData]:
        return cls._all_results.copy()

    @classmethod
    def clear_results(cls):
        cls._all_results.clear()


# ============================================================================
# Source ç®—å­
# ============================================================================


class QuerySource(SourceFunction):
    """æŸ¥è¯¢æº"""

    def __init__(self, queries: list[str], **kwargs):
        super().__init__(**kwargs)
        self.queries = queries
        self.index = 0

    def execute(self) -> QueryData | None:
        if self.index >= len(self.queries):
            return None

        query = self.queries[self.index]
        query_id = f"q_{self.index}"
        self.index += 1

        return QueryData(query=query, query_id=query_id)


# ============================================================================
# Pipeline æ„å»ºå‡½æ•°
# ============================================================================


def build_modular_pipeline(
    env: LocalEnvironment | FlownetEnvironment,
    queries: list[str],
    classifier_type: str = "rule",
    llm_base_url: str = "http://11.11.11.7:8903/v1",
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
    max_iterations: int = 3,
) -> LocalEnvironment | FlownetEnvironment:
    """
    æ„å»ºæ¨¡å—åŒ– Adaptive-RAG Pipeline

    æ¶æ„ç‰¹ç‚¹:
    1. ç®—å­è§£è€¦: Retriever, Prompter, Generator ç‹¬ç«‹
    2. è¿­ä»£å±•å¼€: MULTI åˆ†æ”¯ä½¿ç”¨ N ä¸ª IterationStage ä¸²è”

    Args:
        env: SAGE Environment
        queries: æŸ¥è¯¢åˆ—è¡¨
        classifier_type: åˆ†ç±»å™¨ç±»å‹
        llm_base_url: LLM æœåŠ¡åœ°å€
        llm_model: LLM æ¨¡å‹åç§°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

    Returns:
        é…ç½®å¥½çš„ Environment
    """

    # æ¸…ç©ºä¹‹å‰çš„ç»“æœ
    ResultSink.clear_results()

    # Source -> Classifier
    source = env.source(QuerySource(queries=queries))
    classified = source.map(QueryClassifier(classifier_type=classifier_type))

    # ========== åˆ†æ”¯ 1: ZERO (ç›´æ¥ç”Ÿæˆ) ==========
    zero_branch = classified.filter(ComplexityFilter(target_complexity="ZERO"))
    zero_prompt = zero_branch.map(SimplePrompter(template="direct"))
    zero_result = zero_prompt.map(
        SimpleGenerator(
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            strategy_name="no_retrieval",
        )
    )
    zero_result.sink(ResultSink(branch_name="ZERO"))

    # ========== åˆ†æ”¯ 2: SINGLE (å•æ¬¡æ£€ç´¢) ==========
    single_branch = classified.filter(ComplexityFilter(target_complexity="SINGLE"))
    single_state = single_branch.map(ClassifiedToState(max_iterations=1))
    single_retrieved = single_state.map(SimpleRetriever(top_k=3))
    single_prompt = single_retrieved.map(SimplePrompter(template="rag"))
    single_result = single_prompt.map(
        SimpleGenerator(
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            strategy_name="single_retrieval",
        )
    )
    single_result.sink(ResultSink(branch_name="SINGLE"))

    # ========== åˆ†æ”¯ 3: MULTI (è¿­ä»£æ£€ç´¢ - å¾ªç¯å±•å¼€æ¨¡å¼) ==========
    multi_branch = classified.filter(ComplexityFilter(target_complexity="MULTI"))
    multi_state = multi_branch.map(ClassifiedToState(max_iterations=max_iterations))

    # å¾ªç¯å±•å¼€: ä¸²è” N ä¸ª IterationStage
    current_state = multi_state
    for i in range(max_iterations):
        current_state = current_state.map(
            IterationStage(
                stage_id=i + 1,
                top_k=2,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
            )
        )

    # æœ€ç»ˆåˆæˆ
    multi_prompt = current_state.map(StateToPrompt())
    multi_result = multi_prompt.map(
        SimpleGenerator(
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            strategy_name="iterative_retrieval",
        )
    )
    multi_result.sink(ResultSink(branch_name="MULTI"))

    return env


# ============================================================================
# è¿è¡Œç¤ºä¾‹
# ============================================================================


def main():
    """è¿è¡Œæ¨¡å—åŒ– Adaptive-RAG Pipeline"""

    print("=" * 60)
    print("æ¨¡å—åŒ– Adaptive-RAG Pipeline")
    print("=" * 60)
    print("\nè®¾è®¡ç‰¹ç‚¹:")
    print("  1. ç®—å­è§£è€¦: Retriever, Prompter, Generator ç‹¬ç«‹")
    print("  2. è¿­ä»£å±•å¼€: ä¸²è” N ä¸ª IterationStage")
    print("=" * 60)

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        # ZERO - ç›´æ¥å›ç­”
        "What is 2 + 2?",
        # SINGLE - éœ€è¦æ£€ç´¢
        "What is machine learning?",
        # MULTI - å¤æ‚å¤šæ­¥æ¨ç†
        "Compare the differences between BERT and GPT, and explain how they relate to RAG systems.",
    ]

    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    use_remote = False  # è®¾ä¸º True ä½¿ç”¨ Flownet é›†ç¾¤

    if use_remote:
        print("\nğŸ”§ Using RemoteEnvironment (Flownet Cluster)")
        env = FlownetEnvironment(
            "modular-adaptive-rag",
            config={"flownet": {"address": "flownet://sage-node-1"}},
        )
    else:
        print("\nğŸ”§ Using LocalEnvironment")
        env = LocalEnvironment("modular-adaptive-rag")

    # æ„å»º Pipeline
    env = build_modular_pipeline(
        env=env,
        queries=test_queries,
        classifier_type="rule",
        llm_base_url="http://11.11.11.7:8903/v1",
        llm_model="Qwen/Qwen2.5-7B-Instruct",
        max_iterations=3,
    )

    # æ‰§è¡Œ
    print("\nğŸš€ Starting Pipeline...")
    start_time = time.time()

    env.submit(autostop=True)

    elapsed = time.time() - start_time
    print(f"\nâœ… Pipeline completed in {elapsed:.2f}s")

    # æ‰“å°æ±‡æ€»
    results = ResultSink.get_all_results()
    print(f"\nğŸ“Š Total results: {len(results)}")

    for r in results:
        print(f"  - [{r.complexity}] {r.query[:40]}... -> {r.strategy_used}")


if __name__ == "__main__":
    main()
