#!/usr/bin/env python3
"""
Adaptive-RAG æµåˆ†æ”¯ Pipeline å®ç°

è¿™ä¸ªç‰ˆæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ SAGE çš„æµåˆ†æ”¯æ¨¡å¼ (Multi-Branch Pipeline) æ¥å®ç° Adaptive-RAGã€‚
å…³é”®æ€æƒ³ï¼šå¯¹åŒä¸€ä¸ªåˆ†ç±»åçš„æµå¤šæ¬¡åº”ç”¨ filterï¼Œåˆ›å»ºä¸åŒçš„åˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯å¤„ç†ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢ã€‚

æµåˆ†æ”¯æ¨¡å¼ (å‚è€ƒ SAGE æ–‡æ¡£):
```
                    â”Œâ”€ filter(ZERO) â”€> NoRetrievalMap â”€> sink
    Source â”€> Map â”€â”¼â”€ filter(SINGLE) â”€> SingleRetrievalMap â”€> sink
                    â””â”€ filter(MULTI) â”€> IterativeRetrievalMap â”€> sink
```

ç”¨æ³•:
    from sage.kernel.api import LocalEnvironment
    from sage.benchmark.benchmark_sage.experiments.pipelines.adaptive_rag import (
        build_branching_adaptive_rag_pipeline
    )

    env = LocalEnvironment("adaptive-rag-branch")
    build_branching_adaptive_rag_pipeline(env, queries=["What is AI?", "Compare X and Y"])
    env.submit(autostop=True)
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from pathlib import Path

from sage.common.core import (
    FilterFunction,
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import LocalEnvironment
from sage.kernel.api.remote_environment import RemoteEnvironment

# æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—è¿è¡Œä¸¤ç§æ–¹å¼
try:
    from .classifier import (
        ClassificationResult,
        QueryComplexityLevel,
        create_classifier,
    )
except ImportError:
    from classifier import (
        ClassificationResult,
        QueryComplexityLevel,
        create_classifier,
    )


# ============================================================================
# æ•°æ®ç»“æ„
# ============================================================================


@dataclass
class QueryData:
    """æŸ¥è¯¢æ•°æ®"""

    query: str
    classification: ClassificationResult | None = None
    metadata: dict = field(default_factory=dict)


@dataclass
class ResultData:
    """ç»“æœæ•°æ®"""

    query: str
    answer: str
    strategy_used: str
    complexity: str
    retrieval_steps: int = 0
    processing_time_ms: float = 0.0


@dataclass
class IterativeState:
    """è¿­ä»£æ£€ç´¢çš„ä¸­é—´çŠ¶æ€ - åœ¨æµä¸­ä¼ é€’"""

    original_query: str  # åŸå§‹é—®é¢˜
    current_query: str  # å½“å‰æ£€ç´¢ query (å¯èƒ½æ˜¯å­æŸ¥è¯¢)
    accumulated_docs: list[dict] = field(default_factory=list)  # ç´¯ç§¯çš„æ–‡æ¡£
    reasoning_chain: list[str] = field(default_factory=list)  # æ¨ç†é“¾
    iteration: int = 0  # å½“å‰è¿­ä»£æ¬¡æ•°
    is_complete: bool = False  # æ˜¯å¦å·²å®Œæˆï¼ˆæå‰ç»ˆæ­¢ï¼‰
    start_time: float = 0.0  # å¼€å§‹æ—¶é—´
    classification: ClassificationResult | None = None


# ============================================================================
# Source: æŸ¥è¯¢æ•°æ®æº
# ============================================================================


class QuerySource(SourceFunction):
    """æŸ¥è¯¢æ•°æ®æº"""

    def __init__(self, queries: list[str], delay: float = 0.0, **kwargs):
        super().__init__(**kwargs)
        self.queries = queries
        self.delay = delay
        self.counter = 0

    def execute(self) -> QueryData | None:
        if self.counter >= len(self.queries):
            return None
        query = self.queries[self.counter]
        self.counter += 1
        if self.delay > 0:
            time.sleep(self.delay)
        print(f"ğŸ“¤ Source [{self.counter}/{len(self.queries)}]: {query}")
        return QueryData(query=query, metadata={"index": self.counter - 1})


# ============================================================================
# Classifier MapFunction
# ============================================================================


class ClassifierMap(MapFunction):
    """åˆ†ç±»å™¨ - å¯¹æŸ¥è¯¢è¿›è¡Œå¤æ‚åº¦åˆ†ç±»"""

    def __init__(self, classifier_type: str = "rule", **kwargs):
        super().__init__(**kwargs)
        self.classifier_type = classifier_type
        self._classifier = None

    def execute(self, data: QueryData) -> QueryData:
        if self._classifier is None:
            self._classifier = create_classifier(self.classifier_type)

        classification = self._classifier.classify(data.query)
        data.classification = classification

        print(f"ğŸ·ï¸ Classified: {data.query} -> {classification.complexity.name}")
        return data


# ============================================================================
# Filter Functions: æŒ‰å¤æ‚åº¦åˆ†æ”¯
# ============================================================================


class ZeroComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ ZERO (ç®€å•) å¤æ‚åº¦çš„æŸ¥è¯¢"""

    def execute(self, data: QueryData) -> bool:
        if data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.ZERO
        if is_match:
            print(f"  âœ… ZERO branch: {data.query}")
        return is_match


class SingleComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ SINGLE (ä¸­ç­‰) å¤æ‚åº¦çš„æŸ¥è¯¢"""

    def execute(self, data: QueryData) -> bool:
        if data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.SINGLE
        if is_match:
            print(f"  âœ… SINGLE branch: {data.query}")
        return is_match


class MultiComplexityFilter(FilterFunction):
    """è¿‡æ»¤: åªä¿ç•™ MULTI (å¤æ‚) å¤æ‚åº¦çš„æŸ¥è¯¢"""

    def execute(self, data: QueryData) -> bool:
        if data.classification is None:
            return False
        is_match = data.classification.complexity == QueryComplexityLevel.MULTI
        if is_match:
            print(f"  âœ… MULTI branch: {data.query}")
        return is_match


# ============================================================================
# Strategy MapFunctions: å„åˆ†æ”¯çš„å¤„ç†é€»è¾‘
# ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨ LLM API (å‚è€ƒ SimpleGenerator é£æ ¼)
# ============================================================================


class NoRetrievalStrategy(MapFunction):
    """
    ç­–ç•¥ A: æ— æ£€ç´¢ - ç›´æ¥ LLM ç”Ÿæˆ

    é€‚ç”¨äºç®€å•é—®é¢˜ï¼ŒLLM å¯ç›´æ¥å›ç­”ã€‚
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        max_tokens: int = 512,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens

    def _generate(self, query: str) -> str:
        """ç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆå›å¤"""
        import requests

        messages = [
            {"role": "user", "content": query},
        ]

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": self.max_tokens,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Generation Error] {str(e)}"

    def execute(self, data: QueryData) -> ResultData:
        start_time = time.time()

        print(f"  ğŸ”µ NoRetrieval: {data.query}")

        answer = self._generate(data.query)

        return ResultData(
            query=data.query,
            answer=answer,
            strategy_used="no_retrieval",
            complexity="ZERO",
            retrieval_steps=0,
            processing_time_ms=(time.time() - start_time) * 1000,
        )


class SingleRetrievalStrategy(MapFunction):
    """
    ç­–ç•¥ B: å•æ¬¡æ£€ç´¢ + ç”Ÿæˆ

    é€‚ç”¨äºéœ€è¦æ£€ç´¢ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚
    """

    # ç®€å•çŸ¥è¯†åº“ (å¯æ›¿æ¢ä¸ºçœŸå®æ£€ç´¢)
    KNOWLEDGE_BASE = [
        {
            "content": "Machine learning is a subset of artificial intelligence that learns from data.",
            "id": "1",
        },
        {"content": "Deep learning uses neural networks with multiple layers.", "id": "2"},
        {"content": "Python is a popular programming language for ML tasks.", "id": "3"},
        {"content": "BERT is a transformer-based model for NLP tasks.", "id": "4"},
        {"content": "RAG combines retrieval with generation for better answers.", "id": "5"},
    ]

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        max_tokens: int = 512,
        top_k: int = 3,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.top_k = top_k

    def _retrieve(self, query: str) -> list[dict]:
        """ç®€å•å…³é”®è¯æ£€ç´¢"""
        query_words = set(query.lower().split())
        scored_docs = []

        for doc in self.KNOWLEDGE_BASE:
            content_words = set(doc["content"].lower().split())
            overlap = len(query_words & content_words)
            if overlap > 0:
                scored_docs.append({**doc, "score": overlap / len(query_words)})

        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        return scored_docs[: self.top_k]

    def _generate(self, query: str, context: str) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤"""
        import requests

        messages = [
            {
                "role": "system",
                "content": "Answer the question based on the provided context. If no relevant info, say so.",
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuestion: {query}",
            },
        ]

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": self.max_tokens,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Generation Error] {str(e)}"

    def execute(self, data: QueryData) -> ResultData:
        start_time = time.time()

        print(f"  ğŸŸ¡ SingleRetrieval: {data.query}")

        # æ£€ç´¢
        docs = self._retrieve(data.query)
        context = "\n".join([f"[Doc {i + 1}]: {d['content']}" for i, d in enumerate(docs)])
        if not context:
            context = "No relevant documents found."

        # ç”Ÿæˆ
        answer = self._generate(data.query, context)

        return ResultData(
            query=data.query,
            answer=answer,
            strategy_used="single_retrieval",
            complexity="SINGLE",
            retrieval_steps=len(docs),
            processing_time_ms=(time.time() - start_time) * 1000,
        )


class IterativeRetrievalStrategy(MapFunction):
    """
    ç­–ç•¥ C: è¿­ä»£æ£€ç´¢ (IRCoT é£æ ¼) - åˆå§‹åŒ–çŠ¶æ€

    å°† QueryData è½¬æ¢ä¸º IterativeStateï¼Œå¼€å§‹è¿­ä»£æµç¨‹ã€‚
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def execute(self, data: QueryData) -> IterativeState:
        print(f"  ğŸ”´ IterativeRetrieval Init: {data.query}")
        return IterativeState(
            original_query=data.query,
            current_query=data.query,
            accumulated_docs=[],
            reasoning_chain=[],
            iteration=0,
            is_complete=False,
            start_time=time.time(),
            classification=data.classification,
        )


# ============================================================================
# è¿­ä»£æ£€ç´¢çš„ç‹¬ç«‹ç®—å­ (ç”¨äº MULTI åˆ†æ”¯çš„å¾ªç¯å±•å¼€)
# ============================================================================


class SimpleRetriever(MapFunction):
    """
    æ£€ç´¢ç®—å­ - æ ¹æ® current_query æ£€ç´¢æ–‡æ¡£

    è¾“å…¥: IterativeState
    è¾“å‡º: IterativeState (æ›´æ–° accumulated_docs)
    """

    KNOWLEDGE_BASE = [
        {
            "content": "Machine learning is a subset of artificial intelligence that learns from data.",
            "id": "1",
        },
        {"content": "Deep learning uses neural networks with multiple layers.", "id": "2"},
        {"content": "Python is a popular programming language for ML tasks.", "id": "3"},
        {"content": "BERT is a transformer-based model for NLP tasks.", "id": "4"},
        {"content": "RAG combines retrieval with generation for better answers.", "id": "5"},
        {"content": "Transformers use attention mechanisms for sequence modeling.", "id": "6"},
        {"content": "GPT models are autoregressive language models.", "id": "7"},
    ]

    def __init__(self, top_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.top_k = top_k

    def execute(self, state: IterativeState) -> IterativeState:
        # å¦‚æœå·²å®Œæˆï¼Œç›´æ¥é€ä¼ 
        if state.is_complete:
            return state

        # ç®€å•å…³é”®è¯æ£€ç´¢
        query_words = set(state.current_query.lower().split())
        scored_docs = []

        for doc in self.KNOWLEDGE_BASE:
            content_words = set(doc["content"].lower().split())
            overlap = len(query_words & content_words)
            if overlap > 0:
                scored_docs.append({**doc, "score": overlap / len(query_words)})

        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        new_docs = scored_docs[: self.top_k]

        # æ›´æ–°çŠ¶æ€
        state.accumulated_docs.extend(new_docs)
        state.reasoning_chain.append(
            f"[Retrieve] Query: '{state.current_query}' -> {len(new_docs)} docs"
        )

        print(
            f"    ğŸ“š Retrieve[{state.iteration}]: {len(new_docs)} docs for '{state.current_query[:30]}...'"
        )
        return state


class IterativeReasoner(MapFunction):
    """
    æ¨ç†ç®—å­ - åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿­ä»£ + ç”Ÿæˆä¸‹ä¸€ä¸ªå­æŸ¥è¯¢

    è¾“å…¥: IterativeState
    è¾“å‡º: IterativeState (æ›´æ–° current_query, iteration, is_complete)
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        max_iterations: int = 3,
        min_docs: int = 5,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.max_iterations = max_iterations
        self.min_docs = min_docs

    def _llm_call(self, messages: list[dict]) -> str:
        import requests

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": 256,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[LLM Error] {str(e)}"

    def execute(self, state: IterativeState) -> IterativeState:
        # å¦‚æœå·²å®Œæˆï¼Œç›´æ¥é€ä¼ 
        if state.is_complete:
            return state

        state.iteration += 1

        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if state.iteration >= self.max_iterations or len(state.accumulated_docs) >= self.min_docs:
            state.is_complete = True
            state.reasoning_chain.append(
                f"[Reason] Iteration {state.iteration}: Complete (docs={len(state.accumulated_docs)})"
            )
            print(f"    ğŸ§  Reason[{state.iteration}]: COMPLETE")
            return state

        # ç”Ÿæˆä¸‹ä¸€ä¸ªå­æŸ¥è¯¢
        context_so_far = "\n".join([f"- {d['content']}" for d in state.accumulated_docs[-3:]])
        messages = [
            {
                "role": "system",
                "content": "Generate a follow-up search query to find more information. Reply with ONLY the query.",
            },
            {
                "role": "user",
                "content": f"Original: {state.original_query}\n\nContext:\n{context_so_far}\n\nFollow-up query:",
            },
        ]
        new_query = self._llm_call(messages).strip()

        state.current_query = new_query
        state.reasoning_chain.append(
            f"[Reason] Iteration {state.iteration}: Next query = '{new_query[:50]}'"
        )
        print(f"    ğŸ§  Reason[{state.iteration}]: Next -> '{new_query[:40]}...'")
        return state


class FinalSynthesizer(MapFunction):
    """
    ç»¼åˆç”Ÿæˆç®—å­ - å°†æ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯åˆæˆæœ€ç»ˆç­”æ¡ˆ

    è¾“å…¥: IterativeState
    è¾“å‡º: ResultData
    """

    def __init__(
        self,
        llm_base_url: str = "http://11.11.11.7:8903/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model

    def _llm_call(self, messages: list[dict]) -> str:
        import requests

        try:
            response = requests.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": messages,
                    "max_tokens": 512,
                    "temperature": 0.7,
                },
                timeout=60,
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[LLM Error] {str(e)}"

    def execute(self, state: IterativeState) -> ResultData:
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n".join(
            [f"[Doc {i + 1}]: {d['content']}" for i, d in enumerate(state.accumulated_docs)]
        )
        chain_text = "\n".join(state.reasoning_chain)

        messages = [
            {"role": "system", "content": "Synthesize all information to answer comprehensively."},
            {
                "role": "user",
                "content": f"Question: {state.original_query}\n\nReasoning:\n{chain_text}\n\nContext:\n{context}\n\nAnswer:",
            },
        ]
        answer = self._llm_call(messages)

        print(f"    âœ¨ Synthesize: Generated answer ({len(answer)} chars)")

        return ResultData(
            query=state.original_query,
            answer=answer,
            strategy_used="iterative_retrieval",
            complexity="MULTI",
            retrieval_steps=state.iteration,
            processing_time_ms=(time.time() - state.start_time) * 1000,
        )


# ============================================================================
# Sink: ç»“æœæ”¶é›†å™¨
# ============================================================================


class ResultSink(SinkFunction):
    """
    ç»“æœæ”¶é›†å™¨

    Remote æ¨¡å¼ä¸‹å°†ç»“æœå†™å…¥æ–‡ä»¶ï¼Œæ”¯æŒè·¨èŠ‚ç‚¹æ”¶é›†ã€‚
    """

    # ç»“æœè¾“å‡ºç›®å½•
    RESULTS_OUTPUT_DIR = "/tmp/sage_adaptive_rag_results"

    _all_results: list[ResultData] = []  # Local æ¨¡å¼ç”¨

    def __init__(self, branch_name: str = "", **kwargs):
        super().__init__(**kwargs)
        self.branch_name = branch_name
        self.count = 0

        # åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶ (Remote æ¨¡å¼ç”¨)
        import os
        import socket

        self.instance_id = f"{socket.gethostname()}_{os.getpid()}_{int(time.time() * 1000)}"
        os.makedirs(self.RESULTS_OUTPUT_DIR, exist_ok=True)
        self.results_output_file = f"{self.RESULTS_OUTPUT_DIR}/results_{self.instance_id}.jsonl"

    def _write_to_file(self, data: ResultData) -> None:
        """å°†ç»“æœå†™å…¥æ–‡ä»¶ (Remote æ¨¡å¼)"""

        try:
            record = {
                "query": data.query,
                "answer": data.answer,
                "strategy_used": data.strategy_used,
                "complexity": data.complexity,
                "retrieval_steps": data.retrieval_steps,
                "processing_time_ms": data.processing_time_ms,
                "branch_name": self.branch_name,
                "timestamp": time.time(),
            }
            with open(self.results_output_file, "a") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception as e:
            import sys

            print(f"[ResultSink] Write error: {e}", file=sys.stderr, flush=True)

    def execute(self, data: ResultData):
        self.count += 1
        ResultSink._all_results.append(data)

        # å†™å…¥æ–‡ä»¶ (Remote æ¨¡å¼å¯æ”¶é›†)
        self._write_to_file(data)

        # æ‰“å°åˆ° stderr (Remote æ¨¡å¼ä¸‹å¯èƒ½çœ‹ä¸åˆ°ï¼Œä½† Local å¯ä»¥)
        import sys

        print(
            f"\nğŸ¯ [{self.branch_name}] Result #{self.count}:\n"
            f"   Query: {data.query}\n"
            f"   Strategy: {data.strategy_used}\n"
            f"   Complexity: {data.complexity}\n"
            f"   Retrieval Steps: {data.retrieval_steps}\n"
            f"   Time: {data.processing_time_ms:.1f}ms\n"
            f"   Answer:\n{data.answer}",
            file=sys.stderr,
            flush=True,
        )

        return data

    @classmethod
    def get_all_results(cls) -> list[ResultData]:
        return cls._all_results.copy()

    @classmethod
    def clear_results(cls):
        cls._all_results.clear()

    @classmethod
    def collect_from_files(cls) -> list[dict]:
        """
        ä»æ–‡ä»¶æ”¶é›†æ‰€æœ‰ç»“æœ (Remote æ¨¡å¼ç”¨)

        ä¼šä»æœ¬åœ°å’Œè¿œç¨‹èŠ‚ç‚¹æ”¶é›†ç»“æœæ–‡ä»¶ã€‚
        """
        import subprocess

        results_dir = Path(cls.RESULTS_OUTPUT_DIR)
        results_dir.mkdir(parents=True, exist_ok=True)

        # ä»è¿œç¨‹èŠ‚ç‚¹æ”¶é›†æ–‡ä»¶
        worker_nodes = ["sage-node-2", "sage-node-3", "sage-node-4"]
        for node in worker_nodes:
            try:
                cmd = f"scp -o StrictHostKeyChecking=no {node}:{cls.RESULTS_OUTPUT_DIR}/*.jsonl {cls.RESULTS_OUTPUT_DIR}/ 2>/dev/null"
                subprocess.run(cmd, shell=True, timeout=10)
            except Exception:
                pass

        # è¯»å–æ‰€æœ‰ç»“æœæ–‡ä»¶
        all_results = []
        for f in results_dir.glob("results_*.jsonl"):
            try:
                with open(f) as fp:
                    for line in fp:
                        if line.strip():
                            all_results.append(json.loads(line.strip()))
            except Exception:
                pass

        return all_results


# ============================================================================
# æµåˆ†æ”¯ Pipeline æ„å»ºå‡½æ•°
# ============================================================================


def build_branching_adaptive_rag_pipeline(
    env: LocalEnvironment | RemoteEnvironment,
    queries: list[str],
    classifier_type: str = "rule",
    llm_base_url: str = "http://11.11.11.7:8903/v1",
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
    max_iterations: int = 3,
) -> LocalEnvironment | RemoteEnvironment:
    """
    æ„å»ºæµåˆ†æ”¯æ¨¡å¼çš„ Adaptive-RAG Pipeline

    è¿™æ˜¯ SAGE æ¨èçš„å¤šåˆ†æ”¯æ¨¡å¼ï¼šå¯¹åŒä¸€ä¸ªæµå¤šæ¬¡åº”ç”¨ filter åˆ›å»ºä¸åŒåˆ†æ”¯ã€‚
    MULTI åˆ†æ”¯ä½¿ç”¨"å¾ªç¯å±•å¼€"æ¨¡å¼å®ç°è¿­ä»£æ£€ç´¢ã€‚

    æ¶æ„:
    ```
                          â”Œâ”€ filter(ZERO) â”€> Generator â”€> sink(ZERO)
                          â”‚
    Source â”€> Classifier â”€â”¼â”€ filter(SINGLE) â”€> Retriever â”€> Prompter â”€> Generator â”€> sink(SINGLE)
                          â”‚
                          â””â”€ filter(MULTI) â”€> InitState â”€â”¬â”€> Retrieve â”€> Reason â”€â”
                                                         â”‚                       â”‚
                                                         â”œâ”€> Retrieve â”€> Reason â”€â”¤ (å¾ªç¯å±•å¼€)
                                                         â”‚                       â”‚
                                                         â””â”€> Retrieve â”€> Reason â”€â”´â”€> Synthesize â”€> sink(MULTI)
    ```

    Args:
        env: SAGE LocalEnvironment æˆ– RemoteEnvironment
        queries: æŸ¥è¯¢åˆ—è¡¨
        classifier_type: åˆ†ç±»å™¨ç±»å‹
        llm_base_url: LLM æœåŠ¡åœ°å€
        llm_model: LLM æ¨¡å‹åç§°
        max_iterations: MULTI åˆ†æ”¯çš„æœ€å¤§è¿­ä»£æ¬¡æ•°

    Returns:
        é…ç½®å¥½çš„ Environment
    """
    ResultSink.clear_results()

    # Step 1: åˆ›å»º Source å’Œ Classifierï¼ˆå…±äº«çš„ä¸Šæ¸¸ï¼‰
    classified_stream = env.from_source(QuerySource, queries=queries, delay=0.1).map(
        ClassifierMap, classifier_type=classifier_type
    )

    # Step 2: åˆ†æ”¯ A - ZERO å¤æ‚åº¦ (æ— æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ)
    (
        classified_stream.filter(ZeroComplexityFilter)
        .map(NoRetrievalStrategy, llm_base_url=llm_base_url, llm_model=llm_model)
        .sink(ResultSink, branch_name="ZERO", parallelism=1)
    )

    # Step 3: åˆ†æ”¯ B - SINGLE å¤æ‚åº¦ (å•æ¬¡æ£€ç´¢ + ç”Ÿæˆ)
    (
        classified_stream.filter(SingleComplexityFilter)
        .map(SingleRetrievalStrategy, llm_base_url=llm_base_url, llm_model=llm_model)
        .sink(ResultSink, branch_name="SINGLE", parallelism=1)
    )

    # Step 4: åˆ†æ”¯ C - MULTI å¤æ‚åº¦ (è¿­ä»£æ£€ç´¢ - å¾ªç¯å±•å¼€æ¨¡å¼)
    # æ¶æ„: InitState -> [Retrieve -> Reason] x N -> Synthesize -> Sink
    multi_stream = (
        classified_stream.filter(MultiComplexityFilter).map(
            IterativeRetrievalStrategy
        )  # QueryData -> IterativeState
    )

    # å¾ªç¯å±•å¼€: ä¸²è” N ä¸ª [Retrieve -> Reason] Stage
    for i in range(max_iterations):
        multi_stream = (
            multi_stream.map(SimpleRetriever, top_k=3).map(  # æ£€ç´¢
                IterativeReasoner,
                llm_base_url=llm_base_url,
                llm_model=llm_model,
                max_iterations=max_iterations,
            )  # æ¨ç†
        )

    # æœ€ç»ˆç»¼åˆç”Ÿæˆ
    (
        multi_stream.map(FinalSynthesizer, llm_base_url=llm_base_url, llm_model=llm_model).sink(
            ResultSink, branch_name="MULTI", parallelism=1
        )  # IterativeState -> ResultData
    )

    return env


# ============================================================================
# ä¸»å‡½æ•° - æ¼”ç¤º
# ============================================================================


def main():
    """æ¼”ç¤ºæµåˆ†æ”¯ Adaptive-RAG Pipeline"""
    import sys

    # é»˜è®¤ä½¿ç”¨ Local æ¨¡å¼ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°åˆ‡æ¢
    use_remote = "--remote" in sys.argv

    mode_name = "Remote" if use_remote else "Local"
    print("=" * 70)
    print(f"Adaptive-RAG æµåˆ†æ”¯ Pipeline æ¼”ç¤º ({mode_name} Mode + çœŸå® LLM)")
    print("=" * 70)

    # LLM é…ç½® - ä¼ é€’é…ç½®å‚æ•°è€Œä¸æ˜¯å®¢æˆ·ç«¯å¯¹è±¡ï¼Œæ”¯æŒ Remote åºåˆ—åŒ–
    llm_base_url = "http://11.11.11.7:8903/v1"
    llm_model = "Qwen/Qwen2.5-7B-Instruct"

    print(f"\nğŸ”Œ LLM é…ç½®: {llm_base_url} / {llm_model}")

    queries = [
        "What is machine learning?",  # ZERO
        "What are the key features of Python 3.12?",  # ZERO
        "Compare Japan and Germany economic policies during 2008 crisis and their long-term effects on GDP",  # MULTI
        "Define artificial intelligence",  # ZERO
        "How does BERT work for NLP tasks?",  # SINGLE
    ]

    print(f"\nğŸ“‹ Processing {len(queries)} queries:")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q}")

    print("\n" + "-" * 70)
    print(f"ğŸš€ Building Multi-Branch Pipeline ({mode_name} Mode)...")
    print("-" * 70 + "\n")

    # æ ¹æ®æ¨¡å¼åˆ›å»ºç¯å¢ƒ
    if use_remote:
        env = RemoteEnvironment(
            name="adaptive-rag-branch",
            host="sage-node-1",  # Ray head node
        )
    else:
        env = LocalEnvironment(name="adaptive-rag-branch")

    build_branching_adaptive_rag_pipeline(
        env,
        queries=queries,
        llm_base_url=llm_base_url,
        llm_model=llm_model,
    )

    print("Pipeline structure (å¾ªç¯å±•å¼€æ¨¡å¼):")
    print("  Source -> Classifier -+-> filter(ZERO) -> Generator -> Sink")
    print("                        +-> filter(SINGLE) -> Retriever -> Generator -> Sink")
    print(
        "                        +-> filter(MULTI) -> InitState -+-> [Retrieve -> Reason] x3 -> Synthesize -> Sink"
    )
    print("                                            (å¾ªç¯å±•å¼€: ç‹¬ç«‹ç®—å­ä¸²è”)")
    print()

    try:
        env.submit(autostop=True)
        # ç­‰å¾…å®Œæˆ
        env._wait_for_completion()
    except Exception as e:
        print(f"\nâš ï¸ Pipeline error: {e}")
    finally:
        try:
            env.close()
        except Exception:
            pass

    # æ”¶é›†ç»“æœ
    if use_remote:
        # Remote æ¨¡å¼ï¼šä»æ–‡ä»¶æ”¶é›†
        print("\n[Collecting results from remote nodes...]")
        file_results = ResultSink.collect_from_files()
        print(f"[Collected {len(file_results)} results from files]")
    else:
        # Local æ¨¡å¼ï¼šç›´æ¥ä»ç±»å˜é‡è·å–
        file_results = None

    results = ResultSink.get_all_results() if not use_remote else []

    # åˆå¹¶ç»“æœ
    if file_results:
        total_results = file_results
    else:
        total_results = [
            {
                "query": r.query,
                "answer": r.answer,
                "strategy_used": r.strategy_used,
                "complexity": r.complexity,
                "retrieval_steps": r.retrieval_steps,
                "processing_time_ms": r.processing_time_ms,
            }
            for r in results
        ]

    print("\n" + "=" * 70)
    print(f"ğŸ“Š Summary: Processed {len(total_results)} queries")
    print("=" * 70)

    strategy_counts = {}
    for r in total_results:
        strategy = r.get(
            "strategy_used", r.strategy_used if hasattr(r, "strategy_used") else "unknown"
        )
        strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

    for strategy, count in strategy_counts.items():
        print(f"   - {strategy}: {count} queries")

    # æ‰“å°æ‰€æœ‰ç»“æœçš„è¯¦ç»†ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ“ All Results Details:")
    print("=" * 70)
    for i, r in enumerate(total_results, 1):
        if isinstance(r, dict):
            print(f"\n--- Result {i}/{len(total_results)} ---")
            print(f"Query: {r.get('query', 'N/A')}")
            print(f"Strategy: {r.get('strategy_used', 'N/A')}")
            print(f"Complexity: {r.get('complexity', 'N/A')}")
            print(f"Retrieval Steps: {r.get('retrieval_steps', 'N/A')}")
            print(f"Processing Time: {r.get('processing_time_ms', 0):.1f}ms")
            print(f"Answer:\n{r.get('answer', 'N/A')}")
            print("-" * 40)
        else:
            print(f"\n--- Result {i}/{len(total_results)} ---")
            print(f"Query: {r.query}")
            print(f"Strategy: {r.strategy_used}")
            print(f"Complexity: {r.complexity}")
            print(f"Retrieval Steps: {r.retrieval_steps}")
            print(f"Processing Time: {r.processing_time_ms:.1f}ms")
            print(f"Answer:\n{r.answer}")
            print("-" * 40)

    print("\nâœ… Multi-Branch Pipeline completed.")


# ============================================================================
# å¯¼å‡º
# ============================================================================

__all__ = [
    # æ•°æ®ç»“æ„
    "QueryData",
    "ResultData",
    "IterativeState",
    # Source & Classifier
    "QuerySource",
    "ClassifierMap",
    # Filters
    "ZeroComplexityFilter",
    "SingleComplexityFilter",
    "MultiComplexityFilter",
    # Strategy (ZERO/SINGLE)
    "NoRetrievalStrategy",
    "SingleRetrievalStrategy",
    # è¿­ä»£æ£€ç´¢ç‹¬ç«‹ç®—å­ (MULTI åˆ†æ”¯å¾ªç¯å±•å¼€)
    "IterativeRetrievalStrategy",  # InitState
    "SimpleRetriever",  # æ£€ç´¢ç®—å­
    "IterativeReasoner",  # æ¨ç†ç®—å­
    "FinalSynthesizer",  # ç»¼åˆç”Ÿæˆç®—å­
    # Sink & Builder
    "ResultSink",
    "build_branching_adaptive_rag_pipeline",
]


if __name__ == "__main__":
    main()
