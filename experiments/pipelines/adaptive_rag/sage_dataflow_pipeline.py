#!/usr/bin/env python3
"""
Adaptive-RAG SAGE æ•°æ®æµ Pipeline å®ç°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ SAGE æ•°æ®æµ API å®ç°ï¼Œä½¿ç”¨:
- env.from_source(SourceFunction) - æ•°æ®æº
- .map(MapFunction) - è½¬æ¢ç®—å­
- .filter(FilterFunction) - è¿‡æ»¤ç®—å­
- .flatmap(FlatMapFunction) - ä¸€å¯¹å¤šæ˜ å°„
- .sink(SinkFunction) - æ•°æ®æ±‡

å‚è€ƒè®ºæ–‡: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models
         through Question Complexity (NAACL 2024)

ç”¨æ³•ç¤ºä¾‹:
    from sage.kernel.api import LocalEnvironment
    from sage.benchmark.benchmark_sage.experiments.pipelines.adaptive_rag import (
        QuerySource, ClassifierMapFunction, AdaptiveRouterMapFunction, ResultSink
    )

    env = LocalEnvironment("adaptive-rag")

    # æ„å»ºå®Œæ•´çš„ Adaptive-RAG Pipeline
    (
        env.from_source(QuerySource, queries=["What is AI?", "Compare X and Y"])
        .map(ClassifierMapFunction)
        .map(AdaptiveRouterMapFunction)
        .sink(ResultSink, parallelism=1)
    )

    env.submit(autostop=True)
"""

from __future__ import annotations

import json
import tempfile
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

# ============================================================================
# SAGE æ ¸å¿ƒå¯¼å…¥
# ============================================================================
from sage.common.core import (
    FilterFunction,
    FlatMapFunction,
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import LocalEnvironment

# æœ¬åœ°åˆ†ç±»å™¨å¯¼å…¥
from .classifier import (
    ClassificationResult,
    QueryComplexityLevel,
    create_classifier,
)

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================


@dataclass
class QueryData:
    """æŸ¥è¯¢æ•°æ® - åœ¨ Pipeline ä¸­æµè½¬çš„æ•°æ®ç»“æ„"""

    query: str
    classification: ClassificationResult | None = None
    context: list[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query": self.query,
            "classification": {
                "complexity": self.classification.complexity.value,
                "confidence": self.classification.confidence,
            }
            if self.classification
            else None,
            "context": self.context,
            "metadata": self.metadata,
        }

    @classmethod
    def from_string(cls, query: str) -> QueryData:
        """ä»å­—ç¬¦ä¸²åˆ›å»º"""
        return cls(query=query, metadata={"created_at": time.time()})


@dataclass
class ResultData:
    """ç»“æœæ•°æ® - Pipeline è¾“å‡ºçš„æ•°æ®ç»“æ„"""

    query: str
    answer: str
    strategy_used: str = ""
    classification: ClassificationResult | None = None
    retrieval_steps: int = 0
    retrieved_docs: list[str] = field(default_factory=list)
    reasoning_chain: list[str] = field(default_factory=list)
    processing_time_ms: float = 0.0
    metadata: dict = field(default_factory=dict)

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query": self.query,
            "answer": self.answer,
            "strategy_used": self.strategy_used,
            "classification": {
                "complexity": self.classification.complexity.value,
                "confidence": self.classification.confidence,
            }
            if self.classification
            else None,
            "retrieval_steps": self.retrieval_steps,
            "retrieved_docs": self.retrieved_docs,
            "reasoning_chain": self.reasoning_chain,
            "processing_time_ms": self.processing_time_ms,
            "metadata": self.metadata,
        }


# ============================================================================
# SourceFunction: æŸ¥è¯¢æ•°æ®æº
# ============================================================================


class QuerySource(SourceFunction):
    """
    æŸ¥è¯¢æ•°æ®æº - SAGE SourceFunction å®ç°

    ç”¨æ³•:
        env.from_source(QuerySource, queries=["What is AI?", "Compare X and Y"])
        env.from_source(QuerySource, queries=query_list, delay=0.1)
    """

    def __init__(
        self,
        queries: list[str] | None = None,
        delay: float = 0.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.queries = queries or []
        self.delay = delay
        self.counter = 0
        self.total = len(self.queries)

    def execute(self) -> QueryData | None:
        """ç”Ÿæˆä¸‹ä¸€ä¸ªæŸ¥è¯¢æ•°æ®"""
        if self.counter >= self.total:
            return None  # ä¿¡å·ç»“æŸ

        query = self.queries[self.counter]
        self.counter += 1

        if self.delay > 0:
            time.sleep(self.delay)

        self.logger.info(f"QuerySource: emitted [{self.counter}/{self.total}]: {query[:50]}...")
        print(f"ğŸ“¤ QuerySource [{self.counter}/{self.total}]: {query[:50]}...")

        return QueryData.from_string(query)


# ============================================================================
# MapFunction: åˆ†ç±»å™¨
# ============================================================================


class ClassifierMapFunction(MapFunction):
    """
    åˆ†ç±»å™¨ MapFunction - å¯¹æŸ¥è¯¢è¿›è¡Œå¤æ‚åº¦åˆ†ç±»

    è¾“å…¥: QueryData
    è¾“å‡º: QueryData (å¸¦æœ‰ classification å­—æ®µ)

    ç”¨æ³•:
        stream.map(ClassifierMapFunction)
        stream.map(ClassifierMapFunction, classifier_type="llm", llm_client=client)
    """

    def __init__(
        self,
        classifier_type: str = "rule",
        llm_client: Any = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.classifier_type = classifier_type
        self.llm_client = llm_client
        self._classifier = None

    def setup(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self._classifier = create_classifier(self.classifier_type, llm_client=self.llm_client)
        self.logger.info(
            f"ClassifierMapFunction: initialized with {self.classifier_type} classifier"
        )

    def execute(self, data: QueryData) -> QueryData:
        """å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†ç±»"""
        if self._classifier is None:
            self.setup()

        classification = self._classifier.classify(data.query)
        data.classification = classification

        complexity_name = classification.complexity.name if classification else "UNKNOWN"
        self.logger.info(f"ClassifierMapFunction: classified -> {complexity_name}")
        print(f"ğŸ·ï¸ Classified: {data.query[:30]}... -> {complexity_name}")

        return data


# ============================================================================
# MapFunction: è‡ªé€‚åº”è·¯ç”±å™¨ (æ ¸å¿ƒç»„ä»¶)
# ============================================================================


class AdaptiveRouterMapFunction(MapFunction):
    """
    è‡ªé€‚åº”è·¯ç”±å™¨ MapFunction - æ ¹æ®åˆ†ç±»ç»“æœé€‰æ‹©å¹¶æ‰§è¡Œå¯¹åº” RAG ç­–ç•¥

    è¿™æ˜¯ Adaptive-RAG çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ç°äº†åŠ¨æ€ç­–ç•¥é€‰æ‹©:
    - Level A (NO_RETRIEVAL): ç›´æ¥ LLM ç”Ÿæˆ
    - Level B (SINGLE_HOP): å•æ¬¡æ£€ç´¢ + ç”Ÿæˆ
    - Level C (MULTI_HOP): è¿­ä»£æ£€ç´¢ (IRCoT é£æ ¼)

    è¾“å…¥: QueryData (å¸¦æœ‰ classification)
    è¾“å‡º: ResultData

    ç”¨æ³•:
        stream.map(AdaptiveRouterMapFunction)
        stream.map(AdaptiveRouterMapFunction, llm_client=client, retriever=retriever)
    """

    def __init__(
        self,
        llm_client: Any = None,
        retriever: Any = None,
        max_iterations: int = 5,
        top_k: int = 5,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_client = llm_client
        self.retriever = retriever
        self.max_iterations = max_iterations
        self.top_k = top_k

    def setup(self):
        """å»¶è¿Ÿåˆå§‹åŒ– LLM å’Œ Retriever"""
        if self.llm_client is None:
            try:
                from sage.common.components.sage_llm import UnifiedInferenceClient

                self.llm_client = UnifiedInferenceClient.create()
            except Exception:
                self.llm_client = MockLLMClient()
                self.logger.warning("Using MockLLMClient - no real LLM available")

        if self.retriever is None:
            self.retriever = MockRetriever()
            self.logger.warning("Using MockRetriever - no real retriever available")

    def execute(self, data: QueryData) -> ResultData:
        """æ ¹æ®åˆ†ç±»ç»“æœè·¯ç”±åˆ°å¯¹åº”ç­–ç•¥å¹¶æ‰§è¡Œ"""
        start_time = time.time()

        if self.llm_client is None:
            self.setup()

        # ç¡®å®šå¤æ‚åº¦çº§åˆ« (ä½¿ç”¨ complexity å±æ€§, æšä¸¾å€¼ä¸º ZERO/SINGLE/MULTI)
        if data.classification is None:
            complexity = QueryComplexityLevel.SINGLE
        else:
            complexity = data.classification.complexity

        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©ç­–ç•¥å¹¶æ‰§è¡Œ
        if complexity == QueryComplexityLevel.ZERO:
            result = self._execute_no_retrieval(data)
        elif complexity == QueryComplexityLevel.MULTI:
            result = self._execute_iterative_retrieval(data)
        else:  # SINGLE (default)
            result = self._execute_single_retrieval(data)

        # è®¡ç®—å¤„ç†æ—¶é—´
        result.processing_time_ms = (time.time() - start_time) * 1000

        self.logger.info(
            f"AdaptiveRouter: {data.query[:30]}... -> {result.strategy_used} "
            f"({result.processing_time_ms:.1f}ms)"
        )
        print(
            f"ğŸ”€ Routed: {data.query[:30]}... -> {result.strategy_used} "
            f"({result.retrieval_steps} retrieval steps)"
        )

        return result

    def _generate(self, prompt: str) -> str:
        """è°ƒç”¨ LLM ç”Ÿæˆ"""
        try:
            response = self.llm_client.chat(prompt)
            return response if isinstance(response, str) else str(response)
        except Exception as e:
            return f"[Error: {e}]"

    def _retrieve(self, query: str, top_k: int | None = None) -> list[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        k = top_k or self.top_k
        try:
            docs = self.retriever.retrieve(query, top_k=k)
            if isinstance(docs, list):
                return [str(d.get("content", d)) if isinstance(d, dict) else str(d) for d in docs]
            return [str(docs)]
        except Exception as e:
            return [f"[Retrieval Error: {e}]"]

    def _execute_no_retrieval(self, data: QueryData) -> ResultData:
        """ç­–ç•¥ A: æ— æ£€ç´¢ç›´æ¥ç”Ÿæˆ"""
        prompt = f"""è¯·ç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ˆæ— éœ€æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼‰ï¼š

é—®é¢˜: {data.query}

å›ç­”:"""

        answer = self._generate(prompt)

        return ResultData(
            query=data.query,
            answer=answer,
            strategy_used="no_retrieval",
            classification=data.classification,
            retrieval_steps=0,
            retrieved_docs=[],
            reasoning_chain=["Direct LLM generation without retrieval"],
            metadata=data.metadata,
        )

    def _execute_single_retrieval(self, data: QueryData) -> ResultData:
        """ç­–ç•¥ B: å•æ¬¡æ£€ç´¢ + ç”Ÿæˆ"""
        # Step 1: æ£€ç´¢
        retrieved_docs = self._retrieve(data.query)

        # Step 2: æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"[Doc {i + 1}]: {doc}" for i, doc in enumerate(retrieved_docs)])

        prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜: {data.query}

å›ç­”:"""

        answer = self._generate(prompt)

        return ResultData(
            query=data.query,
            answer=answer,
            strategy_used="single_retrieval",
            classification=data.classification,
            retrieval_steps=1,
            retrieved_docs=retrieved_docs,
            reasoning_chain=["Retrieved documents", "Generated answer with context"],
            metadata=data.metadata,
        )

    def _execute_iterative_retrieval(self, data: QueryData) -> ResultData:
        """ç­–ç•¥ C: è¿­ä»£æ£€ç´¢ (IRCoT é£æ ¼)"""
        reasoning_chain = []
        all_retrieved_docs = []

        # Step 1: åˆ†è§£é—®é¢˜
        decompose_prompt = f"""å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯ä»¥é€æ­¥å›ç­”çš„å­é—®é¢˜ï¼š

é—®é¢˜: {data.query}

è¯·åˆ—å‡ºå­é—®é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰:"""

        sub_questions_text = self._generate(decompose_prompt)
        sub_questions = [q.strip() for q in sub_questions_text.split("\n") if q.strip()]
        reasoning_chain.append(f"Decomposed into {len(sub_questions)} sub-questions")

        # Step 2: è¿­ä»£æ£€ç´¢å’Œæ€è€ƒ
        intermediate_answers = []

        for i, sub_q in enumerate(sub_questions[: self.max_iterations]):
            # æ£€ç´¢
            docs = self._retrieve(sub_q, top_k=3)
            all_retrieved_docs.extend(docs)

            # å›ç­”å­é—®é¢˜
            context = "\n".join(docs)
            think_prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”å­é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡: {context}

å­é—®é¢˜: {sub_q}

å›ç­”:"""

            sub_answer = self._generate(think_prompt)
            intermediate_answers.append(f"Q{i + 1}: {sub_q}\nA{i + 1}: {sub_answer}")
            reasoning_chain.append(f"Step {i + 1}: Answered '{sub_q[:30]}...'")

        # Step 3: ç»¼åˆ
        synthesis_prompt = f"""åŸºäºä»¥ä¸‹å­é—®é¢˜çš„å›ç­”ï¼Œç»¼åˆå›ç­”åŸå§‹é—®é¢˜ï¼š

åŸå§‹é—®é¢˜: {data.query}

å­é—®é¢˜å›ç­”:
{chr(10).join(intermediate_answers)}

ç»¼åˆå›ç­”:"""

        final_answer = self._generate(synthesis_prompt)
        reasoning_chain.append("Synthesized final answer")

        return ResultData(
            query=data.query,
            answer=final_answer,
            strategy_used="iterative_retrieval",
            classification=data.classification,
            retrieval_steps=len(sub_questions[: self.max_iterations]),
            retrieved_docs=all_retrieved_docs,
            reasoning_chain=reasoning_chain,
            metadata=data.metadata,
        )


# ============================================================================
# FilterFunction: æŒ‰å¤æ‚åº¦è¿‡æ»¤
# ============================================================================


class ComplexityFilterFunction(FilterFunction):
    """
    æŒ‰å¤æ‚åº¦çº§åˆ«è¿‡æ»¤æŸ¥è¯¢

    ç”¨æ³•:
        stream.filter(ComplexityFilterFunction, target_levels=["MULTI"])
        stream.filter(ComplexityFilterFunction, target_levels=["ZERO", "SINGLE"])
    """

    def __init__(
        self,
        target_levels: list[str] | None = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        if target_levels is None:
            self.target_levels = set(QueryComplexityLevel)
        else:
            # æ”¯æŒå­—ç¬¦ä¸²å’Œæšä¸¾ä¸¤ç§æ–¹å¼
            self.target_levels = set()
            for lvl in target_levels:
                if isinstance(lvl, str):
                    try:
                        self.target_levels.add(QueryComplexityLevel[lvl])
                    except KeyError:
                        # å°è¯•ä» value è§£æ
                        self.target_levels.add(QueryComplexityLevel.from_label(lvl))
                else:
                    self.target_levels.add(lvl)

    def execute(self, data: QueryData) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¿ç•™è¯¥æŸ¥è¯¢"""
        if data.classification is None:
            return True

        is_match = data.classification.complexity in self.target_levels
        complexity_name = data.classification.complexity.name

        if is_match:
            print(f"âœ… Filter: accepted {complexity_name}")
        else:
            print(f"âŒ Filter: rejected {complexity_name}")

        return is_match


# ============================================================================
# SinkFunction: ç»“æœæ”¶é›†å™¨
# ============================================================================


class ResultSink(SinkFunction):
    """
    ç»“æœæ”¶é›†å™¨ - SAGE SinkFunction å®ç°

    ç”¨æ³•:
        stream.sink(ResultSink)
        stream.sink(ResultSink, output_file="/path/to/results.jsonl", verbose=True)
    """

    # ç±»çº§åˆ«ç»“æœå­˜å‚¨ï¼ˆç”¨äºéªŒè¯ï¼‰
    _all_results: list[ResultData] = []

    def __init__(
        self,
        output_file: str | None = None,
        verbose: bool = True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.verbose = verbose
        self.received_count = 0

        if output_file is None:
            self.output_file = Path(tempfile.gettempdir()) / "adaptive_rag_results.jsonl"
        else:
            self.output_file = Path(output_file)

    def execute(self, data: ResultData):
        """æ¥æ”¶å¹¶å­˜å‚¨ç»“æœ"""
        self.received_count += 1
        ResultSink._all_results.append(data)

        if self.verbose:
            print(
                f"\nğŸ¯ Result #{self.received_count}:\n"
                f"   Query: {data.query[:60]}...\n"
                f"   Strategy: {data.strategy_used}\n"
                f"   Steps: {data.retrieval_steps}\n"
                f"   Answer: {data.answer[:100]}...\n"
                f"   Time: {data.processing_time_ms:.1f}ms"
            )

        # è¿½åŠ å†™å…¥æ–‡ä»¶
        try:
            with open(self.output_file, "a") as f:
                f.write(json.dumps(data.to_dict(), ensure_ascii=False) + "\n")
        except Exception as e:
            self.logger.error(f"Write error: {e}")

        return data

    @classmethod
    def get_all_results(cls) -> list[ResultData]:
        """è·å–æ‰€æœ‰ç»“æœ"""
        return cls._all_results.copy()

    @classmethod
    def clear_results(cls):
        """æ¸…ç©ºç»“æœ"""
        cls._all_results.clear()


# ============================================================================
# FlatMapFunction: ç­–ç•¥åˆ†æ”¯ï¼ˆé«˜çº§ç”¨æ³•ï¼‰
# ============================================================================


class StrategyBranchFlatMap(FlatMapFunction):
    """
    ç­–ç•¥åˆ†æ”¯ FlatMap - å°†æŸ¥è¯¢åˆ†å‘åˆ°ä¸åŒç­–ç•¥æ ‡ç­¾

    ç”¨æ³•:
        stream.flatmap(StrategyBranchFlatMap)
    """

    def execute(self, data: QueryData) -> list[dict]:
        """å°†æŸ¥è¯¢æ ‡è®°å¹¶åˆ†å‘"""
        complexity = (
            data.classification.complexity if data.classification else QueryComplexityLevel.SINGLE
        )

        strategy_map = {
            QueryComplexityLevel.ZERO: "no_retrieval",
            QueryComplexityLevel.SINGLE: "single_retrieval",
            QueryComplexityLevel.MULTI: "iterative_retrieval",
        }

        return [
            {
                "tag": "routed_query",
                "strategy": strategy_map.get(complexity, "single_retrieval"),
                "data": data,
            }
        ]


# ============================================================================
# Mock å®ç°ï¼ˆç”¨äºæ— çœŸå®æœåŠ¡æ—¶çš„æµ‹è¯•ï¼‰
# ============================================================================


class MockLLMClient:
    """Mock LLM å®¢æˆ·ç«¯"""

    def chat(self, prompt: str) -> str:
        return f"[Mock LLM Response for: {prompt[:50]}...]"

    def generate(self, prompt: str) -> str:
        return self.chat(prompt)


class MockRetriever:
    """Mock æ£€ç´¢å™¨"""

    def retrieve(self, query: str, top_k: int = 5) -> list[dict]:
        return [{"content": f"[Mock Doc {i + 1} for: {query[:30]}]"} for i in range(top_k)]


# ============================================================================
# å®Œæ•´ Pipeline æ„å»ºå‡½æ•°
# ============================================================================


def build_adaptive_rag_pipeline(
    env: LocalEnvironment,
    queries: list[str],
    classifier_type: str = "rule",
    llm_client: Any = None,
    retriever: Any = None,
    verbose: bool = True,
) -> LocalEnvironment:
    """
    æ„å»ºå®Œæ•´çš„ Adaptive-RAG Pipeline

    Args:
        env: SAGE LocalEnvironment
        queries: æŸ¥è¯¢åˆ—è¡¨
        classifier_type: åˆ†ç±»å™¨ç±»å‹ ("rule", "llm", "t5")
        llm_client: LLM å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        retriever: æ£€ç´¢å™¨ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        é…ç½®å¥½çš„ Environment

    ç”¨æ³•:
        env = LocalEnvironment("adaptive-rag")
        build_adaptive_rag_pipeline(env, queries=["What is AI?", "Compare X and Y"])
        env.submit(autostop=True)
    """
    # æ¸…ç©ºä¹‹å‰çš„ç»“æœ
    ResultSink.clear_results()

    # æ„å»º Pipeline
    (
        env.from_source(QuerySource, queries=queries, delay=0.1)
        .map(ClassifierMapFunction, classifier_type=classifier_type, llm_client=llm_client)
        .map(AdaptiveRouterMapFunction, llm_client=llm_client, retriever=retriever)
        .sink(ResultSink, verbose=verbose, parallelism=1)
    )

    return env


# ============================================================================
# ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•
# ============================================================================


def main():
    """æ¼”ç¤º Adaptive-RAG SAGE æ•°æ®æµ Pipeline"""
    print("=" * 70)
    print("Adaptive-RAG SAGE æ•°æ®æµ Pipeline æ¼”ç¤º")
    print("=" * 70)

    # ç¤ºä¾‹æŸ¥è¯¢ï¼ˆè¦†ç›–ä¸‰ç§å¤æ‚åº¦çº§åˆ«ï¼‰
    queries = [
        # Level A (ç®€å•): å®šä¹‰ç±»é—®é¢˜
        "What is machine learning?",
        # Level B (ä¸­ç­‰): éœ€è¦äº‹å®æ”¯æ’‘
        "What are the key features of Python 3.12?",
        # Level C (å¤æ‚): å¤šè·³æ¯”è¾ƒé—®é¢˜
        "Compare the economic policies of Japan and Germany in handling the 2008 financial crisis, and analyze their long-term effects on GDP growth.",
    ]

    print(f"\nğŸ“‹ Processing {len(queries)} queries:")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q[:60]}...")

    print("\n" + "-" * 70)
    print("ğŸš€ Building and executing SAGE Pipeline...")
    print("-" * 70 + "\n")

    # åˆ›å»ºç¯å¢ƒ
    env = LocalEnvironment("adaptive-rag-demo")

    # æ„å»º Pipeline
    build_adaptive_rag_pipeline(env, queries=queries)

    # æ‰§è¡Œ
    try:
        env.submit(autostop=True)
        time.sleep(2)  # ç­‰å¾…å¤„ç†å®Œæˆ
    finally:
        env.close()

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    results = ResultSink.get_all_results()
    print("\n" + "=" * 70)
    print(f"ğŸ“Š Summary: Processed {len(results)} queries")
    print("=" * 70)

    strategy_counts = {}
    for r in results:
        strategy_counts[r.strategy_used] = strategy_counts.get(r.strategy_used, 0) + 1

    for strategy, count in strategy_counts.items():
        print(f"   - {strategy}: {count} queries")

    print(f"\nğŸ“ Results saved to: {ResultSink._all_results[0].metadata if results else 'N/A'}")
    print("âœ… Done.")


# ============================================================================
# å¯¼å‡º
# ============================================================================

__all__ = [
    # æ•°æ®ç»“æ„
    "QueryData",
    "ResultData",
    # Source
    "QuerySource",
    # Map Functions
    "ClassifierMapFunction",
    "AdaptiveRouterMapFunction",
    # Filter
    "ComplexityFilterFunction",
    # FlatMap
    "StrategyBranchFlatMap",
    # Sink
    "ResultSink",
    # æ„å»ºå‡½æ•°
    "build_adaptive_rag_pipeline",
    # Mock
    "MockLLMClient",
    "MockRetriever",
]


if __name__ == "__main__":
    main()
