"""
Pipeline E: Priority Scheduling (ä¼˜å…ˆçº§è°ƒåº¦)
============================================

æ‹“æ‰‘: SourceÃ—3 â†’ KeyBy(Scheduler) â†’ Batch â†’ Future(LLM) â†’ Sink

ç®—å­:
- SourceÃ—3: ä¸åŒä¼˜å…ˆçº§çš„è¯·æ±‚æº (High, Medium, Low)
- Map (KeyBy/Scheduler): æ ¹æ®ä¼˜å…ˆçº§è°ƒåº¦
- Map (Batch): æŒ‰ä¼˜å…ˆçº§åˆ†æ‰¹
- Map (LLM): LLM è°ƒç”¨ (Future è¯­ä¹‰)
- Sink: ç»“æœè¾“å‡º + SLO ç»Ÿè®¡

è°ƒåº¦ç­–ç•¥:
- FIFO: å…ˆè¿›å…ˆå‡º
- Priority: é«˜ä¼˜å…ˆçº§ä¼˜å…ˆ
- SLO-Aware: ä¼˜å…ˆå¤„ç†å³å°†è¿çº¦çš„è¯·æ±‚
- Hybrid: ç»¼åˆä¼˜å…ˆçº§å’Œ SLO ç´§è¿«åº¦
"""

from __future__ import annotations

import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Optional

import httpx

from sage.common.core import (
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import RemoteEnvironment

from .scheduler import HeadNodeScheduler


class SchedulerType(str, Enum):
    """è°ƒåº¦å™¨ç±»å‹"""

    FIFO = "fifo"
    PRIORITY = "priority"
    SLO_AWARE = "slo_aware"
    HYBRID = "hybrid"


class RequestPriority(str, Enum):
    """è¯·æ±‚ä¼˜å…ˆçº§"""

    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class SchedulingConfig:
    """Scheduling Pipeline é…ç½®"""

    # è¯·æ±‚é…ç½®
    num_requests: int = 200
    high_priority_ratio: float = 0.2
    medium_priority_ratio: float = 0.5
    low_priority_ratio: float = 0.3

    # SLO ç›®æ ‡
    high_priority_slo_ms: float = 500.0
    medium_priority_slo_ms: float = 1000.0
    low_priority_slo_ms: float = 2000.0

    # è°ƒåº¦ç­–ç•¥
    scheduler_type: SchedulerType = SchedulerType.HYBRID

    # æ‰¹å¤„ç†
    batch_size: int = 8

    # æ¨¡å‹
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # æœåŠ¡ç«¯ç‚¹
    llm_base_url: str = "http://localhost:8001/v1"

    # è¿è¡Œæ—¶
    job_manager_host: str = "localhost"
    job_manager_port: int = 19001
    request_timeout: float = 60.0


@dataclass
class SchedulingRequest:
    """è°ƒåº¦è¯·æ±‚"""

    request_id: str
    priority: RequestPriority
    slo_target_ms: float
    query: str
    arrival_time_ms: float
    answer: str = ""
    completion_time_ms: float = 0.0

    @property
    def latency_ms(self) -> float:
        if self.completion_time_ms == 0:
            return 0.0
        return self.completion_time_ms - self.arrival_time_ms

    @property
    def slo_met(self) -> bool:
        return self.latency_ms <= self.slo_target_ms


# ============================================================================
# Source: å¤šä¼˜å…ˆçº§è¯·æ±‚æº
# ============================================================================


class SchedulingSourceFunction(SourceFunction):
    """Scheduling Source: ç”Ÿæˆä¸åŒä¼˜å…ˆçº§çš„è¯·æ±‚"""

    def __init__(
        self,
        num_requests: int = 200,
        high_priority_ratio: float = 0.2,
        medium_priority_ratio: float = 0.5,
        low_priority_ratio: float = 0.3,
        high_priority_slo_ms: float = 500.0,
        medium_priority_slo_ms: float = 1000.0,
        low_priority_slo_ms: float = 2000.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.num_requests = num_requests
        self.high_ratio = high_priority_ratio
        self.medium_ratio = medium_priority_ratio
        self.low_ratio = low_priority_ratio
        self.high_slo = high_priority_slo_ms
        self.medium_slo = medium_priority_slo_ms
        self.low_slo = low_priority_slo_ms
        self._index = 0
        self._base_time = time.time() * 1000

    def execute(self, data: Any = None) -> Optional[SchedulingRequest]:
        """è¿”å›ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        if self._index >= self.num_requests:
            return None

        i = self._index
        self._index += 1

        # ç¡®å®šæ€§åˆ†é…ä¼˜å…ˆçº§
        segment = i % 10
        high_threshold = int(self.high_ratio * 10)
        medium_threshold = high_threshold + int(self.medium_ratio * 10)

        if segment < high_threshold:
            priority = RequestPriority.HIGH
            slo = self.high_slo
        elif segment < medium_threshold:
            priority = RequestPriority.MEDIUM
            slo = self.medium_slo
        else:
            priority = RequestPriority.LOW
            slo = self.low_slo

        return SchedulingRequest(
            request_id=f"req_{i:04d}",
            priority=priority,
            slo_target_ms=slo,
            query=f"Answer briefly: What is {i} times 2?",
            arrival_time_ms=self._base_time + i * 10,
        )


# ============================================================================
# Map (Scheduler): è°ƒåº¦å™¨
# ============================================================================


class SchedulerMapFunction(MapFunction):
    """Map (Scheduler): æ ¹æ®ç­–ç•¥è°ƒåº¦è¯·æ±‚"""

    def __init__(self, scheduler_type: str = "hybrid", batch_size: int = 8, **kwargs):
        super().__init__(**kwargs)
        self.scheduler_type = scheduler_type
        self.batch_size = batch_size
        self._queue: list[SchedulingRequest] = []

    def execute(self, request: SchedulingRequest) -> Optional[list[SchedulingRequest]]:
        """æ‰§è¡Œè°ƒåº¦"""
        self._queue.append(request)

        if len(self._queue) >= self.batch_size:
            # æ ¹æ®ç­–ç•¥æ’åº
            batch = self._schedule_batch()
            print(f"ğŸ“‹ Scheduled batch: {[r.priority.value for r in batch]}")
            return batch

        return None

    def _schedule_batch(self) -> list[SchedulingRequest]:
        """æ ¹æ®ç­–ç•¥è°ƒåº¦æ‰¹æ¬¡"""
        batch = self._queue[: self.batch_size]
        self._queue = self._queue[self.batch_size :]

        if self.scheduler_type == "fifo":
            # FIFO: ä¿æŒåŸå§‹é¡ºåº
            pass
        elif self.scheduler_type == "priority":
            # Priority: æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_order = {
                RequestPriority.HIGH: 0,
                RequestPriority.MEDIUM: 1,
                RequestPriority.LOW: 2,
            }
            batch.sort(key=lambda r: priority_order[r.priority])
        elif self.scheduler_type == "slo_aware":
            # SLO-Aware: æŒ‰ slack time æ’åº
            current_time = time.time() * 1000
            for req in batch:
                elapsed = current_time - req.arrival_time_ms
                req._slack = req.slo_target_ms - elapsed
            batch.sort(key=lambda r: r._slack)
        else:  # hybrid
            # Hybrid: ç»¼åˆä¼˜å…ˆçº§å’Œ SLO
            priority_order = {
                RequestPriority.HIGH: 0,
                RequestPriority.MEDIUM: 1,
                RequestPriority.LOW: 2,
            }
            current_time = time.time() * 1000
            for req in batch:
                elapsed = current_time - req.arrival_time_ms
                slack = max(req.slo_target_ms - elapsed, 1)
                priority_score = priority_order[req.priority]
                req._score = 0.6 * priority_score + 0.4 * (1000 / slack)
            batch.sort(key=lambda r: r._score)

        return batch


# ============================================================================
# Map (LLM): LLM è°ƒç”¨
# ============================================================================


class SchedulingLLMMapFunction(MapFunction):
    """Map (LLM): æ‰¹é‡ LLM è°ƒç”¨"""

    def __init__(
        self,
        llm_base_url: str = "http://localhost:8001/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: float = 60.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.timeout = timeout

    def execute(
        self, batch: Optional[list[SchedulingRequest]]
    ) -> Optional[list[SchedulingRequest]]:
        """æ‰§è¡Œ LLM è°ƒç”¨"""
        if batch is None:
            return None

        with httpx.Client(timeout=self.timeout) as client:
            for req in batch:
                response = client.post(
                    f"{self.llm_base_url}/chat/completions",
                    json={
                        "model": self.llm_model,
                        "messages": [{"role": "user", "content": req.query}],
                        "max_tokens": 64,
                        "temperature": 0.7,
                    },
                )
                response.raise_for_status()
                result = response.json()
                req.answer = result["choices"][0]["message"]["content"]
                req.completion_time_ms = time.time() * 1000

        return batch


# ============================================================================
# Sink: ç»“æœè¾“å‡º + SLO ç»Ÿè®¡
# ============================================================================


class SchedulingSinkFunction(SinkFunction):
    """Scheduling Sink: è¾“å‡ºç»“æœå¹¶ç»Ÿè®¡ SLO"""

    def __init__(self, output_path: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.results: list[dict] = []
        self.slo_stats: dict[str, dict] = {
            "high": {"total": 0, "met": 0},
            "medium": {"total": 0, "met": 0},
            "low": {"total": 0, "met": 0},
        }

    def execute(self, batch: Optional[list[SchedulingRequest]]) -> None:
        """è¾“å‡ºç»“æœ"""
        if batch is None:
            return

        for req in batch:
            priority = req.priority.value
            self.slo_stats[priority]["total"] += 1
            if req.slo_met:
                self.slo_stats[priority]["met"] += 1

            status = "âœ…" if req.slo_met else "âŒ"
            print(
                f"{status} [{req.request_id}] {priority} latency={req.latency_ms:.0f}ms (SLO={req.slo_target_ms}ms)"
            )

            result = {
                "request_id": req.request_id,
                "priority": priority,
                "slo_target_ms": req.slo_target_ms,
                "latency_ms": req.latency_ms,
                "slo_met": req.slo_met,
                "answer": req.answer,
            }
            self.results.append(result)

        if self.output_path:
            import json

            with open(self.output_path, "a") as f:
                for r in self.results[-len(batch) :]:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")

    def get_slo_compliance(self) -> dict[str, float]:
        """è·å– SLO åˆè§„ç‡"""
        compliance = {}
        for priority, stats in self.slo_stats.items():
            if stats["total"] > 0:
                compliance[priority] = stats["met"] / stats["total"]
            else:
                compliance[priority] = 0.0
        return compliance


# ============================================================================
# Scheduling Pipeline å°è£…
# ============================================================================


class SchedulingPipeline:
    """Scheduling Pipeline å°è£…ç±»"""

    def __init__(self, config: SchedulingConfig):
        self.config = config
        self.env: Optional[RemoteEnvironment] = None

    def build(self) -> RemoteEnvironment:
        """æ„å»º Scheduling Pipeline"""
        scheduler = HeadNodeScheduler()

        self.env = RemoteEnvironment(
            "scheduling_pipeline",
            host=self.config.job_manager_host,
            port=self.config.job_manager_port,
            scheduler=scheduler,
        )

        # æ„å»º Pipeline: Source â†’ Map(Scheduler) â†’ Map(LLM) â†’ Sink
        (
            self.env.from_source(
                SchedulingSourceFunction,
                num_requests=self.config.num_requests,
                high_priority_ratio=self.config.high_priority_ratio,
                medium_priority_ratio=self.config.medium_priority_ratio,
                low_priority_ratio=self.config.low_priority_ratio,
                high_priority_slo_ms=self.config.high_priority_slo_ms,
                medium_priority_slo_ms=self.config.medium_priority_slo_ms,
                low_priority_slo_ms=self.config.low_priority_slo_ms,
            )
            .map(
                SchedulerMapFunction,
                scheduler_type=self.config.scheduler_type.value,
                batch_size=self.config.batch_size,
            )
            .map(
                SchedulingLLMMapFunction,
                llm_base_url=self.config.llm_base_url,
                llm_model=self.config.llm_model,
                timeout=self.config.request_timeout,
            )
            .sink(SchedulingSinkFunction)
        )

        return self.env

    def run(self) -> dict:
        """è¿è¡Œ Pipeline"""
        if self.env is None:
            self.build()

        start_time = time.time()
        try:
            self.env.submit()
            time.sleep(20)  # è°ƒåº¦æµ‹è¯•éœ€è¦æ›´å¤šæ—¶é—´
        finally:
            self.env.close()

        duration = time.time() - start_time
        return {
            "pipeline": "E (Scheduling)",
            "duration_seconds": duration,
            "config": {
                "num_requests": self.config.num_requests,
                "scheduler_type": self.config.scheduler_type.value,
            },
        }
