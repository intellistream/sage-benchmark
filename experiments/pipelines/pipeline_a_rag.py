"""
Pipeline A: RAG Pipeline (æ£€ç´¢å¢å¼ºç”Ÿæˆ)
======================================

æ‹“æ‰‘: Source â†’ Map(Embedding) â†’ Map(Retrieval) â†’ Filter(Rerank) â†’ Future(LLM) â†’ Sink

ç®—å­:
- Source: åŠ è½½é—®ç­”æ•°æ®é›† (qa_base/MMLU/BBH)
- Map (Embedding): æŸ¥è¯¢å‘é‡åŒ–
- Map (Retrieval): å‘é‡æ£€ç´¢ / BM25 / æ··åˆæ£€ç´¢
- Filter (Rerank): é‡æ’åºå¹¶è¿‡æ»¤ top-k
- Future (LLM): å¼‚æ­¥ LLM è°ƒç”¨ç”Ÿæˆç­”æ¡ˆ
- Sink: è¾“å‡ºç»“æœ

æ•°æ®é›†: qa_base, MMLU, BBH
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Optional

import httpx

from sage.common.core import (
    FilterFunction,
    MapFunction,
    SinkFunction,
    SourceFunction,
)
from sage.kernel.api import RemoteEnvironment

try:
    from .scheduler import HeadNodeScheduler
except ImportError:
    # ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    from sage.benchmark.benchmark_sage.experiments.pipelines.scheduler import (
        HeadNodeScheduler,
    )


class RetrievalMode(str, Enum):
    """æ£€ç´¢æ¨¡å¼"""

    DENSE = "dense"  # å‘é‡æ£€ç´¢
    SPARSE = "sparse"  # BM25
    HYBRID = "hybrid"  # æ··åˆ


@dataclass
class RAGConfig:
    """RAG Pipeline é…ç½®"""

    # æ•°æ®é›†
    dataset_name: str = "qa_base"
    num_samples: int = 100

    # æ£€ç´¢
    retrieval_mode: RetrievalMode = RetrievalMode.HYBRID
    top_k: int = 5
    rerank_top_k: int = 3

    # æ¨¡å‹
    embedding_model: str = "BAAI/bge-large-en-v1.5"
    llm_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # æœåŠ¡ç«¯ç‚¹
    embedding_base_url: str = "http://localhost:8090/v1"
    llm_base_url: str = "http://localhost:8001/v1"

    # è¿è¡Œæ—¶
    job_manager_host: str = "localhost"
    job_manager_port: int = 19001
    request_timeout: float = 60.0

    # è¾“å‡º
    output_path: Optional[str] = None


@dataclass
class Document:
    """æ£€ç´¢åˆ°çš„æ–‡æ¡£"""

    text: str
    score: float
    metadata: dict = field(default_factory=dict)


# ============================================================================
# Source: æ•°æ®é›†åŠ è½½
# ============================================================================


class RAGSourceFunction(SourceFunction):
    """RAG Source: ä»æ•°æ®é›†åŠ è½½é—®ç­”æ•°æ®

    æ”¯æŒæ•°æ®é›†:
    - qa_base: å†…ç½®é—®ç­”æ•°æ®é›†
    - mmlu: MMLU å¤šé€‰é¢˜
    - bbh: BIG-Bench Hard
    """

    def __init__(
        self,
        dataset_name: str = "qa_base",
        num_samples: int = 100,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.dataset_name = dataset_name
        self.num_samples = num_samples
        self._data: list[dict] = []
        self._index = 0
        self._loaded = False

    def _load_data(self) -> None:
        """åŠ è½½æ•°æ®é›†"""
        if self._loaded:
            return

        # åŠ¨æ€å¯¼å…¥æ•°æ®åŠ è½½å™¨
        if self.dataset_name == "qa_base":
            from sage.data.sources.qa_base.dataloader import QADataLoader

            loader = QADataLoader()
            raw_data = loader.load_queries()  # ä½¿ç”¨ load_queries() æ–¹æ³•
        elif self.dataset_name == "mmlu":
            from sage.data.sources.mmlu.dataloader import MMLUDataLoader

            loader = MMLUDataLoader()
            raw_data = loader.load()
        elif self.dataset_name == "bbh":
            from sage.data.sources.bbh.dataloader import BBHDataLoader

            loader = BBHDataLoader()
            raw_data = loader.load()
        else:
            raise ValueError(f"Unknown dataset: {self.dataset_name}")

        self._data = raw_data[: self.num_samples]
        self._loaded = True
        print(f"ğŸ“‚ Loaded {len(self._data)} samples from {self.dataset_name}")

    def execute(self, data: Any = None) -> Optional[dict]:
        """è¿”å›ä¸‹ä¸€ä¸ªé—®ç­”æ ·æœ¬"""
        self._load_data()

        if self._index >= len(self._data):
            return None

        sample = self._data[self._index]
        self._index += 1

        # æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼
        return {
            "id": self._index,
            "query": sample.get("question", sample.get("query", "")),
            "context": sample.get("context", sample.get("passage", "")),
            "ground_truth": sample.get("answer", sample.get("answers", "")),
        }


# ============================================================================
# Map (Embedding): æŸ¥è¯¢å‘é‡åŒ–
# ============================================================================


class EmbeddingMapFunction(MapFunction):
    """Map (Embedding): è°ƒç”¨ embedding æœåŠ¡å°†æŸ¥è¯¢è½¬ä¸ºå‘é‡"""

    def __init__(
        self,
        embedding_base_url: str = "http://localhost:8090/v1",
        embedding_model: str = "BAAI/bge-large-en-v1.5",
        timeout: float = 60.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.timeout = timeout

    def execute(self, data: dict) -> dict:
        """æ‰§è¡Œ embedding"""
        query = data["query"]

        with httpx.Client(timeout=self.timeout, proxy=None) as client:
            response = client.post(
                f"{self.embedding_base_url}/embeddings",
                json={"input": query, "model": self.embedding_model},
            )
            response.raise_for_status()
            result = response.json()

        data["query_embedding"] = result["data"][0]["embedding"]
        return data


# ============================================================================
# Map (Retrieval): æ£€ç´¢æ–‡æ¡£
# ============================================================================


class RetrievalMapFunction(MapFunction):
    """Map (Retrieval): æ ¹æ®æ¨¡å¼æ£€ç´¢æ–‡æ¡£"""

    def __init__(
        self,
        retrieval_mode: str = "hybrid",
        top_k: int = 5,
        embedding_base_url: str = "http://localhost:8090/v1",
        embedding_model: str = "BAAI/bge-large-en-v1.5",
        timeout: float = 60.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.retrieval_mode = retrieval_mode
        self.top_k = top_k
        self.embedding_base_url = embedding_base_url
        self.embedding_model = embedding_model
        self.timeout = timeout

    def execute(self, data: dict) -> dict:
        """æ‰§è¡Œæ£€ç´¢"""
        query = data["query"]
        query_vector = data.get("query_embedding", [])
        context = data.get("context", "")

        # å°† context åˆ†å—ä½œä¸ºå€™é€‰æ–‡æ¡£
        chunk_size = 500
        chunks = [context[i : i + chunk_size] for i in range(0, len(context), chunk_size)]
        if not chunks:
            chunks = [context] if context else ["No context available"]

        if self.retrieval_mode == "dense":
            docs = self._dense_retrieve(query_vector, chunks)
        elif self.retrieval_mode == "sparse":
            docs = self._sparse_retrieve(query, chunks)
        else:  # hybrid
            dense_docs = self._dense_retrieve(query_vector, chunks)
            sparse_docs = self._sparse_retrieve(query, chunks)
            docs = self._merge_results(dense_docs, sparse_docs)

        data["retrieved_docs"] = docs[: self.top_k]
        return data

    def _dense_retrieve(self, query_vector: list[float], chunks: list[str]) -> list[Document]:
        """å‘é‡æ£€ç´¢"""
        if not query_vector:
            return [Document(text=c, score=0.0) for c in chunks]

        with httpx.Client(timeout=self.timeout, proxy=None) as client:
            response = client.post(
                f"{self.embedding_base_url}/embeddings",
                json={"input": chunks[:20], "model": self.embedding_model},
            )
            response.raise_for_status()
            result = response.json()

        docs = []
        for i, emb_data in enumerate(result["data"]):
            chunk_vector = emb_data["embedding"]
            similarity = self._cosine_similarity(query_vector, chunk_vector)
            docs.append(Document(text=chunks[i], score=similarity))

        docs.sort(key=lambda d: d.score, reverse=True)
        return docs

    def _sparse_retrieve(self, query: str, chunks: list[str]) -> list[Document]:
        """BM25 æ£€ç´¢"""
        query_terms = set(query.lower().split())
        docs = []

        for chunk in chunks:
            chunk_terms = set(chunk.lower().split())
            overlap = len(query_terms & chunk_terms)
            score = overlap / (len(query_terms) + 1)
            docs.append(Document(text=chunk, score=score))

        docs.sort(key=lambda d: d.score, reverse=True)
        return docs

    def _merge_results(
        self, dense_docs: list[Document], sparse_docs: list[Document]
    ) -> list[Document]:
        """RRF èåˆ"""
        k = 60
        scores: dict[str, float] = {}

        for rank, doc in enumerate(dense_docs):
            scores[doc.text] = scores.get(doc.text, 0) + 1 / (k + rank + 1)
        for rank, doc in enumerate(sparse_docs):
            scores[doc.text] = scores.get(doc.text, 0) + 1 / (k + rank + 1)

        merged = [Document(text=t, score=s) for t, s in scores.items()]
        merged.sort(key=lambda d: d.score, reverse=True)
        return merged

    @staticmethod
    def _cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot / (norm1 * norm2)


# ============================================================================
# Map (Rerank): é‡æ’åº
# ============================================================================


class RerankMapFunction(MapFunction):
    """Map (Rerank): å¯¹æ£€ç´¢ç»“æœé‡æ’åºå¹¶é€‰å– top-k

    è¿™æ˜¯ä¸€ä¸ª MapFunction å› ä¸ºå®ƒè½¬æ¢æ•°æ®ï¼ˆæ·»åŠ  reranked_docs å­—æ®µï¼‰ã€‚
    """

    def __init__(self, rerank_top_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.rerank_top_k = rerank_top_k

    def execute(self, data: dict) -> dict:
        """æ‰§è¡Œé‡æ’åº"""
        query = data["query"]
        docs = data.get("retrieved_docs", [])

        if not docs:
            data["reranked_docs"] = []
            return data

        query_terms = set(query.lower().split())

        for doc in docs:
            doc_terms = set(doc.text.lower().split())
            term_overlap = len(query_terms & doc_terms)
            doc.score = 0.7 * doc.score + 0.3 * (term_overlap / (len(query_terms) + 1))

        docs.sort(key=lambda d: d.score, reverse=True)
        data["reranked_docs"] = docs[: self.rerank_top_k]
        return data


class HasDocsFilterFunction(FilterFunction):
    """Filter: è¿‡æ»¤æ‰æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£çš„æŸ¥è¯¢

    FilterFunction.execute() è¿”å› boolï¼Œè¡¨ç¤ºæ•°æ®æ˜¯å¦åº”è¯¥é€šè¿‡ã€‚
    """

    def execute(self, data: dict) -> bool:
        """è¿‡æ»¤ç©ºç»“æœ"""
        docs = data.get("reranked_docs", [])
        return len(docs) > 0


# ============================================================================
# Map (LLM Generate): å¼‚æ­¥ç”Ÿæˆ
# ============================================================================


class LLMGenerateMapFunction(MapFunction):
    """Map (LLM): è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ"""

    def __init__(
        self,
        llm_base_url: str = "http://localhost:8001/v1",
        llm_model: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: float = 60.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.timeout = timeout

    def execute(self, data: dict) -> dict:
        """æ‰§è¡Œ LLM ç”Ÿæˆ"""
        query = data["query"]
        docs = data.get("reranked_docs", [])

        # æ„å»º prompt
        context = "\n".join([doc.text for doc in docs]) if docs else "No context available"
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:"""

        with httpx.Client(timeout=self.timeout, proxy=None) as client:
            response = client.post(
                f"{self.llm_base_url}/chat/completions",
                json={
                    "model": self.llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 256,
                    "temperature": 0.7,
                },
            )
            response.raise_for_status()
            result = response.json()

        data["answer"] = result["choices"][0]["message"]["content"]
        return data


# ============================================================================
# Sink: ç»“æœè¾“å‡º
# ============================================================================


class RAGSinkFunction(SinkFunction):
    """RAG Sink: è¾“å‡ºç»“æœå¹¶æ”¶é›†æŒ‡æ ‡"""

    def __init__(self, output_path: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.output_path = output_path
        self.results: list[dict] = []

    def execute(self, data: dict) -> None:
        """è¾“å‡ºç»“æœ"""
        result = {
            "id": data.get("id"),
            "query": data.get("query"),
            "answer": data.get("answer"),
            "ground_truth": data.get("ground_truth"),
        }
        self.results.append(result)

        print(f"âœ… [{result['id']}] Q: {result['query'][:50]}... â†’ A: {result['answer'][:50]}...")

        if self.output_path:
            import json

            with open(self.output_path, "a") as f:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")


# ============================================================================
# RAG Pipeline å°è£…
# ============================================================================


class RAGPipeline:
    """RAG Pipeline å°è£…ç±»

    æä¾›:
    - build(): æ„å»º Pipeline
    - run(): è¿è¡Œ Pipeline
    """

    def __init__(self, config: RAGConfig):
        self.config = config
        self.env: Optional[RemoteEnvironment] = None

    def build(self) -> RemoteEnvironment:
        """æ„å»º RAG Pipeline"""
        scheduler = HeadNodeScheduler()

        self.env = RemoteEnvironment(
            "rag_pipeline",
            host=self.config.job_manager_host,
            port=self.config.job_manager_port,
            scheduler=scheduler,
        )

        # æ„å»º Pipeline: Source â†’ Map â†’ Map â†’ Map(Rerank) â†’ Filter â†’ Map â†’ Sink
        (
            self.env.from_source(
                RAGSourceFunction,
                dataset_name=self.config.dataset_name,
                num_samples=self.config.num_samples,
            )
            .map(
                EmbeddingMapFunction,
                embedding_base_url=self.config.embedding_base_url,
                embedding_model=self.config.embedding_model,
                timeout=self.config.request_timeout,
            )
            .map(
                RetrievalMapFunction,
                retrieval_mode=self.config.retrieval_mode.value,
                top_k=self.config.top_k,
                embedding_base_url=self.config.embedding_base_url,
                embedding_model=self.config.embedding_model,
                timeout=self.config.request_timeout,
            )
            .map(RerankMapFunction, rerank_top_k=self.config.rerank_top_k)
            .filter(HasDocsFilterFunction)
            .map(
                LLMGenerateMapFunction,
                llm_base_url=self.config.llm_base_url,
                llm_model=self.config.llm_model,
                timeout=self.config.request_timeout,
            )
            .sink(RAGSinkFunction, output_path=self.config.output_path)
        )

        return self.env

    def run(self) -> dict:
        """è¿è¡Œ Pipeline"""
        if self.env is None:
            self.build()

        start_time = time.time()
        try:
            # autostop=True ä¼šç­‰å¾… Pipeline æ‰§è¡Œå®Œæˆ
            self.env.submit(autostop=True)
        finally:
            self.env.close()

        duration = time.time() - start_time
        return {
            "pipeline": "A (RAG)",
            "duration_seconds": duration,
            "config": {
                "dataset": self.config.dataset_name,
                "num_samples": self.config.num_samples,
                "retrieval_mode": self.config.retrieval_mode.value,
            },
        }


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ—¶çš„å…¥å£
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    print("=" * 60)
    print("Pipeline A: RAG Pipeline")
    print("=" * 60)

    config = RAGConfig(
        num_samples=10,  # æµ‹è¯•ç”¨å°‘é‡æ ·æœ¬
        embedding_base_url="http://11.11.11.7:8090/v1",
        llm_base_url="http://11.11.11.7:8903/v1",
        output_path="/tmp/rag_pipeline_output.jsonl",
    )

    pipeline = RAGPipeline(config=config)
    result = pipeline.run()

    print(f"\nResult: {result}")
